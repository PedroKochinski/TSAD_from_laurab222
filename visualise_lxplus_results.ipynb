{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TranAD, iTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([hep.style.ROOT, hep.style.firamath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    " 'iTransformer1': {'window': 10, 'steps': 1, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w10 s1 l2'},\n",
    " 'iTransformer2': {'window': 100, 'steps': 50, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w100 s50 l2'},\n",
    " 'iTransformer3': {'window': 10, 'steps': 5, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w10 s5 l2'},\n",
    " 'iTransformer4': {'window': 100, 'steps': 10, 'latent': 2, 'eps': 100, 'lab': 'iTransformer w100 s10 l2'},\n",
    " 'TranAD': {'window': 10, 'steps': 1, 'eps': 100, 'latent': '', 'lab': 'TranAD: w10 s1'},\n",
    "#  'MAD_GAN': {'lab': 'MAD_GAN'},\n",
    "#  'OmniAnomaly': {'lab': 'OmniAnomaly'},\n",
    "#  'LSTM_AE': {'lab': 'LSTM_AE'},\n",
    "#  'DAGMM': {'lab': 'DAGMM'},\n",
    "#  'USAD': {'lab': 'USAD'},\n",
    "#  'IF': {'lab': 'IF'},\n",
    "#  'None': {'lab': 'None'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['GECCO', 'IEEECIS_new2.2', 'UCR', 'SWaT_1D', 'SMAP_new', 'MSL_new', 'SMD'] \n",
    "models = ['iTransformer2'] # ['iTransformer1', 'iTransformer2', 'iTransformer3', 'iTransformer4', 'TranAD']\n",
    "\n",
    "\n",
    "all_paths = []\n",
    "losses = {}\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        if 'iTransformer' in model and model != 'iTransformer':\n",
    "            # paths = glob.glob(f'iTransformer_results_lxplus/iTransformer_{dataset}')\n",
    "            paths = glob.glob(f'iTransformer/iTransformer_{dataset}')\n",
    "        else:\n",
    "            # paths = glob.glob(f'{model}_results_lxplus/{model}_{dataset}')\n",
    "            paths = glob.glob(f'{model}/{model}_{dataset}')\n",
    "        all_paths.extend(paths)\n",
    "        if not paths:\n",
    "            print(f'No paths found for {model} on {dataset}')\n",
    "        feats = 30 if dataset == 'IEEECIS_new2.2' else -1\n",
    "\n",
    "        for path in paths:\n",
    "            if model in config.keys():\n",
    "                if model == 'TranAD':\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}*feats{feats}*/checkpoints/model_final.ckpt\")\n",
    "                else:\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}_feats{feats}_eps{config[model]['eps']}_latent{config[model]['latent']}_7fold*/checkpoints/model_final.ckpt\")\n",
    "            else:\n",
    "                res_path = glob.glob(f'{path}/*n_window10_steps1*feats{feats}*/checkpoints/model_final.ckpt')\n",
    "\n",
    "            if res_path:\n",
    "                res_path = np.sort(res_path)\n",
    "                print(model, dataset, len(res_path))\n",
    "                # print(res_path)\n",
    "                tmp = pd.DataFrame()\n",
    "                key = res_path[0].split('/')[1]\n",
    "                if 'iTransformer' in model and model != 'iTransformer':\n",
    "                        idx = len('iTransformer')  # len(model) - 1\n",
    "                        diff = len(model) - len('iTransformer')\n",
    "                        # insert a number in the key to distinguish between the models at position idx\n",
    "                        key = key[:idx] + model[-diff:] + key[idx:]\n",
    "                losses[key] = {'val_loss': [], 'train_loss': []}\n",
    "                for i, p in enumerate(res_path):\n",
    "                    model = torch.load(p)\n",
    "                    accuracy_list = model['accuracy_list']\n",
    "                    lossT = [i[0] for i in accuracy_list]\n",
    "                    lossV = [i[1] for i in accuracy_list] \n",
    "                    losses[key]['val_loss'].append(lossV)\n",
    "                    losses[key]['train_loss'].append(lossT)\n",
    "            else:\n",
    "                print(f'No results found for {model} on {dataset}')\n",
    "\n",
    "print(losses.keys())\n",
    "# print(len(losses['iTransformer_GECCO']['val_loss']), len(losses['iTransformer_GECCO']['val_loss'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "def plot_losses(losses, dataset):\n",
    "    # colors = current_cycler.by_key()['color']\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 7))\n",
    "\n",
    "    for key in losses.keys():\n",
    "        if dataset in key:\n",
    "            print(key)\n",
    "            train_losses = losses[key]['train_loss']\n",
    "            val_losses = losses[key]['val_loss']\n",
    "            for i in range(len(train_losses)):\n",
    "                plt.plot(train_losses[i], '-o', label=f'Train loss fold {i+1}', color=colors[i])\n",
    "                plt.plot(val_losses[i], '-*', label=f'Val loss fold {i+1}', color=colors[i], markersize=8)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Losses on {dataset}')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(f'./studies_results_lxplus/losses/{key}_losses.png', facecolor='w')\n",
    "            plt.show()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "for dataset in datasets:\n",
    "    plot_losses(losses, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['IEEECIS_new2.2', 'SMAP_new', 'MSL_new', 'UCR', 'SMD', 'SWaT_1D', 'GECCO', 'SMD']  #, 'ATLAS_TS']\n",
    "models = ['iTransformer1', 'iTransformer2', 'iTransformer3', 'TranAD']\n",
    "        #   'OmniAnomaly', 'MAD_GAN', 'LSTM_AE', 'DAGMM', 'USAD', 'IF', 'None'] \n",
    "\n",
    "all_paths = []\n",
    "results_mean_std = {}\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        if 'iTransformer' in model and model != 'iTransformer':\n",
    "            paths = glob.glob(f'iTransformer_results_lxplus/iTransformer_{dataset}')\n",
    "        else:\n",
    "            paths = glob.glob(f'{model}_results_lxplus/{model}_{dataset}')\n",
    "        all_paths.extend(paths)\n",
    "        if not paths:\n",
    "            print(f'No paths found for {model} on {dataset}')\n",
    "        feats = 30 if dataset == 'IEEECIS_new2.2' else -1\n",
    "\n",
    "        for path in paths:\n",
    "            if model in config.keys():\n",
    "                if model == 'TranAD':\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}*feats{feats}*/results/res.csv\")\n",
    "                else:\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}_feats{feats}_eps{config[model]['eps']}_latent{config[model]['latent']}*/results/res.csv\")\n",
    "            elif model == 'None':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            elif model == 'IF':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            else:\n",
    "                res_path = glob.glob(f'{path}/*n_window10_steps1*feats{feats}*/results/res.csv')\n",
    "\n",
    "            if res_path:\n",
    "                res_path = np.sort(res_path)\n",
    "                print(model, dataset, len(res_path))\n",
    "                # print(res_path)\n",
    "                tmp = pd.DataFrame()\n",
    "                for p in res_path:\n",
    "                    res = pd.read_csv(p)\n",
    "                    tmp = pd.concat((tmp, res.iloc[-3:]))\n",
    "\n",
    "                key = path.split('/')[1]\n",
    "                if 'iTransformer' in model and model != 'iTransformer':\n",
    "                    idx = len('iTransformer')  # len(model) - 1\n",
    "                    diff = len(model) - len('iTransformer')\n",
    "                    # insert a number in the key to distinguish between the models at position idx\n",
    "                    key = key[:idx] + model[-diff:] + key[idx:]\n",
    "\n",
    "                mean_values = tmp.groupby('Unnamed: 0').mean()\n",
    "                std_values = tmp.groupby('Unnamed: 0').std()\n",
    "                mean_values = mean_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "                std_values = std_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "                results_mean_std[key] = {'mean': mean_values, 'std': std_values}\n",
    "                # print(results_mean_std[key])\n",
    "            else:\n",
    "                print(f'No results found for {model} on {dataset}')\n",
    "\n",
    "            break\n",
    "\n",
    "# print(len(all_paths))\n",
    "print(results_mean_std.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'lines.markersize': 6})\n",
    "plt.rcParams.update({'errorbar.capsize': 8})\n",
    "plt.rcParams.update({'lines.linewidth': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['local (incl. OR)', 'local (maj. voting)', 'global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_mean_std_err(results, modes, datasets, models, metric='MCC', name=None, labels=None):\n",
    "    # colors = plt.cm.plasma(np.linspace(0, 1, len(models)+1))\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(22, 16), sharex=True, sharey=True)\n",
    "\n",
    "    for i, mode in enumerate(modes):\n",
    "        for j, model in enumerate(models):\n",
    "            scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "            for dataset in datasets:\n",
    "                key = f'{model}_{dataset}'\n",
    "               \n",
    "                for val in ['mean', 'std']:\n",
    "                    if key in results:\n",
    "                        scores[val] = np.append(scores[val], results[key][val][metric].iloc[i])\n",
    "                    else:\n",
    "                        scores[val] = np.append(scores[val], 0)\n",
    "            \n",
    "            x_positions = np.arange(len(datasets)) + j * 0.1  # add offset for each model\n",
    "            if labels:\n",
    "                axs[i].errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=labels[j])  #, color=colors[j % len(colors)], capsize=5)\n",
    "            else:\n",
    "                axs[i].errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=model)  #, color=colors[j % len(colors)], capsize=5)\n",
    "            \n",
    "            axs[i].set_xticks(np.arange(len(datasets)) + 0.1 * (len(models) - 1) / 2)\n",
    "            axs[i].set_xticklabels(labels=datasets)\n",
    "            if metric == 'MCC':\n",
    "                axs[i].set_ylim(-1, 1)\n",
    "            else:\n",
    "                axs[i].set_ylim(top=1.0)\n",
    "            if metric == 'ROC/AUC':\n",
    "                axs[i].set_ylabel('ROC AUC')\n",
    "            elif metric == 'f1':\n",
    "                axs[i].set_ylabel('F1')\n",
    "            else:\n",
    "                axs[i].set_ylabel(metric)\n",
    "            axs[i].set_title(mode)\n",
    "        axs[i].legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        if metric == 'ROC/AUC':\n",
    "            metric = 'rocauc'\n",
    "        plt.savefig(f'./studies_results_lxplus/{name}_{metric}.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_scores_mean_std_err(results, datasets, models, metric='MCC', name=None, labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "        for dataset in datasets:\n",
    "            key = f'{model}_{dataset}'\n",
    "            \n",
    "            if key in results:\n",
    "                scores['mean'] = np.append(scores['mean'], results[key]['mean'][metric].max())\n",
    "                idx = np.where(results[key]['mean'][metric] == results[key]['mean'][metric].max())[0][0]\n",
    "                scores['std'] = np.append(scores['std'], results[key]['std'][metric].iloc[idx])\n",
    "            else:\n",
    "                scores['mean'] = np.append(scores['mean'], 0)\n",
    "                scores['std'] = np.append(scores['std'], 0)\n",
    "        \n",
    "        x_positions = np.arange(len(datasets)) + j * 0.1  # add offset for each model\n",
    "        if labels:\n",
    "            ax.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=labels[j])  \n",
    "        else:\n",
    "            ax.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=model) \n",
    "            \n",
    "    ax.set_xticks(np.arange(len(datasets)) + 0.1 * (len(models) - 1) / 2)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    # ax.set_ylim(top=1.0)\n",
    "    if metric == 'ROC/AUC':\n",
    "        ax.set_ylabel('ROC AUC')\n",
    "        ax.set_title(f'Best ROC AUC Scores')\n",
    "        metric = 'rocauc'\n",
    "    elif metric == 'f1':\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.set_title(f'Best F1 Scores')\n",
    "    else:\n",
    "        ax.set_ylabel(metric)    \n",
    "        ax.set_title(f'Best {metric} scores')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        plt.savefig(f'./studies_results_lxplus/{name}_{metric}best.png', facecolor='w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = ['IEEECIS_new2.2', 'GECCO', 'SMD', 'SMAP_new', 'MSL_new', 'SWaT_1D', 'UCR'] \n",
    "models_plot = ['iTransformer4'] #, 'MAD_GAN', 'OmniAnomaly', 'LSTM_AE', 'DAGMM', 'USAD']\n",
    "lab = [config[m]['lab'] for m in models_plot]\n",
    "\n",
    "name = None  # 'new'  # 'rep5_new_all'\n",
    "plot_best_scores_mean_std_err(results_mean_std, data_plot, models_plot, metric='MCC', labels=lab, name=name)\n",
    "plot_scores_mean_std_err(results_mean_std, modes, data_plot, models_plot, metric='MCC', labels=lab, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_results(results, datasets, models, metric='MCC', labels=None, val='mean'):\n",
    "    dict = {}\n",
    "    for j, model in enumerate(models):\n",
    "        mcc_scores = []\n",
    "        for dataset in datasets:\n",
    "            key = f'{model}_{dataset}'\n",
    "            print(key)\n",
    "            if key in results:\n",
    "                mcc_scores.append(results[key][val][metric].max().round(3))\n",
    "            else:\n",
    "                mcc_scores.append(0)  # If no data, append 0\n",
    "\n",
    "            dict[labels[j]] = mcc_scores\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_results(results, datasets, models, metric='MCC', labels=None):\n",
    "    dict = {}\n",
    "    for j, model in enumerate(models):\n",
    "        scores = []\n",
    "        for dataset in datasets:\n",
    "            key = f'{model}_{dataset}'\n",
    "            print(key)\n",
    "            if key in results:\n",
    "                scores.append(rf\"${results[key]['mean'][metric].round(3)} \\pm {results[key]['mean'][metric].round(3)}$\")\n",
    "            else:\n",
    "                scores.append(0)  # If no data, append 0\n",
    "\n",
    "            dict[labels[j]] = scores\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'mean'\n",
    "metric = 'train_loss'\n",
    "namee = 'latent2_rep5_new'\n",
    "data3 = ['IEEECIS_new2.2', 'GECCO', 'SMD', 'SMAP_new', 'MSL_new','UCR']\n",
    "# ['ATLAS_TS', 'IEEECIS_new2.2', 'GECCO6', 'SMD', 'SMAP_new', 'MSL_new', 'SWaT', 'UCR'] #'ATLAS_TS',\n",
    "models3 = ['iTransformer3', 'iTransformer4', 'TranAD']\n",
    "# ['iTransformer4', 'iTransformer8', 'iTransformer6', 'iTransformer9', 'iTransformer11', 'TranAD']\n",
    "# ['iTransformer3', 'iTransformer5', 'iTransformer7', 'iTransformer10', 'TranAD']\n",
    "# ['iTransformer3', 'iTransformer5', 'iTransformer7', 'TranAD']\n",
    "# ['iTransformer4', 'iTransformer8', 'iTransformer6', 'iTransformer9', 'TranAD']\n",
    "lab = [config[m]['lab'] for m in models3]\n",
    "\n",
    "dict_bestMCC = list_results(results_mean_std, data3, models3, metric, labels=lab, val=val)\n",
    "df_bestMCC = pd.DataFrame(dict_bestMCC, index=data3).T\n",
    "print(df_bestMCC)\n",
    "print(df_bestMCC.values)\n",
    "metric = metric.replace(' ', '_')\n",
    "# df_bestMCC.to_csv(f'studies_earlystopping/data/best{metric}_{namee}_{val}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot valid vs train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import MyDataset\n",
    "from src.plotting import features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_valid(dataset):\n",
    "    for k in range(1, 8):\n",
    "        # window and step size don't matter because we use complete data\n",
    "        train = MyDataset(dataset, 100, 100, 'iTransformer', flag='train', feats=-1, enc=False, k=k)\n",
    "        valid = MyDataset(dataset,  100, 100, 'iTransformer', flag='valid', feats=-1, enc=False, k=k)\n",
    "        x_train = train.get_complete_data()\n",
    "        x_valid = valid.get_complete_data()\n",
    "        feats = x_train.shape[1]\n",
    "        # print(train.__len__(), valid.__len__())\n",
    "        # print(x_train.shape, x_valid.shape)\n",
    "        if feats > 30:\n",
    "            feats = 30\n",
    "            x_train = x_train[:, :feats]\n",
    "            x_valid = x_valid[:, :feats]\n",
    "\n",
    "        if dataset in features_dict.keys():\n",
    "            features = features_dict[dataset]\n",
    "        else:\n",
    "            features = [f'Dim {i}' for i in range(feats)]\n",
    "        size = int(len(features))\n",
    "\n",
    "        if feats > 1:\n",
    "            fig, axs = plt.subplots(feats, 1, figsize=(15, size), sharex=True, constrained_layout=True)\n",
    "            for i, feat in enumerate(features):\n",
    "                axs[i].plot(x_train[:, i], label=f'Train')\n",
    "                axs[i].plot(x_valid[:, i], '--', label=f'Valid fold {k}', color='tab:orange')\n",
    "                # axs[2*i].plot(x_train[:, i], label=f'Train dim {i}')\n",
    "                # axs[2*i+1].plot(x_valid[:, i], label=f'Valid dim {i}', color='tab:orange')\n",
    "                axs[i].set_ylabel(feat, rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "                axs[i].yaxis.set_label_coords(-0.1, 0.5)\n",
    "            axs[0].set_title(f'Train and Validation data for {dataset}')\n",
    "            if dataset == 'GECCO':\n",
    "                axs[0].legend(ncol=2, bbox_to_anchor=(0.5, -0.1), loc='lower center', borderaxespad=0., frameon=False)\n",
    "            else:\n",
    "                axs[0].legend(ncol=2, bbox_to_anchor=(0.98, 0.25), loc='lower right', borderaxespad=0., frameon=False)\n",
    "            axs[-1].set_xlabel('Timestamp')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(15, 5), constrained_layout=True)\n",
    "            ax.plot(x_train, label='Train')\n",
    "            ax.plot(x_valid, '--', label=f'Valid fold {k}', color='tab:orange')\n",
    "            ax.set_ylabel(features[0], rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "            ax.yaxis.set_label_coords(0.3, -0.3)\n",
    "            ax.set_title(f'Train and Validation data for {dataset}')\n",
    "            ax.legend(ncol=2, loc='upper right', borderaxespad=0., frameon=False)\n",
    "            ax.set_xlabel('Timestamp')\n",
    "        \n",
    "        plt.savefig(f'./studies_results_lxplus/train_valid/{dataset}_valid_7fold{k}.png', facecolor='w')\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['IEEECIS_new2.2']  #, 'MSL_new', 'SMD', 'GECCO', 'IEEECIS_new2.2'] #  'GECCO', 'IEEECIS_new2.2', 'UCR', 'SWaT_1D'\n",
    "\n",
    "for dataset in datasets:\n",
    "    plot_train_valid(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## study MSE vs MCC evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import backprop, local_pot, local_anomaly_labels\n",
    "from src.data_loader import MyDataset, DataLoader\n",
    "from src.pot import pot_eval\n",
    "from src.diagnosis import hit_att, ndcg\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(modelname, dims, n_window, step_size=None, path=None, prob=False, weighted=False):\n",
    "\timport src.models\n",
    "\tmodel_class = getattr(src.models, modelname)\n",
    "\tif modelname == 'iTransformer':\n",
    "\t\tmodel = model_class(dims, n_window, step_size, prob, weighted).double()\n",
    "\telse:\n",
    "\t\tmodel = model_class(dims, n_window, prob).double()\n",
    "\t\n",
    "\toptimizer = torch.optim.AdamW(model.parameters() , lr=model.lr, weight_decay=1e-5)\n",
    "\tscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.9)\n",
    "\n",
    "\tprint(f\"Loading pre-trained model: {model.name} from {path}\")\n",
    "\tcheckpoint = torch.load(path)\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\tscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\tepoch = checkpoint['epoch']\n",
    "\taccuracy_list = checkpoint['accuracy_list']\n",
    "\n",
    "\treturn model, optimizer, scheduler, epoch, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(dataset, model, index, modeltype):\n",
    "    if modeltype == 'iTransformer':\n",
    "        checkpoints_path = glob.glob(f'{modeltype}_results_lxplus/{modeltype}_{dataset}/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_latent{config[model][\"latent\"]}_{index}/checkpoints/model_epoch*.ckpt')\n",
    "    else:\n",
    "        checkpoints_path = glob.glob(f'{model}_results_lxplus/{model}_{dataset}/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_{index}/checkpoints/model_epoch*.ckpt')\n",
    "    checkpoints_path = sorted(checkpoints_path, key=lambda x: int(x.split('_')[-1].split('.')[0].replace('epoch', '')))\n",
    "    print(len(checkpoints_path))\n",
    "    res_path = os.path.join(checkpoints_path[0].split('/')[0], checkpoints_path[0].split('/')[1], 'results', checkpoints_path[0].split('/')[2])\n",
    "    print(res_path)\n",
    "    if not os.path.exists(res_path):\n",
    "        os.makedirs(res_path)\n",
    "\n",
    "    flag_less = False\n",
    "    feats = -1\n",
    "    if dataset == 'IEEECIS_new2.2':\n",
    "        feats = 30\n",
    "    elif dataset in ['SMAP_new', 'SMD']:\n",
    "        flag_less = True\n",
    "\n",
    "    test = MyDataset(dataset, config[model]['window'], config[model]['window'], modeltype, flag='test', feats=feats, enc=False, less=flag_less, k=-1)\n",
    "    train_test = MyDataset(dataset, config[model]['window'], config[model]['window'], modeltype, flag='train', feats=feats, less=flag_less, enc=False, k=-1)\n",
    "    feats = test.feats\n",
    "    enc_feats = test.enc_feats\n",
    "    labels = test.get_labels()\n",
    "\n",
    "    for path in checkpoints_path:\n",
    "        trained_model, optimizer, scheduler, epoch, accuracy_list = load_model(modeltype, feats, config[model]['window'], config[model]['steps'], path, prob=False, weighted=False)\n",
    "        trained_model.eval()\n",
    "        print(f'Loaded model from epoch {epoch}')\n",
    "\n",
    "        if os.path.exists(f'{res_path}/res_epoch{epoch}.csv') and os.path.exists(f'{res_path}/pred_labels_epoch{epoch}.csv'):\n",
    "            print(f'Epoch {epoch} already evaluated')\n",
    "            continue\n",
    "\n",
    "        data_loader_train_test = DataLoader(train_test, batch_size=trained_model.batch, shuffle=False)\n",
    "        data_loader_test = DataLoader(test, batch_size=trained_model.batch, shuffle=False)\n",
    "        \n",
    "        lossT = backprop(-1, trained_model, data_loader_train_test, feats, optimizer, scheduler, training=False, enc_feats=enc_feats, prob=False, pred=False)  # need anomaly scores on training data for POT\n",
    "        loss, y_pred = backprop(-1, trained_model, data_loader_test, feats, optimizer, scheduler, training=False, enc_feats=enc_feats, prob=False, pred=True)\n",
    "\n",
    "        if 'iTransformer' in trained_model.name or trained_model.name in ['LSTM_AE']:\n",
    "            # cut out the padding from test data, loss tensors\n",
    "            lossT_tmp, loss_tmp, y_pred_tmp = [], [], []\n",
    "            # print(test.get_ts_lengths(), np.sum(test.get_ts_lengths()), len(test.get_ts_lengths()))\n",
    "            # print(test.get_ideal_lengths(), np.sum(test.get_ideal_lengths()), len(test.get_ideal_lengths()))\n",
    "            start = 0\n",
    "            for i, l in enumerate(test.get_ts_lengths()):\n",
    "                loss_tmp.append(loss[start:start+l])\n",
    "                y_pred_tmp.append(y_pred[start:start+l])\n",
    "                start += test.get_ideal_lengths()[i]\n",
    "            \n",
    "            start = 0\n",
    "            for i, l in enumerate(train_test.get_ts_lengths()):\n",
    "                lossT_tmp.append(lossT[start:start+l])\n",
    "                start += train_test.get_ideal_lengths()[i]\n",
    "\n",
    "            lossT = np.concatenate(lossT_tmp, axis=0)\n",
    "            loss = np.concatenate(loss_tmp, axis=0)\n",
    "            y_pred = np.concatenate(y_pred_tmp, axis=0)\n",
    "        train_loss = np.mean(lossT)\n",
    "        test_loss = np.mean(loss)\n",
    "        \n",
    "        ### anomaly labels\n",
    "        preds, _ = local_pot(loss, lossT, labels)\n",
    "        true_labels = (np.sum(labels, axis=1) >= 1) + 0\n",
    "        # local anomaly labels\n",
    "        labelspred, result_local1 = local_anomaly_labels(preds, true_labels, nb_adim=1)\n",
    "        majority = math.ceil(labels.shape[1] / 2)  # do majority voting over dimensions for local results instead of inclusive OR\n",
    "        labelspred_maj, result_local2 = local_anomaly_labels(preds, true_labels, nb_adim=majority)\n",
    "        labelspred_all = []\n",
    "        results_all = pd.DataFrame()\n",
    "\n",
    "        # global anomaly labels\n",
    "        lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "        true_labels = (np.sum(labels, axis=1) >= 1) + 0\n",
    "        result_global, pred2 = pot_eval(lossTfinal, lossFinal, true_labels, None, f'all_dim')\n",
    "        labelspred_glob = (pred2 >= 1) + 0\n",
    "        result_global.update(hit_att(loss, labels))\n",
    "        result_global.update(ndcg(loss, labels))\n",
    "        result_global.update({'detection_level_q': 1e-5})\n",
    "        result_global.update({'train_loss': train_loss, 'test_loss': test_loss})\n",
    "        # print('\\nglobal results') \n",
    "        # print(result_global)\n",
    "\n",
    "        # saving results\n",
    "        df_res_global = pd.DataFrame.from_dict(result_global, orient='index').T\n",
    "        df_res_global.index = ['global']\n",
    "        result_local1 = pd.DataFrame.from_dict(result_local1, orient='index').T\n",
    "        result_local2 = pd.DataFrame.from_dict(result_local2, orient='index').T\n",
    "        result_local1.index = ['local_all']\n",
    "        result_local2.index = ['local_all_maj']\n",
    "        df_res_local = pd.concat([result_local1, result_local2])\n",
    "        df_res = pd.concat([df_res_local, df_res_global]) \n",
    "        df_labels = pd.DataFrame({'local': labelspred, 'local_maj': labelspred_maj, 'global': labelspred_glob})\n",
    "\n",
    "        df_res.to_csv(f'{res_path}/res_epoch{epoch}.csv')    \n",
    "        df_labels.to_csv(f'{res_path}/pred_labels_epoch{epoch}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype = 'TranAD'  #  'iTransformer'\n",
    "datasets =  ['SMAP_new']  #['GECCO', 'IEEECIS_new2.2', 'UCR', 'SWaT_1D', 'SMAP_new', 'MSL_new', 'SMD'] \n",
    "models = ['TranAD']  # , 'iTransformer4', 'iTransformer3']  # 'TranAD'\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        for i in range(1, 6):\n",
    "            get_scores(dataset, model, i, modeltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datasets, models, modeltype):\n",
    "    data = {f'{dataset}_{model}_fold{i}': {} for dataset in datasets for model in models for i in range(1, 6)}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            print(dataset, model)\n",
    "            for i in range(1, 6):\n",
    "                if modeltype == 'iTransformer':\n",
    "                    datapath = glob.glob(f'{modeltype}_results_lxplus/{modeltype}_{dataset}/results/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_latent{config[model][\"latent\"]}_{i}/res_epoch*.csv')\n",
    "                else:\n",
    "                    datapath = glob.glob(f'{model}_results_lxplus/{model}_{dataset}/results/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_{i}/res_epoch*.csv')\n",
    "                datapath = sorted(datapath, key=lambda x: int(x.split('_')[-1].split('.')[0].replace('epoch', '')))\n",
    "                # print(len(datapath), datapath)\n",
    "\n",
    "                for e, path in enumerate(datapath):\n",
    "                    res = pd.read_csv(path, index_col=0)\n",
    "                    modes = list(res.index)\n",
    "                    for mode in modes:\n",
    "                        if e == 0:\n",
    "                            data[f'{dataset}_{model}_fold{i}'][mode] = pd.DataFrame(res.loc[mode]).T\n",
    "                        else:\n",
    "                            data[f'{dataset}_{model}_fold{i}'][mode] = pd.concat([data[f'{dataset}_{model}_fold{i}'][mode], pd.DataFrame(res.loc[mode]).T], axis=0)\n",
    "                    \n",
    "                for mode in modes:\n",
    "                    data[f'{dataset}_{model}_fold{i}'][mode] = data[f'{dataset}_{model}_fold{i}'][mode].reset_index(drop=True) \n",
    "                # print(data[f'{dataset}_{model}_fold{i}']['local_all'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MCC scores and test_loss over all epochs for each mode\n",
    "def plot_mcc_test_loss(data, dataset, model, metric='MCC'):\n",
    "    modes = data[f'{dataset}_{model}_fold1'].keys()\n",
    "    # epochs = range(len(data[f'{dataset}_{model}_fold1']['global'][metric]))\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, 5))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        for j, mode in enumerate(modes):\n",
    "            if j == 0:\n",
    "                mcc_scores = data[f'{dataset}_{model}_fold{i}'][mode][metric]\n",
    "            else:\n",
    "                if mcc_scores.max() < data[f'{dataset}_{model}_fold{i}'][mode][metric].max():\n",
    "                    mcc_scores = data[f'{dataset}_{model}_fold{i}'][mode][metric]\n",
    "                    labell = f'{metric} {mode} fold {i}'\n",
    "    \n",
    "       \n",
    "        test_losses = data[f'{dataset}_{model}_fold{i}']['global']['test_loss']\n",
    "        plt.plot(mcc_scores, '-o', label=labell, color=colors[i-1])\n",
    "        plt.plot(test_losses, '--', label=f'Test MSE fold {i}', color=colors[i-1])\n",
    "        # ax = plt.gca()\n",
    "        # ax.plot(mcc_scores, '-', label=labell, color=colors[i-1])\n",
    "        # ax.set_ylim(0,1)\n",
    "        # if i==1:\n",
    "        #     ax.set_ylabel('Scores')\n",
    "        # ax2 = ax.twinx()\n",
    "        # ax2.plot(test_losses, label=f'Test MSE', linestyle='--', color=colors[i-1])\n",
    "        # ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # ax.set_xlabel('Epochs')\n",
    "    # ax2.set_ylabel('Test MSE', rotation=270, labelpad=15, ha='left', va='center')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.title(f'{dataset} - {config[model][\"lab\"]}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./studies_results_lxplus/mcc_vs_mse/{dataset}_{model}_{metric}.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MCC scores and test_loss over all epochs for each mode\n",
    "def plot_mcc_test_loss2(data, dataset, model, modeltype, metric='MCC'):\n",
    "    modes = data[f'{dataset}_{model}_fold1'].keys()\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, 5))\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(24, 12), sharex=False, sharey=True, constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "    print(model, dataset)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        for j, mode in enumerate(modes):\n",
    "            if j == 0:\n",
    "                mcc_scores = data[f'{dataset}_{model}_fold{i}'][mode][metric]\n",
    "                labell = f'{metric} {mode}'\n",
    "            else:\n",
    "                if mcc_scores.max() < data[f'{dataset}_{model}_fold{i}'][mode][metric].max():\n",
    "                    mcc_scores = data[f'{dataset}_{model}_fold{i}'][mode][metric]\n",
    "                    labell = f'{metric} {mode}'\n",
    "    \n",
    "        if modeltype == 'iTransformer':\n",
    "            accuracy_list_path = glob.glob(f'{modeltype}_results_lxplus/{modeltype}_{dataset}/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_latent{config[model][\"latent\"]}_{i}/checkpoints/accuracy_list.npy')\n",
    "        else:\n",
    "            accuracy_list_path = glob.glob(f'{model}_results_lxplus/{model}_{dataset}/n_window{config[model][\"window\"]}_steps{config[model][\"steps\"]}_feats*_eps{config[model][\"eps\"]}_{i}/checkpoints/accuracy_list.npy')\n",
    "        accuracy_list = np.load(accuracy_list_path[0])\n",
    "        train_losses = [i[0] for i in accuracy_list]\n",
    "        valid_losses = [i[1] for i in accuracy_list]\n",
    "\n",
    "        best_model = torch.load(f'{accuracy_list_path[0].replace(\"accuracy_list.npy\", \"model_best.ckpt\")}')\n",
    "        best_epoch = best_model['epoch'] + 1 # because numbering began at 0, but in plots we want to start at 1\n",
    "\n",
    "        axs[i-1].plot(np.arange(1, len(mcc_scores)+1), mcc_scores, 'r-o', label=labell)  #, color=colors[i-1])\n",
    "        axs[i-1].axvline(x=best_epoch, label=f'Best epoch {best_epoch}', color='k', linestyle=':')\n",
    "\n",
    "        ax2 = axs[i-1].twinx()\n",
    "        ax2.plot(np.arange(1, len(train_losses)+1), train_losses, '--', label=f'Train MSE', linewidth=3)  #, color=colors[i-1])\n",
    "        ax2.plot(np.arange(1, len(valid_losses)+1),valid_losses, '--', label=f'Valid MSE', linewidth=3)  #, color=colors[i-1])\n",
    "        if metric == 'f1':\n",
    "            axs[i-1].set_ylim(top=1, bottom=0)\n",
    "            axs[i-1].set_ylabel(f'F1 score')\n",
    "        else:\n",
    "            axs[i-1].set_ylabel(f'{metric}')\n",
    "            # axs[i-1].set_ylim(top=1, bottom=0.4)\n",
    "        # ax2.set_ylim(0, 0.1)\n",
    "        ax2.set_ylabel('Test Loss')\n",
    "        axs[i-1].legend(loc='upper right')\n",
    "        ax2.legend(loc='center right')\n",
    "        # ax2.legend(bbox_to_anchor=(1, 0.9), loc='upper right')\n",
    "        axs[i-1].set_title(f'Valid fold {i}')\n",
    "        axs[i-1].grid(True)\n",
    "        axs[i-1].set_xlabel('Epochs')\n",
    "    axs[-1].remove()\n",
    "\n",
    "    fig.suptitle(f'{dataset} - {config[model][\"lab\"]}')\n",
    "    plt.xlabel('Epochs')\n",
    "    if metric == 'ROC/AUC':\n",
    "        metric = 'rocauc'\n",
    "    plt.savefig(f'./studies_results_lxplus/mcc_vs_mse/{dataset}_{model}_{metric}2.png', facecolor='w')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype = 'iTransformer'  #  'iTransformer'\n",
    "datasets = ['GECCO', 'IEEECIS_new2.2', 'UCR', 'SWaT_1D', 'SMAP_new', 'MSL_new', 'SMD'] # 'MSL_new',\n",
    "models =  ['iTransformer1', 'iTransformer2', 'iTransformer3', 'iTransformer4']  # 'TranAD' ['iTransformer1'\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        # data = get_data(datasets, models, modeltype)\n",
    "        data = get_data([dataset], [model], modeltype)\n",
    "        # print(data.keys())\n",
    "        # plot_mcc_test_loss(data, dataset, model, 'MCC')\n",
    "        plot_mcc_test_loss2(data, dataset, model, modeltype, 'MCC')\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['IEEECIS_new2.2', 'UCR', 'SWaT_1D', 'GECCO'] # ['IEEECIS_new2.2', 'SMAP_new', 'MSL_new', 'UCR', 'SMD', 'SWaT_1D', 'GECCO', 'SMD']  #, 'ATLAS_TS']\n",
    "models = ['iTransformer1', 'iTransformer2', 'iTransformer3'] #, 'TranAD',\n",
    "        #   'OmniAnomaly', 'MAD_GAN', 'LSTM_AE', 'DAGMM', 'USAD', 'IF', 'None'] \n",
    "\n",
    "all_paths = []\n",
    "results_all = {}\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        if 'iTransformer' in model and model != 'iTransformer':\n",
    "            paths = glob.glob(f'iTransformer_results_lxplus/iTransformer_{dataset}')\n",
    "        else:\n",
    "            paths = glob.glob(f'{model}_results_lxplus/{model}_{dataset}')\n",
    "        all_paths.extend(paths)\n",
    "        if not paths:\n",
    "            print(f'No paths found for {model} on {dataset}')\n",
    "        feats = 30 if dataset == 'IEEECIS_new2.2' else -1\n",
    "\n",
    "        for path in paths:\n",
    "            if model in config.keys():\n",
    "                if model == 'TranAD':\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}*feats{feats}*/results/res.csv\")\n",
    "                else:\n",
    "                    if config[model]['weighted']:\n",
    "                        res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}_feats{feats}_eps{config[model]['eps']}_latent{config[model]['latent']}_weighted*/results/res.csv\")\n",
    "                    else:\n",
    "                        res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}_feats{feats}_eps{config[model]['eps']}_latent{config[model]['latent']}*/results/res.csv\")\n",
    "            elif model == 'None':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            elif model == 'IF':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            else:\n",
    "                res_path = glob.glob(f'{path}/*n_window10_steps1*feats{feats}*/results/res.csv')\n",
    "\n",
    "            if res_path:\n",
    "                res_path = np.sort(res_path)\n",
    "                print(model, dataset, len(res_path))\n",
    "                # print(res_path)\n",
    "                tmp = pd.DataFrame()\n",
    "                for p in res_path:\n",
    "                    res = pd.read_csv(p)\n",
    "                    tmp = pd.concat((tmp, res.iloc[-3:]))\n",
    "\n",
    "                key = path.split('/')[1]\n",
    "                if 'iTransformer' in model and model != 'iTransformer':\n",
    "                    idx = len('iTransformer')  # len(model) - 1\n",
    "                    diff = len(model) - len('iTransformer')\n",
    "                    # insert a number in the key to distinguish between the models at position idx\n",
    "                    key = key[:idx] + model[-diff:] + key[idx:]\n",
    "                    results_all[key] = {}\n",
    "\n",
    "                    # Create a dictionary to store the dataframes\n",
    "                    grouped = tmp.groupby('Unnamed: 0')\n",
    "                    grouped_dfs = {name: group for name, group in grouped}\n",
    "                    grouped_dfs = {name: group.drop(columns='Unnamed: 0') for name, group in grouped_dfs.items()}\n",
    "                    for mode in grouped_dfs.keys():\n",
    "                        results_all[key][mode] = grouped_dfs[mode]\n",
    "                else:\n",
    "                    print(f'No results found for {model} on {dataset}')\n",
    "\n",
    "            break\n",
    "\n",
    "# print(len(all_paths))\n",
    "print(results_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all['iTransformer1_GECCO']['global']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(results_all, datasets, models, metric='MCC'):\n",
    "    for model in models:\n",
    "        modes = list(results_all[f'{model}_{datasets[0]}'].keys())\n",
    "        fig, axs = plt.subplots(len(modes), 1, figsize=(2 * len(datasets), 3 * len(modes)), \n",
    "                                constrained_layout=True, sharex=True, sharey=True)\n",
    "        \n",
    "        colors = plt.cm.plasma(np.linspace(0, 1, len(datasets) + 1))\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            data_box = []\n",
    "            for dataset in datasets:\n",
    "                if f'{model}_{dataset}' in results_all:\n",
    "                    data_box.append(results_all[f'{model}_{dataset}'][mode][metric])\n",
    "            \n",
    "            box = axs[i].boxplot(data_box, labels=datasets, patch_artist=True)\n",
    "            \n",
    "            # Change the colors of the boxplots\n",
    "            for patch, color in zip(box['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "            axs[i].text(0.98, 0.1, f'{mode}', horizontalalignment='right', verticalalignment='bottom', transform=axs[i].transAxes, fontsize=20)\n",
    "            axs[i].set_ylabel(metric)\n",
    "            if metric == 'MCC':\n",
    "                axs[i].set_ylim(-1, 1)\n",
    "            else:\n",
    "                axs[i].set_ylim(top=1.0)\n",
    "\n",
    "        axs[0].set_title(f'{config[model][\"lab\"]}')\n",
    "        plt.savefig(f'./studies_results_lxplus/boxplots_{model}_{metric}.png', facecolor='w')\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "plot_boxplots(results_all, datasets, models, metric='MCC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTransf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
