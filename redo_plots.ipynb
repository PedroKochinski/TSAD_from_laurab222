{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import mplhep as hep\n",
    "from src.models import TranAD, iTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from src.data_loader import convert_to_windows_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['axes.prop_cycle'] = cycler('color', ['#17becf','#8c564b', '#e377c2', '#7f7f7f','#bcbd22', '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iTransf_n_windows = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000, 2000]\n",
    "TranAD_n_windows = [10, 20, 30, 40, 50, 60, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "dataset = 'SMAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(dataset):\n",
    "# \tloader = []\n",
    "# \tfor file in ['train', 'test', 'labels']:\n",
    "# \t\tif dataset == 'SMD': file = 'machine-1-1_' + file\n",
    "# \t\tif dataset == 'SMAP': file = 'P-1_' + file\n",
    "# \t\tif dataset == 'MSL': file = 'C-1_' + file\n",
    "# \t\tif dataset == 'UCR': file = '136_' + file\n",
    "# \t\tif dataset == 'NAB': file = 'ec2_request_latency_system_failure_' + file\n",
    "# \t\tloader.append(np.load(os.path.join(f'processed/{dataset}', f'{file}.npy')))\n",
    "\t\n",
    "# \ttrain_loader = DataLoader(loader[0], batch_size=loader[0].shape[0])\n",
    "# \ttest_loader = DataLoader(loader[1], batch_size=loader[1].shape[0])\n",
    "# \tlabels = loader[2]\n",
    "# \tprint('training set shape:', train_loader.dataset.shape)\n",
    "# \tprint('test set shape:', test_loader.dataset.shape)\n",
    "# \treturn train_loader, test_loader, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting import plot_ascore, plot_labels, plot_metrics, compare_labels\n",
    "from main import backprop\n",
    "from src.pot import pot_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replotting(model_name, n_windows, dataset='SMAP', N=25):\n",
    "    for i, elem in enumerate(n_windows):\n",
    "        # path = f'studies/{model_name}_{dataset}_2/n_window{elem}/results'\n",
    "        model_path = f'studies/{model_name}_{dataset}_2/n_window{elem}/checkpoints/model.ckpt'\n",
    "        plot_path = f'studies/{model_name}_{dataset}_2/n_window{elem}/plots'\n",
    "\n",
    "        # initialise model etc\n",
    "        checkpoint = torch.load(model_path)\n",
    "        if model_name == 'TranAD':\n",
    "            model = TranAD(N, elem).double()\n",
    "        elif model_name == 'iTransformer':\n",
    "            model = iTransformer(N, elem).double()\n",
    "        else:\n",
    "            print('pbm with model loading')\n",
    "            break\n",
    "        optimizer = torch.optim.AdamW(model.parameters() , lr=model.lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.9)\n",
    "        # load model etc\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        # epoch = checkpoint['epoch']\n",
    "        # accuracy_list = checkpoint['accuracy_list']\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'total params: {total_params}, trainable params: {trainable_params}')\n",
    "\n",
    "        train_loader, test_loader, labels = load_dataset(dataset)\n",
    "        trainD, testD = next(iter(train_loader)), next(iter(test_loader))\n",
    "        trainO, testO = trainD, testD\n",
    "        # prepare data\n",
    "        if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN', 'iTransformer'] or 'TranAD' in model.name: \n",
    "            trainD, testD = convert_to_windows(trainD, model), convert_to_windows(testD, model)\n",
    "        \n",
    "        ### Testing phase\n",
    "        torch.zero_grad = True\n",
    "        model.eval()\n",
    "        lossT, _ = backprop(0, model, trainD, trainO, optimizer, scheduler, training=False)  # training loss for POT\n",
    "        loss, y_pred = backprop(0, model, testD, testO, optimizer, scheduler, training=False)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(loss.shape[1]):\n",
    "            lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]\n",
    "            result, pred = pot_eval(lt, l, ls, plot_path, f'dim{i}')\n",
    "            preds.append(pred)\n",
    "            df_res = pd.DataFrame.from_dict(result, orient='index').T\n",
    "        lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "        labelsFinal = (np.sum(labels, axis=1) >= 1) + 0\n",
    "        preds = np.array(preds).T\n",
    "        preds = preds.astype(int)\n",
    "        df_labels = pd.DataFrame(preds)\n",
    "        labelspred = (np.sum(preds, axis=1) >= 1) + 0\n",
    "        print(loss.shape, labelsFinal.shape)\n",
    "        \n",
    "        plot_ascore(plot_path, 'ascore_local', loss, labelsFinal)\n",
    "        plot_labels(plot_path, 'labels_local', labelspred, labelsFinal)\n",
    "        plot_metrics(plot_path, 'metrics_local', labelspred, labelsFinal)\n",
    "\n",
    "        result2, pred2 = pot_eval(lossTfinal, lossFinal, labelsFinal, plot_path, f'all_dim')\n",
    "        labelspred2 = (pred2 >= 1) + 0\n",
    "        plot_ascore(plot_path, 'ascore_global', lossFinal, labelsFinal)\n",
    "        plot_labels(plot_path, 'labels_global', labelspred2, labelsFinal)\n",
    "        plot_metrics(plot_path, 'metrics_global', labelspred2, labelsFinal)\n",
    "        \n",
    "        # arr = np.where(labelspred!=labelspred2)\n",
    "        # print(len(arr[0]), len(np.where(labelspred==labelspred2)[0]))\n",
    "\n",
    "        compare_labels(plot_path, labels_loc=labelspred, labels_glob=labelspred2, labels=labelsFinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replotting('TranAD', TranAD_n_windows, dataset, 25)\n",
    "replotting('iTransformer', iTransf_n_windows, dataset, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply POT directly to ATLAS time series features without applying any model before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from main import load_dataset\n",
    "from src.pot import calc_point2point\n",
    "from src.pot import pot_eval\n",
    "from src.plotting import plot_ascore, plot_labels, plot_metrics, compare_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'IEEECIS_pca_scaled'  # 'ATLAS_TS'\n",
    "res_path = f'None_{dataset}/feats150'\n",
    "plot_path = res_path\n",
    "os.makedirs(plot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, label, _ = load_dataset(dataset, feats=150)\n",
    "train, test = next(iter(train_loader)), next(iter(test_loader))\n",
    "print(train.shape, test.shape, label.shape)\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "labels = np.array(label)\n",
    "print(train.shape, test.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossT = np.abs(train)\n",
    "loss = np.abs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local anomaly labels\n",
    "df_res_local = pd.DataFrame()\n",
    "preds = []\n",
    "for i in range(loss.shape[1]):\n",
    "    lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]  \n",
    "    result_local, pred = pot_eval(lt, l, ls, plot_path, f'dim{i}', q=1e-5)\n",
    "    preds.append(pred)\n",
    "    df_res = pd.DataFrame.from_dict(result_local, orient='index').T\n",
    "    df_res_local = pd.concat([df_res_local, df_res], ignore_index=True)\n",
    "lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "true_labels = (np.sum(labels, axis=1) >= 1) + 0\n",
    "preds = np.array(preds).T\n",
    "preds = preds.astype(int)\n",
    "labelspred = (np.sum(preds, axis=1) >= 1) + 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ascore(plot_path, 'ascore_local', ascore=loss, labels=true_labels)\n",
    "plot_labels(plot_path, 'labels_local', y_pred=labelspred, y_true=true_labels)\n",
    "result_local = calc_point2point(predict=labelspred, actual=true_labels)\n",
    "result_local1 = {'f1': result_local[0], 'precision': result_local[1], 'recall': result_local[2], \n",
    "                'TP': result_local[3], 'TN': result_local[4], 'FP': result_local[5], 'FN': result_local[6], \n",
    "                'ROC/AUC': result_local[7], 'MCC': result_local[8]}\n",
    "print('local results')\n",
    "print(result_local1)\n",
    "\n",
    "# do majority voting over dimensions for local results instead of inclusive OR\n",
    "majority = math.ceil(labels.shape[1] / 2)\n",
    "labelspred_maj = (np.sum(preds, axis=1) >= majority) + 0\n",
    "plot_labels(plot_path, 'labels_local_maj', y_pred=labelspred_maj, y_true=true_labels)\n",
    "result_local = calc_point2point(predict=labelspred_maj, actual=true_labels)\n",
    "result_local2 = {'f1': result_local[0], 'precision': result_local[1], 'recall': result_local[2], \n",
    "                'TP': result_local[3], 'TN': result_local[4], 'FP': result_local[5], 'FN': result_local[6], \n",
    "                'ROC/AUC': result_local[7], 'MCC': result_local[8]}\n",
    "print('\\nlocal results with majority voting')\n",
    "print(result_local2)\n",
    "temp = np.where(labelspred_maj != true_labels)\n",
    "print(temp, np.all(labelspred_maj == true_labels))\n",
    "\n",
    "# global anomaly labels\n",
    "result_global, pred2 = pot_eval(lossTfinal, lossFinal, true_labels, plot_path, f'all_dim', q=1e-5)\n",
    "labelspred_glob = (pred2 >= 1) + 0\n",
    "plot_ascore(plot_path, 'ascore_global', ascore=lossFinal, labels=true_labels)\n",
    "plot_labels(plot_path, 'labels_global', y_pred=labelspred_glob, y_true=true_labels)\n",
    "metrics_global = calc_point2point(predict=labelspred_glob, actual=true_labels)\n",
    "print('\\nglobal results') \n",
    "print(result_global)\n",
    "\n",
    "plot_metrics(plot_path, ['local (incl. OR)', 'local (maj. voting)', 'global'], \n",
    "\t\t\t  y_pred=[labelspred, labelspred_maj, labelspred_glob], y_true=true_labels)\n",
    "\n",
    "# compare local & global anomaly labels\n",
    "compare_labels(plot_path, pred_labels=[labelspred, labelspred_maj], true_labels=true_labels, \n",
    "            plot_labels=['Local anomaly\\n(inclusive OR)', 'Local anomaly\\n(majority voting)'], name='_loc_vs_maj')\n",
    "compare_labels(plot_path, pred_labels=[labelspred, labelspred_maj, labelspred_glob], true_labels=true_labels, \n",
    "            plot_labels=['Local anomaly\\n(inclusive OR)', 'Local anomaly\\n(majority voting)', 'Global anomaly'], name='_all')\n",
    "\n",
    "# saving results\n",
    "df_res_global = pd.DataFrame.from_dict(result_global, orient='index').T\n",
    "df_res_global.index = ['global']\n",
    "result_local1 = pd.DataFrame.from_dict(result_local1, orient='index').T\n",
    "result_local2 = pd.DataFrame.from_dict(result_local2, orient='index').T\n",
    "result_local1.index = ['local_all']\n",
    "result_local2.index = ['local_all_maj']\n",
    "df_res_local = pd.concat([df_res_local, result_local1, result_local2])\n",
    "df_res = pd.concat([df_res_local, df_res_global]) \n",
    "df_labels = pd.DataFrame( {'local': labelspred, 'local_maj': labelspred_maj, 'global': labelspred_glob} )\n",
    "\n",
    "df_res.to_csv(f'{res_path}/res.csv')\t\n",
    "df_labels.to_csv(f'{res_path}/pred_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
