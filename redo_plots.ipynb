{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import mplhep as hep\n",
    "from src.models import TranAD, iTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from src.data_loader import convert_to_windows_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['axes.prop_cycle'] = cycler('color', ['#17becf','#8c564b', '#e377c2', '#7f7f7f','#bcbd22', '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redo older plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iTransf_n_windows = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000, 2000]\n",
    "TranAD_n_windows = [10, 20, 30, 40, 50, 60, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "dataset = 'SMAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(dataset):\n",
    "# \tloader = []\n",
    "# \tfor file in ['train', 'test', 'labels']:\n",
    "# \t\tif dataset == 'SMD': file = 'machine-1-1_' + file\n",
    "# \t\tif dataset == 'SMAP': file = 'P-1_' + file\n",
    "# \t\tif dataset == 'MSL': file = 'C-1_' + file\n",
    "# \t\tif dataset == 'UCR': file = '136_' + file\n",
    "# \t\tif dataset == 'NAB': file = 'ec2_request_latency_system_failure_' + file\n",
    "# \t\tloader.append(np.load(os.path.join(f'processed/{dataset}', f'{file}.npy')))\n",
    "\t\n",
    "# \ttrain_loader = DataLoader(loader[0], batch_size=loader[0].shape[0])\n",
    "# \ttest_loader = DataLoader(loader[1], batch_size=loader[1].shape[0])\n",
    "# \tlabels = loader[2]\n",
    "# \tprint('training set shape:', train_loader.dataset.shape)\n",
    "# \tprint('test set shape:', test_loader.dataset.shape)\n",
    "# \treturn train_loader, test_loader, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting import plot_ascore, plot_labels, plot_metrics, compare_labels\n",
    "from main import backprop\n",
    "from src.pot import pot_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replotting(model_name, n_windows, dataset='SMAP', N=25):\n",
    "    for i, elem in enumerate(n_windows):\n",
    "        # path = f'studies/{model_name}_{dataset}_2/n_window{elem}/results'\n",
    "        model_path = f'studies/{model_name}_{dataset}_2/n_window{elem}/checkpoints/model.ckpt'\n",
    "        plot_path = f'studies/{model_name}_{dataset}_2/n_window{elem}/plots'\n",
    "\n",
    "        # initialise model etc\n",
    "        checkpoint = torch.load(model_path)\n",
    "        if model_name == 'TranAD':\n",
    "            model = TranAD(N, elem).double()\n",
    "        elif model_name == 'iTransformer':\n",
    "            model = iTransformer(N, elem).double()\n",
    "        else:\n",
    "            print('pbm with model loading')\n",
    "            break\n",
    "        optimizer = torch.optim.AdamW(model.parameters() , lr=model.lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.9)\n",
    "        # load model etc\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        # epoch = checkpoint['epoch']\n",
    "        # accuracy_list = checkpoint['accuracy_list']\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'total params: {total_params}, trainable params: {trainable_params}')\n",
    "\n",
    "        train_loader, test_loader, labels = load_dataset(dataset)\n",
    "        trainD, testD = next(iter(train_loader)), next(iter(test_loader))\n",
    "        trainO, testO = trainD, testD\n",
    "        # prepare data\n",
    "        if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN', 'iTransformer'] or 'TranAD' in model.name: \n",
    "            trainD, testD = convert_to_windows(trainD, model), convert_to_windows(testD, model)\n",
    "        \n",
    "        ### Testing phase\n",
    "        torch.zero_grad = True\n",
    "        model.eval()\n",
    "        lossT, _ = backprop(0, model, trainD, trainO, optimizer, scheduler, training=False)  # training loss for POT\n",
    "        loss, y_pred = backprop(0, model, testD, testO, optimizer, scheduler, training=False)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(loss.shape[1]):\n",
    "            lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]\n",
    "            result, pred = pot_eval(lt, l, ls, plot_path, f'dim{i}')\n",
    "            preds.append(pred)\n",
    "            df_res = pd.DataFrame.from_dict(result, orient='index').T\n",
    "        lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "        labelsFinal = (np.sum(labels, axis=1) >= 1) + 0\n",
    "        preds = np.array(preds).T\n",
    "        preds = preds.astype(int)\n",
    "        df_labels = pd.DataFrame(preds)\n",
    "        labelspred = (np.sum(preds, axis=1) >= 1) + 0\n",
    "        print(loss.shape, labelsFinal.shape)\n",
    "        \n",
    "        plot_ascore(plot_path, 'ascore_local', loss, labelsFinal)\n",
    "        plot_labels(plot_path, 'labels_local', labelspred, labelsFinal)\n",
    "        plot_metrics(plot_path, 'metrics_local', labelspred, labelsFinal)\n",
    "\n",
    "        result2, pred2 = pot_eval(lossTfinal, lossFinal, labelsFinal, plot_path, f'all_dim')\n",
    "        labelspred2 = (pred2 >= 1) + 0\n",
    "        plot_ascore(plot_path, 'ascore_global', lossFinal, labelsFinal)\n",
    "        plot_labels(plot_path, 'labels_global', labelspred2, labelsFinal)\n",
    "        plot_metrics(plot_path, 'metrics_global', labelspred2, labelsFinal)\n",
    "        \n",
    "        # arr = np.where(labelspred!=labelspred2)\n",
    "        # print(len(arr[0]), len(np.where(labelspred==labelspred2)[0]))\n",
    "\n",
    "        compare_labels(plot_path, labels_loc=labelspred, labels_glob=labelspred2, labels=labelsFinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replotting('TranAD', TranAD_n_windows, dataset, 25)\n",
    "replotting('iTransformer', iTransf_n_windows, dataset, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply POT directly to ATLAS time series features without applying any model before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from main import load_dataset\n",
    "from src.pot import calc_point2point\n",
    "from src.pot import pot_eval\n",
    "from src.plotting import plot_ascore, plot_labels, plot_metrics, compare_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'GECCO'\n",
    "feats = -1\n",
    "res_path = f'None/None_{dataset}/feats{feats}'\n",
    "plot_path = res_path\n",
    "os.makedirs(plot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: (83739, 9)\n",
      "test set shape: (55827, 9)\n",
      "labels shape: (55827, 9)\n",
      "ts_lengths 0: 83739\n",
      "ts_lengths 1: 55827\n",
      "torch.Size([83739, 9]) torch.Size([55827, 9]) (55827, 9)\n",
      "(83739, 9) (55827, 9) (55827, 9)\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, label, _ = load_dataset(dataset, feats=feats, less=False)\n",
    "train, test = next(iter(train_loader)), next(iter(test_loader))\n",
    "print(train.shape, test.shape, label.shape)\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "labels = np.array(label)\n",
    "print(train.shape, test.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossT = np.abs(train)\n",
    "loss = np.abs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local anomaly labels\n",
    "df_res_local = pd.DataFrame()\n",
    "preds = []\n",
    "for i in range(loss.shape[1]):\n",
    "    lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]  \n",
    "    result_local, pred = pot_eval(lt, l, ls, plot_path, f'dim{i}', q=1e-5)\n",
    "    preds.append(pred)\n",
    "    df_res = pd.DataFrame.from_dict(result_local, orient='index').T\n",
    "    df_res_local = pd.concat([df_res_local, df_res], ignore_index=True)\n",
    "lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)\n",
    "true_labels = (np.sum(labels, axis=1) >= 1) + 0\n",
    "preds = np.array(preds).T\n",
    "preds = preds.astype(int)\n",
    "labelspred = (np.sum(preds, axis=1) >= 1) + 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local results\n",
      "{'f1': 0.011370215364927579, 'precision': 0.005717670105078095, 'recall': 0.9999999601593641, 'TP': 251, 'TN': 11928, 'FP': 43648, 'FN': 0, 'ROC/AUC': 0.6073125089966892, 'MCC': 0.03503077289273145}\n",
      "\n",
      "local results with majority voting\n",
      "{'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'TP': 0, 'TN': 55576, 'FP': 0, 'FN': 251, 'ROC/AUC': 0.5, 'MCC': 0.0}\n",
      "(array([ 7828,  7829,  7830,  7831,  7832,  7833,  7834,  7835,  7836,\n",
      "        7837,  7838,  7839,  7840,  7841,  7842,  7843,  7844,  7845,\n",
      "        7846,  7847,  7848,  7849,  7850,  7851,  7852,  7853,  7854,\n",
      "        7855,  7856,  9027,  9028,  9029,  9030,  9031,  9032,  9033,\n",
      "        9034,  9035,  9036,  9037,  9038,  9039,  9040,  9041,  9042,\n",
      "        9043,  9044,  9045,  9046,  9047,  9048,  9049,  9050,  9051,\n",
      "        9052,  9053,  9054,  9055, 10324, 10325, 10326, 10327, 10328,\n",
      "       10329, 10330, 10331, 10332, 10333, 10334, 10335, 10336, 10337,\n",
      "       10338, 10339, 10340, 10341, 10342, 10343, 10344, 10345, 10346,\n",
      "       10347, 10348, 10349, 10350, 10351, 10352, 11340, 11341, 11342,\n",
      "       11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351,\n",
      "       11352, 11353, 11354, 11355, 11356, 11357, 11358, 11359, 11360,\n",
      "       11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 12355,\n",
      "       12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364,\n",
      "       12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373,\n",
      "       12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382,\n",
      "       12383, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600,\n",
      "       13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609,\n",
      "       13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618,\n",
      "       13619, 13620, 13621, 48534, 48535, 48536, 48537, 48538, 48539,\n",
      "       48540, 48541, 48542, 48543, 48544, 48545, 48546, 48547, 48548,\n",
      "       48549, 48550, 48551, 48552, 48553, 48554, 48555, 48556, 48557,\n",
      "       48558, 48559, 48560, 48561, 48562, 48563, 48564, 48565, 48566,\n",
      "       48567, 48568, 48569, 48570, 48571, 48572, 48573, 48574, 48575,\n",
      "       48576, 48577, 48578, 48579, 48580, 48581, 48582, 48583, 48584,\n",
      "       48585, 48586, 48587, 48588, 48589, 48590, 48591, 48592, 48593,\n",
      "       48594, 48595, 48596, 48597, 48598, 48599, 48600, 48601, 48602,\n",
      "       48603, 48604, 48605, 48606, 48607, 48608, 48609, 48610]),) False\n",
      "\n",
      "global results\n",
      "{'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'TP': 0, 'TN': 55575, 'FP': 1, 'FN': 251, 'ROC/AUC': 0.4999910033107816, 'MCC': -0.00028442971241615493, 'threshold': 2.168676442346598}\n"
     ]
    }
   ],
   "source": [
    "plot_ascore(plot_path, 'ascore_local', ascore=loss, labels=true_labels)\n",
    "plot_labels(plot_path, 'labels_local', y_pred=labelspred, y_true=true_labels)\n",
    "result_local = calc_point2point(predict=labelspred, actual=true_labels)\n",
    "result_local1 = {'f1': result_local[0], 'precision': result_local[1], 'recall': result_local[2], \n",
    "                'TP': result_local[3], 'TN': result_local[4], 'FP': result_local[5], 'FN': result_local[6], \n",
    "                'ROC/AUC': result_local[7], 'MCC': result_local[8]}\n",
    "print('local results')\n",
    "print(result_local1)\n",
    "\n",
    "# do majority voting over dimensions for local results instead of inclusive OR\n",
    "majority = math.ceil(labels.shape[1] / 2)\n",
    "labelspred_maj = (np.sum(preds, axis=1) >= majority) + 0\n",
    "plot_labels(plot_path, 'labels_local_maj', y_pred=labelspred_maj, y_true=true_labels)\n",
    "result_local = calc_point2point(predict=labelspred_maj, actual=true_labels)\n",
    "result_local2 = {'f1': result_local[0], 'precision': result_local[1], 'recall': result_local[2], \n",
    "                'TP': result_local[3], 'TN': result_local[4], 'FP': result_local[5], 'FN': result_local[6], \n",
    "                'ROC/AUC': result_local[7], 'MCC': result_local[8]}\n",
    "print('\\nlocal results with majority voting')\n",
    "print(result_local2)\n",
    "temp = np.where(labelspred_maj != true_labels)\n",
    "print(temp, np.all(labelspred_maj == true_labels))\n",
    "\n",
    "# global anomaly labels\n",
    "result_global, pred2 = pot_eval(lossTfinal, lossFinal, true_labels, plot_path, f'all_dim', q=1e-5)\n",
    "labelspred_glob = (pred2 >= 1) + 0\n",
    "plot_ascore(plot_path, 'ascore_global', ascore=lossFinal, labels=true_labels)\n",
    "plot_labels(plot_path, 'labels_global', y_pred=labelspred_glob, y_true=true_labels)\n",
    "metrics_global = calc_point2point(predict=labelspred_glob, actual=true_labels)\n",
    "print('\\nglobal results') \n",
    "print(result_global)\n",
    "\n",
    "plot_metrics(plot_path, ['local (incl. OR)', 'local (maj. voting)', 'global'], \n",
    "\t\t\t  y_pred=[labelspred, labelspred_maj, labelspred_glob], y_true=true_labels)\n",
    "\n",
    "# compare local & global anomaly labels\n",
    "compare_labels(plot_path, pred_labels=[labelspred, labelspred_maj], true_labels=true_labels, \n",
    "            plot_labels=['Local anomaly\\n(inclusive OR)', 'Local anomaly\\n(majority voting)'], name='_loc_vs_maj')\n",
    "compare_labels(plot_path, pred_labels=[labelspred, labelspred_maj, labelspred_glob], true_labels=true_labels, \n",
    "            plot_labels=['Local anomaly\\n(inclusive OR)', 'Local anomaly\\n(majority voting)', 'Global anomaly'], name='_all')\n",
    "\n",
    "# saving results\n",
    "df_res_global = pd.DataFrame.from_dict(result_global, orient='index').T\n",
    "df_res_global.index = ['global']\n",
    "result_local1 = pd.DataFrame.from_dict(result_local1, orient='index').T\n",
    "result_local2 = pd.DataFrame.from_dict(result_local2, orient='index').T\n",
    "result_local1.index = ['local_all']\n",
    "result_local2.index = ['local_all_maj']\n",
    "df_res_local = pd.concat([df_res_local, result_local1, result_local2])\n",
    "df_res = pd.concat([df_res_local, df_res_global]) \n",
    "df_labels = pd.DataFrame( {'local': labelspred, 'local_maj': labelspred_maj, 'global': labelspred_glob} )\n",
    "\n",
    "df_res.to_csv(f'{res_path}/res.csv')\t\n",
    "df_labels.to_csv(f'{res_path}/pred_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test Isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from main import load_dataset\n",
    "from src.pot import calc_point2point\n",
    "from src.pot import pot_eval\n",
    "from src.plotting import plot_ascore, plot_labels, plot_metrics, compare_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'GECCO'\n",
    "feats = -1\n",
    "contamination = 0.01\n",
    "res_path = f'IF/IF_{dataset}/feats{feats}'\n",
    "plot_path = f'{res_path}/plots'\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "res_path = f'{res_path}/results'\n",
    "os.makedirs(res_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: (83739, 9)\n",
      "test set shape: (55827, 9)\n",
      "labels shape: (55827, 9)\n",
      "ts_lengths 0: 83739\n",
      "ts_lengths 1: 55827\n",
      "torch.Size([83739, 9]) torch.Size([55827, 9]) (55827, 9)\n",
      "(83739, 9) (55827, 9) (55827, 9)\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, labels, _ = load_dataset(dataset, feats=feats)\n",
    "train, test = next(iter(train_loader)), next(iter(test_loader))\n",
    "print(train.shape, test.shape, labels.shape)\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "labels = np.array(labels)\n",
    "print(train.shape, test.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040464291471868453 2259\n"
     ]
    }
   ],
   "source": [
    "# print(len(labels[:,0][labels[:,0]==1])/len(labels[:,0]))\n",
    "print(len(labels[labels==1])/len(labels), len(labels[labels==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 53812\n",
      "(55827,)\n",
      "(83739,) (55827,)\n"
     ]
    }
   ],
   "source": [
    "# isolation forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(contamination=contamination, random_state=0).fit(train)\n",
    "lossTfinal = clf.decision_function(train)\n",
    "lossFinal = clf.decision_function(test)\n",
    "labelspredIF = clf.predict(test)\n",
    "labelspredIF = (labelspredIF == -1) + 0\n",
    "print(len(labelspredIF[labelspredIF == 1]), len(labelspredIF[labelspredIF == 0]))\n",
    "print(labelspredIF.shape)\n",
    "print(lossTfinal.shape, lossFinal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = (np.sum(labels, axis=1) >= 1) + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF results\n",
      "{'f1': 0.03795036872341088, 'precision': 0.021339950266302976, 'recall': 0.17131473421056836, 'TP': 43, 'TN': 53604, 'FP': 1972, 'FN': 208, 'ROC/AUC': 0.5679158993793073, 'MCC': 0.04871972896281428}\n",
      "\n",
      "global results\n",
      "{'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'TP': 0, 'TN': 55576, 'FP': 0, 'FN': 251, 'ROC/AUC': 0.5, 'MCC': 0.0, 'threshold': 0.21992938575143553}\n"
     ]
    }
   ],
   "source": [
    "plot_labels(plot_path, 'labels_local', y_pred=labelspredIF, y_true=true_labels)\n",
    "metrics_IF = calc_point2point(predict=labelspredIF, actual=true_labels)\n",
    "result_IF = {'f1': metrics_IF[0], 'precision': metrics_IF[1], 'recall': metrics_IF[2],\n",
    "            'TP': metrics_IF[3], 'TN': metrics_IF[4], 'FP': metrics_IF[5], 'FN': metrics_IF[6],\n",
    "            'ROC/AUC': metrics_IF[7], 'MCC': metrics_IF[8]}\n",
    "print('IF results')\n",
    "print(result_IF)\n",
    "df_res_IF = pd.DataFrame.from_dict(result_IF, orient='index').T\n",
    "\n",
    "\n",
    "# global anomaly labels\n",
    "result_global, pred2 = pot_eval(lossTfinal, lossFinal, true_labels, plot_path, f'all_dim', q=1e-5)\n",
    "labelspred_glob = (pred2 >= 1) + 0\n",
    "plot_ascore(plot_path, 'ascore_global', ascore=lossFinal, labels=true_labels)\n",
    "plot_labels(plot_path, 'labels_global', y_pred=labelspred_glob, y_true=true_labels)\n",
    "metrics_global = calc_point2point(predict=labelspred_glob, actual=true_labels)\n",
    "print('\\nglobal results') \n",
    "print(result_global)\n",
    "\n",
    "plot_metrics(plot_path, ['IF', 'global'], \n",
    "\t\t\t  y_pred=[labelspredIF, labelspred_glob], y_true=true_labels)\n",
    "\n",
    "# compare local & global anomaly labels\n",
    "compare_labels(plot_path, pred_labels=[labelspredIF, labelspred_glob], true_labels=true_labels, \n",
    "            plot_labels=['Anomaly from IF', 'Global anomaly'], name='_all')\n",
    "\n",
    "# saving results\n",
    "df_res_global = pd.DataFrame.from_dict(result_global, orient='index').T\n",
    "df_res_global.index = ['global']\n",
    "df_res = pd.concat([df_res_IF, df_res_global]) \n",
    "df_labels = pd.DataFrame( {'IF': labelspredIF, 'global': labelspred_glob} )\n",
    "\n",
    "df_res.to_csv(f'{res_path}/res.csv')\t\n",
    "df_labels.to_csv(f'{res_path}/pred_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
