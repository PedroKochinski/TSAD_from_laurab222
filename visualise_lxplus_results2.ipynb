{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TranAD, iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([hep.style.ROOT, hep.style.firamath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for local results (sometimes reduced data sets)\n",
    "config = {\n",
    " 'iTransformer1': {'window': 10, 'steps': 1, 'latent': 2, 'eps': 5, 'lab': 'iTransformer: w10 s1 l2'},\n",
    " 'iTransformer2': {'window': 100, 'steps': 50, 'latent': 2, 'eps': 5, 'lab': 'iTransformer: w100 s50 l2'},\n",
    " 'iTransformer3': {'window': 10, 'steps': 5, 'latent': 2, 'eps': 5, 'lab': 'iTransformer: w10 s5 l2'},\n",
    " 'iTransformer4': {'window': 100, 'steps': 10, 'latent': 2, 'eps': 5, 'lab': 'iTransformer w100 s10 l2'},\n",
    " 'TranAD': {'window': 10, 'steps': 1, 'eps': 5, 'latent': '', 'lab': 'TranAD: w10 s1'},\n",
    " 'MAD_GAN': {'lab': 'MAD_GAN'},\n",
    " 'OmniAnomaly': {'lab': 'OmniAnomaly'},\n",
    " 'LSTM_AE': {'lab': 'LSTM_AE'},\n",
    " 'DAGMM': {'lab': 'DAGMM'},\n",
    " 'USAD': {'lab': 'USAD'},\n",
    " 'IF': {'lab': 'IF'},\n",
    " 'None': {'lab': 'None'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # configs for lxplus results\n",
    "# config = {\n",
    "#  'iTransformer1': {'window': 10, 'steps': 1, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w10 s1 l2'},\n",
    "#  'iTransformer2': {'window': 100, 'steps': 50, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w100 s50 l2'},\n",
    "#  'iTransformer3': {'window': 10, 'steps': 5, 'latent': 2, 'eps': 100, 'lab': 'iTransformer: w10 s5 l2'},\n",
    "#  'iTransformer4': {'window': 100, 'steps': 10, 'latent': 2, 'eps': 100, 'lab': 'iTransformer w100 s10 l2'},\n",
    "#  'TranAD': {'window': 10, 'steps': 1, 'eps': 100, 'latent': '', 'lab': 'TranAD: w10 s1'},\n",
    "# #  'MAD_GAN': {'lab': 'MAD_GAN'},\n",
    "# #  'OmniAnomaly': {'lab': 'OmniAnomaly'},\n",
    "# #  'LSTM_AE': {'lab': 'LSTM_AE'},\n",
    "# #  'DAGMM': {'lab': 'DAGMM'},\n",
    "# #  'USAD': {'lab': 'USAD'},\n",
    "# #  'IF': {'lab': 'IF'},\n",
    "# #  'None': {'lab': 'None'}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['IEEECIS_new2.2', 'SMAP_new', 'MSL_new', 'UCR', 'SMD', 'SWaT_1D', 'GECCO', 'SMD']  #, 'ATLAS_TS']\n",
    "models = ['iTransformer1', 'iTransformer2', 'TranAD']  # 'iTransformer3', 'iTransformer4', 'TranAD']\n",
    "        #   'OmniAnomaly', 'MAD_GAN', 'LSTM_AE', 'DAGMM', 'USAD', 'IF', 'None'] \n",
    "\n",
    "all_paths = []\n",
    "results_mean_std = {}\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        if 'iTransformer' in model and model != 'iTransformer':\n",
    "            paths = glob.glob(f'iTransformer_reduced_data/iTransformer_{dataset}')\n",
    "            # paths = glob.glob(f'iTransformer_results_lxplus/iTransformer_{dataset}')\n",
    "        else:\n",
    "            paths = glob.glob(f'{model}_reduced_data/{model}_{dataset}')\n",
    "            # paths = glob.glob(f'{model}_results_lxplus/{model}_{dataset}')\n",
    "        all_paths.extend(paths)\n",
    "        if not paths:\n",
    "            print(f'No paths found for {model} on {dataset}')\n",
    "        feats = 30 if dataset == 'IEEECIS_new2.2' else -1\n",
    "\n",
    "        for path in paths:\n",
    "            if model in config.keys():\n",
    "                if model == 'TranAD':\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}*feats{feats}*/results/res.csv\")\n",
    "                else:\n",
    "                    res_path = glob.glob(f\"{path}/*n_window{config[model]['window']}_steps{config[model]['steps']}_feats{feats}_eps{config[model]['eps']}_latent{config[model]['latent']}*/results/res.csv\")\n",
    "            elif model == 'None':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            elif model == 'IF':\n",
    "                res_path = glob.glob(f'{path}/*feats{feats}*/results/res.csv')\n",
    "            else:\n",
    "                res_path = glob.glob(f'{path}/*n_window10_steps1*feats{feats}*/results/res.csv')\n",
    "\n",
    "            if res_path:\n",
    "                res_path = np.sort(res_path)\n",
    "                print(model, dataset, len(res_path))\n",
    "                # print(res_path)\n",
    "                tmp = pd.DataFrame()\n",
    "                for p in res_path:\n",
    "                    res = pd.read_csv(p)\n",
    "                    tmp = pd.concat((tmp, res.iloc[-3:]))\n",
    "\n",
    "                key = path.split('/')[1]\n",
    "                if 'iTransformer' in model and model != 'iTransformer':\n",
    "                    idx = len('iTransformer')  # len(model) - 1\n",
    "                    diff = len(model) - len('iTransformer')\n",
    "                    # insert a number in the key to distinguish between the models at position idx\n",
    "                    key = key[:idx] + model[-diff:] + key[idx:]\n",
    "\n",
    "                mean_values = tmp.groupby('Unnamed: 0').mean()\n",
    "                std_values = tmp.groupby('Unnamed: 0').std()\n",
    "                mean_values = mean_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "                std_values = std_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "                results_mean_std[key] = {'mean': mean_values, 'std': std_values}\n",
    "                # print(results_mean_std[key])\n",
    "            else:\n",
    "                print(f'No results found for {model} on {dataset}')\n",
    "\n",
    "            break\n",
    "\n",
    "# print(len(all_paths))\n",
    "print(results_mean_std.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'lines.markersize': 6})\n",
    "plt.rcParams.update({'errorbar.capsize': 8})\n",
    "plt.rcParams.update({'lines.linewidth': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['local (incl. OR)', 'local (maj. voting)', 'global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_mean_std_err(results, modes, datasets, models, metric='MCC', name=None, labels=None):\n",
    "    # colors = plt.cm.plasma(np.linspace(0, 1, len(models)+1))\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(22, 16), sharex=True, sharey=True)\n",
    "\n",
    "    for i, mode in enumerate(modes):\n",
    "        for j, model in enumerate(models):\n",
    "            scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "            for dataset in datasets:\n",
    "                key = f'{model}_{dataset}'\n",
    "               \n",
    "                for val in ['mean', 'std']:\n",
    "                    if key in results:\n",
    "                        scores[val] = np.append(scores[val], results[key][val][metric].iloc[i])\n",
    "                    else:\n",
    "                        scores[val] = np.append(scores[val], 0)\n",
    "            \n",
    "            x_positions = np.arange(len(datasets)) + j * 0.1  # add offset for each model\n",
    "            if labels:\n",
    "                axs[i].errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=labels[j])  #, color=colors[j % len(colors)], capsize=5)\n",
    "            else:\n",
    "                axs[i].errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=model)  #, color=colors[j % len(colors)], capsize=5)\n",
    "            \n",
    "            axs[i].set_xticks(np.arange(len(datasets)) + 0.1 * (len(models) - 1) / 2)\n",
    "            axs[i].set_xticklabels(labels=datasets)\n",
    "            if metric == 'MCC':\n",
    "                axs[i].set_ylim(-1, 1)\n",
    "            else:\n",
    "                axs[i].set_ylim(top=1.0)\n",
    "            if metric == 'ROC/AUC':\n",
    "                axs[i].set_ylabel('ROC AUC')\n",
    "            elif metric == 'f1':\n",
    "                axs[i].set_ylabel('F1')\n",
    "            else:\n",
    "                axs[i].set_ylabel(metric)\n",
    "            axs[i].set_title(mode)\n",
    "        axs[i].legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        if metric == 'ROC/AUC':\n",
    "            metric = 'rocauc'\n",
    "        plt.savefig(f'./studies_results_lxplus/{name}_{metric}.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_mean_std_err2(results, modes, datasets, models, metric='MCC', name=None, labels=None):\n",
    "    num_datasets = len(datasets)\n",
    "    num_subplots = (num_datasets + 2) // 3  # Calculate the number of subplots needed\n",
    "    fig, axs = plt.subplots(num_subplots, 1, figsize=(22, 8 * num_subplots), sharey=True)\n",
    "    if num_subplots == 1:\n",
    "        axs = [axs]  # Make axs iterable if it's a single subplot\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(models)+1))\n",
    "    markers = ['o', 's', 'D']  # Different markers for different modes\n",
    "    plt.rcParams.update({'lines.markersize': 8})\n",
    "\n",
    "    width = 0.05  # the width of the bars\n",
    "    x = np.arange(num_datasets)  # the label locations\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        for i, mode in enumerate(modes):\n",
    "            scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "            for dataset in datasets:\n",
    "                key = f'{model}_{dataset}'\n",
    "                for val in ['mean', 'std']:\n",
    "                    if key in results:\n",
    "                        scores[val] = np.append(scores[val], results[key][val][metric].iloc[i])\n",
    "                    else:\n",
    "                        scores[val] = np.append(scores[val], 0)\n",
    "\n",
    "            x_positions = x + (j * len(modes) + i) * width  # add offset for each mode and model\n",
    "            if labels:\n",
    "                for k in range(num_subplots):\n",
    "                    start_idx = k * 3\n",
    "                    end_idx = min((k + 1) * 3, num_datasets)\n",
    "                    axs[k].errorbar(x_positions[start_idx:end_idx], scores['mean'][start_idx:end_idx], yerr=scores['std'][start_idx:end_idx], fmt=markers[i], label=f'{labels[j]} ({mode})', color=colors[j])\n",
    "            else:\n",
    "                for k in range(num_subplots):\n",
    "                    start_idx = k * 3\n",
    "                    end_idx = min((k + 1) * 3, num_datasets)\n",
    "                    axs[k].errorbar(x_positions[start_idx:end_idx], scores['mean'][start_idx:end_idx], yerr=scores['std'][start_idx:end_idx], fmt=markers[i], label=f'{model} ({mode})', color=colors[j])\n",
    "\n",
    "    for k in range(num_subplots):\n",
    "        start_idx = k * 3\n",
    "        end_idx = min((k + 1) * 3, num_datasets)\n",
    "        axs[k].set_xticks(x[start_idx:end_idx] + (len(modes) * len(models) - 1) * width / 2)\n",
    "        axs[k].set_xticklabels(labels=datasets[start_idx:end_idx])\n",
    "        if metric == 'MCC':\n",
    "            axs[k].set_ylim(-0.2, 1.05)\n",
    "        else:\n",
    "            axs[k].set_ylim(top=1.0)\n",
    "        if metric == 'ROC/AUC':\n",
    "            axs[k].set_ylabel('ROC AUC')\n",
    "        elif metric == 'f1':\n",
    "            axs[k].set_ylabel('F1')\n",
    "        else:\n",
    "            axs[k].set_ylabel(metric)\n",
    "        # axs[k].legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "        # Create legend handles for colors\n",
    "        color_handles = [\n",
    "            mpatches.Patch(color=colors[i], label=config[model]['lab']) for i, model in enumerate(models)\n",
    "        ]\n",
    "\n",
    "        # Create legend handles for marker shapes\n",
    "        shape_handles = [\n",
    "            mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label=modes[0]),\n",
    "            mlines.Line2D([], [], color='black', marker='s', linestyle='None', markersize=10, label=modes[1]),\n",
    "            mlines.Line2D([], [], color='black', marker='D', linestyle='None', markersize=10, label=modes[2]),\n",
    "        ]\n",
    "\n",
    "        # Add separate legends to the plot\n",
    "        legend1 = axs[k].legend(handles=color_handles, title='Models', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "        legend2 = axs[k].legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(1.0, 0.5), loc='upper left')\n",
    "\n",
    "        # Add the first legend back to the plot to ensure it shows up\n",
    "        axs[k].add_artist(legend1)\n",
    "\n",
    "    fig.subplots_adjust(right=0.5)  # Adjust the right padding to make space for the legend\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        if metric == 'ROC/AUC':\n",
    "            metric = 'rocauc'\n",
    "        plt.savefig(f'./studies_results_lxplus/{name}_{metric}all.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_scores_mean_std_err(results, datasets, models, metric='MCC', name=None, labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "        for dataset in datasets:\n",
    "            key = f'{model}_{dataset}'\n",
    "            \n",
    "            if key in results:\n",
    "                scores['mean'] = np.append(scores['mean'], results[key]['mean'][metric].max())\n",
    "                idx = np.where(results[key]['mean'][metric] == results[key]['mean'][metric].max())[0][0]\n",
    "                scores['std'] = np.append(scores['std'], results[key]['std'][metric].iloc[idx])\n",
    "            else:\n",
    "                scores['mean'] = np.append(scores['mean'], 0)\n",
    "                scores['std'] = np.append(scores['std'], 0)\n",
    "        \n",
    "        x_positions = np.arange(len(datasets)) + j * 0.1  # add offset for each model\n",
    "        if labels:\n",
    "            ax.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=labels[j])  \n",
    "        else:\n",
    "            ax.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=model) \n",
    "            \n",
    "    ax.set_xticks(np.arange(len(datasets)) + 0.1 * (len(models) - 1) / 2)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.set_ylim(top=1.05)\n",
    "    if metric == 'ROC/AUC':\n",
    "        ax.set_ylabel('ROC AUC')\n",
    "        ax.set_title(f'Best ROC AUC Scores')\n",
    "        metric = 'rocauc'\n",
    "    elif metric == 'f1':\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.set_title(f'Best F1 Scores')\n",
    "    else:\n",
    "        ax.set_ylabel(metric)    \n",
    "        ax.set_title(f'Best {metric} scores')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        plt.savefig(f'./studies_results_lxplus/{name}_{metric}best.png', facecolor='w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = ['IEEECIS_new2.2', 'GECCO', 'SMD', 'SMAP_new', 'MSL_new', 'SWaT_1D', 'UCR'] \n",
    "models_plot = ['iTransformer1', 'iTransformer3', 'iTransformer2', 'iTransformer4', 'TranAD'] # 'MAD_GAN', 'OmniAnomaly', 'LSTM_AE', 'DAGMM', 'USAD']\n",
    "lab = [config[m]['lab'] for m in models_plot]\n",
    "\n",
    "name = None  # 'new'\n",
    "plot_best_scores_mean_std_err(results_mean_std, data_plot, models_plot, metric='MCC', labels=lab, name=name)\n",
    "plot_scores_mean_std_err(results_mean_std, modes, data_plot, models_plot, metric='MCC', labels=lab, name=name)\n",
    "plot_scores_mean_std_err2(results_mean_std, modes, data_plot, models_plot, metric='MCC', labels=lab, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_results(results, datasets, models, metric='MCC', labels=None, val='mean'):\n",
    "    dict = {}\n",
    "    for j, model in enumerate(models):\n",
    "        mcc_scores = []\n",
    "        for dataset in datasets:\n",
    "            key = f'{model}_{dataset}'\n",
    "            print(key)\n",
    "            if key in results:\n",
    "                mcc_scores.append(results[key][val][metric].max().round(3))\n",
    "            else:\n",
    "                mcc_scores.append(0)  # If no data, append 0\n",
    "\n",
    "            dict[labels[j]] = mcc_scores\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_results2(results, datasets, models, metric='MCC', labels=None, val='mean'):\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    for j, model in enumerate(models):\n",
    "        for i, mode in enumerate(modes):\n",
    "            rows = pd.Series([results[f\"{model}_{dataset}\"][val][metric].iloc[i].round(3) if f\"{model}_{dataset}\" in results else 0 for dataset in datasets], index=datasets)\n",
    "            # print(rows)\n",
    "            complete_row = pd.Series({'model': labels[j], 'mode': mode})\n",
    "            complete_row = pd.concat([complete_row, rows])\n",
    "            df = pd.concat([df, pd.DataFrame(complete_row).T], ignore_index=True)\n",
    "\n",
    "    \n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_results3(results, datasets, models, metric='MCC', labels=None):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    rows = []\n",
    "    for j, model in enumerate(models):\n",
    "        for i, mode in enumerate(modes):\n",
    "            i_datasets = [f'{dataset}_std' for dataset in datasets]\n",
    "            rows = pd.Series([rf'{results[f\"{model}_{dataset}\"][\"mean\"][metric].iloc[i].round(3)} $\\pm$ {results[f\"{model}_{dataset}\"][\"std\"][metric].iloc[i].round(3)}' if f\"{model}_{dataset}\" in results else 0 for dataset in datasets], index=datasets)\n",
    "            # rows = pd.Series([results[f\"{model}_{dataset}\"]['mean'][metric].iloc[i].round(3) if f\"{model}_{dataset}\" in results else 0 for dataset in datasets], index=datasets)\n",
    "            # rows2 = pd.Series([results[f\"{model}_{dataset}\"]['std'][metric].iloc[i].round(3) if f\"{model}_{dataset}\" in results else 0 for dataset in datasets], index=i_datasets)\n",
    "            # interwoven = [p for pair in zip(rows, rows2) for p in pair]\n",
    "            # print(interwoven)\n",
    "            # interwoven_index = datasets\n",
    "            # interwoven_index = [val for pair in zip(datasets, i_datasets) for val in pair]\n",
    "            # rows = pd.Series(interwoven, index=interwoven_index)\n",
    "            # print(rows)\n",
    "            complete_row = pd.Series({'model': labels[j], 'mode': mode})\n",
    "            complete_row = pd.concat([complete_row, rows])\n",
    "            df = pd.concat([df, pd.DataFrame(complete_row).T], ignore_index=True)\n",
    "    \n",
    "    # print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'mean'\n",
    "metric = 'MCC'\n",
    "namee = 'latent2_rep5_new'\n",
    "data3 = ['IEEECIS_new2.2', 'GECCO', 'SMD', 'SMAP_new', 'MSL_new', 'SWaT_1D', 'UCR']\n",
    "# ['ATLAS_TS', 'IEEECIS_new2.2', 'GECCO6', 'SMD', 'SMAP_new', 'MSL_new', 'SWaT', 'UCR'] #'ATLAS_TS',\n",
    "models3 = ['iTransformer1', 'iTransformer2', 'TranAD'] # ['iTransformer1', 'iTransformer3', 'iTransformer2', 'iTransformer4', 'TranAD']\n",
    "\n",
    "lab = [config[m]['lab'] for m in models3]\n",
    "\n",
    "# dict_bestMCC = list_results(results_mean_std, data3, models3, metric, labels=lab, val=val)\n",
    "# df_bestMCC2 = list_results2(results_mean_std, data3, models3, metric, labels=lab, val=val)\n",
    "df_bestMCC3 = list_results3(results_mean_std, data3, models3, metric, labels=lab)\n",
    "# df_bestMCC = pd.DataFrame(dict_bestMCC, index=data3).T\n",
    "print(df_bestMCC3)\n",
    "# print(df_bestMCC.values)\n",
    "# metric = metric.replace(' ', '_')\n",
    "# df_bestMCC.to_csv(f'studies_results_lxplus/{metric}_{namee}_{val}.csv')\n",
    "# df_bestMCC2.to_csv(f'studies_results_lxplus/{metric}_{namee}_all.csv')\n",
    "df_bestMCC3.to_csv(f'studies_datasets/{metric}_{namee}_all_latex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTransf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
