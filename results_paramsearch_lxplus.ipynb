{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import torch\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "plt.rcParams.update({'lines.markersize': 6})\n",
    "plt.rcParams.update({'errorbar.capsize': 8})\n",
    "plt.rcParams.update({'lines.linewidth': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.models import TranAD, iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['local (incl. OR)', 'local (maj. voting)', 'global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "# ['creditcard_normal', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "# ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "modeltype = 'USAD'\n",
    "reco = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading creditcard\n",
      "0\n",
      "Loading creditcard_normal\n",
      "1\n",
      "Loading GECCO\n",
      "0\n",
      "Loading GECCO_normal\n",
      "1\n",
      "Loading IEEECIS_new2.2\n",
      "1\n",
      "Loading MSL_new\n",
      "1\n",
      "Loading SMAP_new\n",
      "1\n",
      "Skipping window10_steps5_dmodel2_feats-1_eps10_MSE rep 3\n",
      "Loading SMD\n",
      "1\n",
      "Loading SWaT\n",
      "1\n",
      "Loading SWaT_1D\n",
      "1\n",
      "Loading UCR\n",
      "1\n",
      "Loading WADI\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "all_results = {dataset: {} for dataset in datasets}\n",
    "\n",
    "for dataset in datasets:\n",
    "    config = {}\n",
    "\n",
    "    print(f'Loading {dataset}')\n",
    "    if reco:\n",
    "        if modeltype == 'Transformer':\n",
    "            paths = glob.glob(f'{modeltype}/{modeltype}_{dataset}/*' )\n",
    "        elif modeltype == 'None':\n",
    "            paths = glob.glob(f'{modeltype}/{modeltype}_{dataset}/' )\n",
    "        elif modeltype == 'TranAD':\n",
    "            paths = glob.glob(f'{modeltype}_results_lxplus_eps10/{modeltype}_{dataset}/*' )\n",
    "            # paths = glob.glob(f'{modeltype}_results_lxplus/{modeltype}_{dataset}/' )  # up to 100 epochs with early stopping\n",
    "        elif modeltype == 'USAD':\n",
    "            paths = glob.glob(f'{modeltype}_results_lxplus/{modeltype}_{dataset}/*' )\n",
    "        elif modeltype == 'IF':\n",
    "            paths = glob.glob(f'{modeltype}/{modeltype}_{dataset}/' )\n",
    "        else:\n",
    "            paths = glob.glob(f'{modeltype}_paramsearch_reco_lxplus/{modeltype}_{dataset}/*' )\n",
    "            # paths = glob.glob(f'{modeltype}_new/{modeltype}_{dataset}/*' )\n",
    "    else:\n",
    "        paths = glob.glob(f'{modeltype}_paramsearch_fc_lxplus/{modeltype}_{dataset}/*' )\n",
    "    paths = [p for p in paths if not '.txt' in p]\n",
    "    paths = sorted(paths)\n",
    "    print(len(paths)) \n",
    "    # print(paths)\n",
    "\n",
    "    for path in paths:\n",
    "        key = path.split('/')[2]\n",
    "        if modeltype == 'TranAD':\n",
    "            # path = glob.glob(f'{path}/n_window*/results/')\n",
    "            path = glob.glob(f'{path}/rep_*/results/')\n",
    "        elif modeltype in ['IF', 'None']:\n",
    "            path = glob.glob(f'{path}/feats*/')\n",
    "        else:\n",
    "            path = glob.glob(f'{path}/rep_*/results/')  # will give all 5 repetitions\n",
    "        path = sorted(path)\n",
    "        # print(path)\n",
    "\n",
    "        tmp = pd.DataFrame()\n",
    "        for i, rep_path in enumerate(path):\n",
    "            res_path = os.path.join(rep_path, 'res.csv')\n",
    "            # print(res_path)\n",
    "            if not os.path.exists(res_path):\n",
    "                print(f'Skipping {key} rep {i+1}')\n",
    "                continue\n",
    "            res = pd.read_csv(res_path).iloc[-3:]\n",
    "            tmp = pd.concat((tmp, res))\n",
    "        \n",
    "        # print(len(tmp))\n",
    "        if modeltype == 'IF':\n",
    "            tmp = tmp.iloc[-1:]\n",
    "            tmp = tmp.rename(columns={'Unnamed: 0': 'mode'})\n",
    "            mean_values = tmp.groupby('mode').mean()\n",
    "            std_values = tmp.groupby('mode').std()\n",
    "            median_values = tmp.groupby('mode').median()\n",
    "            mean_values = mean_values.reindex(['global'])\n",
    "            std_values = std_values.reindex(['global'])\n",
    "            median_values = median_values.reindex(['global'])\n",
    "        else:\n",
    "            tmp.rename(columns={'Unnamed: 0': 'mode'}, inplace=True)\n",
    "            mean_values = tmp.groupby('mode').mean()\n",
    "            std_values = tmp.groupby('mode').std()\n",
    "            median_values = tmp.groupby('mode').median()\n",
    "            mean_values = mean_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "            std_values = std_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "            median_values = median_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "        if modeltype in ['IF', 'None']:\n",
    "            feats_value = -1\n",
    "            eps_value = 0\n",
    "            window_value = 10\n",
    "            steps_value = 10\n",
    "            dmodel_value = 0\n",
    "        # elif modeltype == 'TranAD':\n",
    "        #     feats_value = -1\n",
    "        #     # eps_value = 100\n",
    "        #     window_value = 10\n",
    "        #     steps_value = 1\n",
    "        #     dmodel_value = 2*int(feats_value)\n",
    "        else:\n",
    "            window_value = key.split('_')[0].replace('window', '')\n",
    "            steps_value = key.split('_')[1].replace('steps', '')\n",
    "            dmodel_value = key.split('_')[2].replace('dmodel', '')\n",
    "            feats_value = key.split('_')[3].replace('feats', '')\n",
    "            eps_value = key.split('_')[4].replace('eps', '')\n",
    "        config[key] = {'window': window_value, 'steps': steps_value, 'dmodel': dmodel_value, 'feats': feats_value, 'eps': eps_value}\n",
    "        all_results[dataset][key] = {'mean': mean_values, 'std': std_values, 'median': median_values, 'window': window_value, 'steps': steps_value, 'dmodel': dmodel_value}\n",
    "\n",
    "    config = pd.DataFrame(config).T\n",
    "    if reco:\n",
    "        if modeltype == 'Transformer':\n",
    "            config.to_csv(f'studies_paramsearch_reco/configs/{dataset}_transformer_config.csv')\n",
    "        elif modeltype == 'TranAD':\n",
    "            config.to_csv(f'studies_paramsearch_reco/configs/{dataset}_TranAD_config_10eps.csv')\n",
    "        elif modeltype == 'IF':\n",
    "            config.to_csv(f'studies_paramsearch_reco/configs/{dataset}_IF_config.csv')\n",
    "        elif modeltype == 'iTransformer':\n",
    "            config.to_csv(f'studies_paramsearch_reco/configs/{dataset}_config.csv')\n",
    "    else:\n",
    "        if modeltype == 'iTransformer':\n",
    "            config.to_csv(f'studies_paramsearch_fc/configs/{dataset}_config.csv')\n",
    "        \n",
    "# print(all_results['creditcard_normal']['window10_steps1_dmodel10_feats-1_eps10']['mean'])\n",
    "# print(all_results['creditcard_normal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creditcard_normal\n",
      "GECCO_normal\n",
      "IEEECIS_new2.2\n",
      "MSL_new\n",
      "SMAP_new\n",
      "SMD\n",
      "SWaT\n",
      "SWaT_1D\n",
      "UCR\n",
      "WADI\n",
      "             dataset                                      model  max_MCC  \\\n",
      "0  creditcard_normal  window10_steps5_dmodel2_feats-1_eps10_MSE    0.000   \n",
      "1       GECCO_normal  window10_steps5_dmodel2_feats-1_eps10_MSE    0.000   \n",
      "2     IEEECIS_new2.2  window10_steps5_dmodel2_feats30_eps10_MSE    0.000   \n",
      "3            MSL_new  window10_steps5_dmodel2_feats-1_eps10_MSE    0.626   \n",
      "4           SMAP_new  window10_steps5_dmodel2_feats-1_eps10_MSE    0.832   \n",
      "5                SMD  window10_steps5_dmodel2_feats-1_eps10_MSE    0.326   \n",
      "6               SWaT  window10_steps5_dmodel2_feats-1_eps10_MSE    0.838   \n",
      "7            SWaT_1D  window10_steps5_dmodel2_feats-1_eps10_MSE    0.807   \n",
      "8                UCR  window10_steps5_dmodel2_feats-1_eps10_MSE    0.105   \n",
      "9               WADI  window10_steps5_dmodel2_feats30_eps10_MSE    0.411   \n",
      "\n",
      "   max_MCC_std   max_MCC_mode  window steps  dmodel  \n",
      "0        0.000  local_all_maj      10     5       2  \n",
      "1        0.000  local_all_maj      10     5       2  \n",
      "2        0.000  local_all_maj      10     5       2  \n",
      "3        0.034      local_all      10     5       2  \n",
      "4        0.018         global      10     5       2  \n",
      "5        0.056      local_all      10     5       2  \n",
      "6        0.023         global      10     5       2  \n",
      "7        0.002      local_all      10     5       2  \n",
      "8        0.234      local_all      10     5       2  \n",
      "9        0.080         global      10     5       2  \n"
     ]
    }
   ],
   "source": [
    "best_config = []\n",
    "\n",
    "for dataset, results in all_results.items():\n",
    "    if results == {}:\n",
    "        continue\n",
    "    print(dataset)\n",
    "    mcc_scores = []\n",
    "    for key, metrics in results.items():\n",
    "        maxmode = metrics['mean']['MCC'].idxmax()\n",
    "        mcc_scores.append({\n",
    "            'dataset': dataset,\n",
    "            'model': key,\n",
    "            'local_all': metrics['mean'].loc['local_all', 'MCC'],\n",
    "            'local_all_maj': metrics['mean'].loc['local_all_maj', 'MCC'],\n",
    "            'global': metrics['mean'].loc['global', 'MCC'],\n",
    "            'max_MCC': metrics['mean']['MCC'].max().round(3),\n",
    "            'max_MCC_std': metrics['std'].loc[f'{maxmode}', 'MCC'].round(3),\n",
    "            'max_MCC_mode': metrics['mean']['MCC'].idxmax(),\n",
    "            'local_all_std': rf\"{metrics['mean'].loc['local_all', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all', 'MCC'].round(3)}\",\n",
    "            'local_all_maj_std': rf\"{metrics['mean'].loc['local_all_maj', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all_maj', 'MCC'].round(3)}\",\n",
    "            'global_std': rf\"{metrics['mean'].loc['global', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['global', 'MCC'].round(3)}\",\n",
    "            'window': metrics['window'],\n",
    "            'steps': metrics['steps'],\n",
    "            'dmodel': metrics['dmodel'],\n",
    "            # 'train_time': metrics['mean'].loc['global','train_time'].round(3),\n",
    "        })\n",
    "    mcc_df = pd.DataFrame(mcc_scores)\n",
    "    # print(mcc_df)\n",
    "    mcc_df['dmodel'] = mcc_df['dmodel'].astype(int)\n",
    "    mcc_df['window'] = mcc_df['window'].astype(int)\n",
    "    mcc_df = mcc_df.sort_values(by=['window', 'dmodel'], ascending=[True, True])\n",
    "    # print(mcc_df)\n",
    "    if reco:\n",
    "        if modeltype == 'Transformer':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_transformer_mcc.csv')\n",
    "        elif modeltype == 'TranAD':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_TranAD_mcc.csv')\n",
    "        elif modeltype == 'None':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_None_mcc.csv')\n",
    "        elif modeltype == 'IF':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_IF_mcc.csv')\n",
    "        elif modeltype == 'USAD':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_USAD_mcc.csv')\n",
    "        elif modeltype == 'iTransformer':\n",
    "            mcc_df.to_csv(f'studies_paramsearch_reco/data/{dataset}_mcc.csv')\n",
    "    else:\n",
    "        mcc_df.to_csv(f'studies_paramsearch_fc/data/{dataset}_mcc.csv')\n",
    "\n",
    "    best_idx = mcc_df['max_MCC'].idxmax()\n",
    "    # print('idx', best_idx)\n",
    "    best_config.append({\n",
    "        'dataset': dataset,\n",
    "        'model': mcc_df['model'][best_idx],\n",
    "        'max_MCC': mcc_df['max_MCC'].max(),\n",
    "        'max_MCC_std': mcc_df['max_MCC_std'][best_idx],\n",
    "        'max_MCC_mode': mcc_df['max_MCC_mode'][best_idx],\n",
    "        'window': mcc_df['window'][best_idx],\n",
    "        'steps': mcc_df['steps'][best_idx],\n",
    "        'dmodel': mcc_df['dmodel'][best_idx],\n",
    "    })\n",
    "    \n",
    "best_config = pd.DataFrame(best_config)\n",
    "print(best_config)\n",
    "# best_config.to_csv(f'studies_loss_reco/softdtw_testMSE.csv')\n",
    "\n",
    "if reco:\n",
    "    if modeltype == 'Transformer':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/data/best_config_transformer.csv')\n",
    "    elif modeltype == 'TranAD':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/data/best_config_TranAD_eps10.csv')\n",
    "    elif modeltype == 'None':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/data/best_config_None.csv')\n",
    "    elif modeltype == 'IF':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/data/best_config_IF.csv')\n",
    "    elif modeltype == 'USAD':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/data/best_config_USAD.csv')\n",
    "    elif modeltype == 'iTransformer':\n",
    "        best_config.to_csv(f'studies_paramsearch_reco/configs/best_config.csv')\n",
    "else:\n",
    "    best_config.to_csv(f'studies_paramsearch_fc/configs/best_config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_mean_std_err2(results, modes, dataset, models, metric='MCC', name=None, labels=None):\n",
    "    if labels is None:\n",
    "        labels = [f\"W: {results[dataset][model]['window']}, S: {results[dataset][model]['steps']}, M: {results[dataset][model]['dmodel']}\" for model in models]\n",
    "\n",
    "    if 'MSE' in results[dataset].keys():\n",
    "        fig = plt.figure(figsize=(20, 7))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(25, 8))\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(models)+1))\n",
    "    markers = ['o', 's', 'D']  # Different markers for different modes\n",
    "    plt.rcParams.update({'lines.markersize': 8})\n",
    "\n",
    "    width_modes = 0.015  # the width of the bars\n",
    "    width_models = 0.025\n",
    "    x = np.arange(1)  # the label locations\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        for i, mode in enumerate(modes):\n",
    "            # print(f'{model} {mode} {dataset}')\n",
    "            scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "            for val in ['mean', 'std']:\n",
    "                if model in results[dataset].keys():\n",
    "                    scores[val] = np.append(scores[val], results[dataset][model][val][metric].iloc[i])\n",
    "                else:\n",
    "                    scores[val] = np.append(scores[val], 0)\n",
    "                # print(f'{val}: {scores[val]}')\n",
    "\n",
    "            x_positions = x + j * len(modes) * width_models + i * width_modes  # add different offset for each model and mode\n",
    "            plt.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt=markers[i], label=f'{labels[j]} ({mode})', color=colors[j])\n",
    "    x_end = x_positions[-1]\n",
    "    plt.xticks(ticks=[x_end / 2], labels=[dataset])\n",
    "    plt.grid('gray', axis='y')\n",
    "    if metric == 'MCC':\n",
    "        plt.ylim(-0.2, 1.05)\n",
    "    else:\n",
    "        plt.ylim(top=1.0)\n",
    "    if metric == 'ROC/AUC':\n",
    "        plt.ylabel('ROC AUC')\n",
    "    elif metric == 'f1':\n",
    "        plt.ylabel('F1')\n",
    "    else:\n",
    "        plt.ylabel(metric)\n",
    "    \n",
    "    if 'MSE' in results[dataset].keys():\n",
    "        plt.title(f\"iTransformer config: W: {results[dataset]['MSE']['window']}, S: {results[dataset]['MSE']['steps']}, M: {results[dataset]['MSE']['dmodel']}\")\n",
    "\n",
    "    # Create legend handles for colors\n",
    "    color_handles = [\n",
    "        mpatches.Patch(color=colors[i], label=labels[i]) for i, model in enumerate(models)\n",
    "    ]\n",
    "\n",
    "    # Create legend handles for marker shapes\n",
    "    shape_handles = [\n",
    "        mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label=modes[0]),\n",
    "        mlines.Line2D([], [], color='black', marker='s', linestyle='None', markersize=10, label=modes[1]),\n",
    "        mlines.Line2D([], [], color='black', marker='D', linestyle='None', markersize=10, label=modes[2]),\n",
    "    ]\n",
    "\n",
    "    # Add separate legends to the plot\n",
    "    if dataset in ['MSL_new', 'SMAP_new', 'SWaT', 'WADI']:  # 'SWaT_1D'\n",
    "        legend2 = plt.legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(0.7, 0.4), loc='upper left') \n",
    "    elif dataset in ['SMD']:\n",
    "        legend2 = plt.legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(0.7, 0.65), loc='upper left')\n",
    "    else:\n",
    "        legend2 = plt.legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(0.7, 1.0), loc='upper left')  \n",
    "    plt.gca().add_artist(legend2)  # Add the first legend manually to the axes\n",
    "    if 'MSE' in results[dataset].keys():\n",
    "        legend1 = plt.legend(handles=color_handles, title='Loss functions', bbox_to_anchor=(1, 1.05), loc='upper left')\n",
    "    else:\n",
    "        legend1 = plt.legend(handles=color_handles, title='Models', bbox_to_anchor=(1, 1.05), loc='upper left')\n",
    "    # plt.gca().add_artist(legend1)  # Add the first legend manually to the axes\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        if metric == 'ROC/AUC':\n",
    "            metric = 'rocauc'\n",
    "        if 'MSE' in results[dataset].keys(): \n",
    "            if reco:\n",
    "                plt.savefig(f'./studies_loss_reco/{dataset}_{metric}_{name}.png', facecolor='w')\n",
    "            else:\n",
    "                plt.savefig(f'./studies_loss_fc/{dataset}_{metric}_{name}.png', facecolor='w')\n",
    "        else:\n",
    "            if reco:\n",
    "                plt.savefig(f'./studies_paramsearch_reco/{dataset}_{metric}_{name}.png', facecolor='w')\n",
    "            else:\n",
    "                plt.savefig(f'./studies_paramsearch_fc/{dataset}_{metric}_{name}.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_configs(results, dataset, safe=False, reco=True):\n",
    "    markers = ['o', 's', 'D', '*']  \n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(results['model'])+1))\n",
    "\n",
    "    x = results['dmodel']\n",
    "    y = results['window']\n",
    "    z = results['steps']\n",
    "    score = results['max_MCC']\n",
    "    ylimit = math.floor(min(score) * 10) / 10 - 0.05\n",
    "\n",
    "    plt.rcParams.update({'lines.markersize': 8})\n",
    "\n",
    "    # Create 2D scatter plots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(20, 7), constrained_layout=True, sharex=True, sharey=True)\n",
    "\n",
    "    for i, group in enumerate(results.groupby('window')):\n",
    "        group_name, group_data = group\n",
    "        for j, elem in group_data.iterrows():\n",
    "            # print(f'{group_name} {elem[\"steps\"]} {elem[\"dmodel\"]}')\n",
    "            axes.errorbar(elem['dmodel'], elem['max_MCC'], yerr=elem['max_MCC_std'], linestyle='', marker=markers[i % 4], color=colors[j], markersize=10,\n",
    "                      label=f'W: {elem[\"window\"]}, S: {elem[\"steps\"]}, M: {elem[\"dmodel\"]}')\n",
    "        # axes.plot(group_data['dmodel'], group_data['max_MCC'], 'o', label=f'W: {group_name}, S: {group_data[\"steps\"].iloc[0]}, M: {group_data[\"dmodel\"].iloc[0]}')\n",
    "        # axes.scatter(group_data['dmodel'], group_data['max_MCC'], c=col[i:i+4], cmap='plasma', \n",
    "        #              label=f'W: {group_name}, S: {group_data[\"steps\"].iloc[0]}, M: {group_data[\"dmodel\"].iloc[0]}')  # because always 4 window sizes while rest fixed\n",
    "        axes.set_xlabel('Internal size M')\n",
    "        axes.set_ylabel('MCC')\n",
    "        axes.set_ylim(ylimit,1)\n",
    "    plt.title(f'iTransformer configurations on {dataset}')\n",
    "    plt.legend(bbox_to_anchor=(1, 1.05), loc='upper left')\n",
    "    plt.grid('gray', axis='y')\n",
    "\n",
    "    # version 1\n",
    "    # axes[0].scatter(x, y, c=score, cmap='viridis')\n",
    "    # axes[0].set_xlabel('Internal size M')\n",
    "    # axes[0].set_ylabel('Window size W')\n",
    "    # # axes[0].set_title('Parameter 1 vs Parameter 2')\n",
    "\n",
    "    # axes[1].scatter(x, z, c=score, cmap='viridis')\n",
    "    # axes[1].set_xlabel('Internal size M')\n",
    "    # axes[1].set_ylabel('Step size S')\n",
    "    # # axes[1].set_title('Parameter 1 vs Parameter 3')\n",
    "\n",
    "    # axes[2].scatter(y, z, c=score, cmap='viridis')\n",
    "    # axes[2].set_xlabel('Window size W')\n",
    "    # axes[2].set_ylabel('Step size S')\n",
    "    # # axes[2].set_title('Parameter 2 vs Parameter 3')\n",
    "\n",
    "    # version 2   \n",
    "    # for i, group in enumerate(results.groupby('window')):\n",
    "    #     group_name, group_data = group\n",
    "    #     axes[i].scatter(group_data['dmodel'], group_data['steps'], c=group_data['max_MCC'], cmap='viridis')\n",
    "    #     axes[i].set_title(f'Window size {group_name}')\n",
    "    #     axes[i].set_xlabel('Internal size M')\n",
    "    #     axes[i].set_ylabel('Step size S')\n",
    "\n",
    "    # # Add color bar\n",
    "    # cbar = fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax=axes, orientation='vertical', fraction=0.02)\n",
    "    # cbar.set_label('MCC')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    if safe:\n",
    "        if reco:\n",
    "            plt.savefig(f'./studies_paramsearch_reco/dmodel_{dataset}.png', facecolor='w')\n",
    "        else:\n",
    "            plt.savefig(f'./studies_paramsearch_fc/dmodel_{dataset}.png', facecolor='w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance(results, dataset, safe=False, reco=True):\n",
    "    markers = ['o', 's', 'D']  # Different markers for different modes\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(results['model'])+1))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 7), constrained_layout=True)\n",
    "\n",
    "    for i in range(len(results['model'])):\n",
    "        plt.errorbar(results['steps'][i], results['max_MCC_std'][i], linestyle='', marker=markers[i // 4 % len(markers)], color=colors[i], markersize=10,\n",
    "             label=f'W: {results[\"window\"][i]}, S: {results[\"steps\"][i]}, M: {results[\"dmodel\"][i]}')\n",
    "    \n",
    "    plt.title(f'iTransformer configurations on {dataset}')\n",
    "    plt.xlabel('Step size S')\n",
    "    plt.ylabel('Std of MCC over 5 repetitions')\n",
    "    plt.legend(bbox_to_anchor=(1, 1.05), loc='upper left')\n",
    "    plt.grid('gray', axis='y')\n",
    "\n",
    "    if safe:\n",
    "        if reco:\n",
    "            plt.savefig(f'./studies_paramsearch_reco/steps_{dataset}.png', facecolor='w', bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(f'./studies_paramsearch_fc/steps_{dataset}.png', facecolor='w', bbox_inches='tight')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = datasets  # ['creditcard_normal', 'GECCO_normal', 'MSL_new', 'SMAP_new', 'SWaT_1D', 'UCR']\n",
    "\n",
    "for dataset in data_plot:\n",
    "    models_plot = sorted(list(all_results[dataset].keys()))\n",
    "    labels = [l[:-14] for l in models_plot]\n",
    "    labels = [l.replace('_', ' ') for l in labels]\n",
    "    # if reco:\n",
    "    #     name = 'reco2'\n",
    "    # else:\n",
    "    #     name = 'fc2'\n",
    "    name = None\n",
    "    plot_scores_mean_std_err2(all_results, modes, dataset, models_plot, metric='MCC', name=name, labels=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = datasets \n",
    "\n",
    "for dataset in data_plot:\n",
    "    if dataset == 'SWaT' and not reco:\n",
    "        continue\n",
    "    if reco:\n",
    "        results = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_mcc.csv')\n",
    "    else:\n",
    "        results = pd.read_csv(f'studies_paramsearch_fc/data/{dataset}_mcc.csv')\n",
    "    # print(results.keys())\n",
    "    # plot_configs(results, dataset, reco=reco, safe=True)\n",
    "    plot_variance(results, dataset, reco=reco, safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss fct comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['creditcard_normal', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI'] # 'creditcard', 'GECCO'\n",
    "# ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal']\n",
    "# ['creditcard_normal', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI'] # 'creditcard', 'GECCO'\n",
    "modeltype = 'iTransformer'\n",
    "# loss_fct = ['MSE', 'Huber', 'Huber_quant', 'penalty']\n",
    "reco = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {dataset: {} for dataset in datasets}\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    print(f'Loading {dataset}')\n",
    "    if reco:\n",
    "        paths = glob.glob(f'{modeltype}_loss_fct/{modeltype}_{dataset}/*' )\n",
    "    else:\n",
    "        paths = glob.glob(f'{modeltype}_loss_fct/{modeltype}_{dataset}/*' )\n",
    "    paths = [p for p in paths if not '.txt' in p]\n",
    "    paths = [p for p in paths if not '.log' in p]\n",
    "    paths = sorted(paths)\n",
    "    print(len(paths)) \n",
    "    # print(paths)\n",
    "\n",
    "    for path in paths:\n",
    "        if 'penalty' in path:\n",
    "            loss_type = 'penalty'\n",
    "        elif 'Huber_quant' in path:\n",
    "            loss_type = 'Huber_quant'\n",
    "        elif 'Huber' in path:\n",
    "            loss_type = 'Huber'\n",
    "        elif 'softdtw' in path:\n",
    "            loss_type = 'softDTW'\n",
    "        else:\n",
    "            loss_type = 'MSE'\n",
    "        key = path.split('/')[2]\n",
    "        \n",
    "        for j in range(2):\n",
    "            if j == 0:\n",
    "                res_path = glob.glob(f'{path}/rep_*/results/')  # will give all 5 repetitions\n",
    "                res_path = sorted(res_path)\n",
    "            elif j == 1 and loss_type not in ['MSE', 'softDTW']: # unnecessary for MSE and softDTW (because always MSE for testing)\n",
    "                res_path = glob.glob(f'{path}/testMSE/rep_*/results/') # will give all 5 repetitions with MSE for test loss\n",
    "                res_path = sorted(res_path)\n",
    "                loss_type = loss_type + '_testMSE'\n",
    "            else:\n",
    "                continue\n",
    "            # print(len(res_path))\n",
    "            # print(res_path)\n",
    "\n",
    "            tmp = pd.DataFrame()\n",
    "            for i, rep_path in enumerate(res_path):\n",
    "                final_path = os.path.join(rep_path, 'res.csv')\n",
    "                # print(final_path)\n",
    "                if not os.path.exists(final_path):\n",
    "                    print(f'Skipping {key} rep {i+1}')\n",
    "                    continue\n",
    "                res = pd.read_csv(final_path).iloc[-3:]\n",
    "                tmp = pd.concat((tmp, res))\n",
    "            \n",
    "            tmp.rename(columns={'Unnamed: 0': 'mode'}, inplace=True)\n",
    "            mean_values = tmp.groupby('mode').mean()\n",
    "            std_values = tmp.groupby('mode').std()\n",
    "            median_values = tmp.groupby('mode').median()\n",
    "            mean_values = mean_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "            std_values = std_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "            median_values = median_values.reindex(['local_all', 'local_all_maj', 'global'])\n",
    "\n",
    "            window_value = key.split('_')[0].replace('window', '')\n",
    "            steps_value = key.split('_')[1].replace('steps', '')\n",
    "            dmodel_value = key.split('_')[2].replace('dmodel', '')\n",
    "            feats_value = key.split('_')[3].replace('feats', '')\n",
    "            eps_value = key.split('_')[4].replace('eps', '')\n",
    "            all_results[dataset][loss_type] = {'mean': mean_values, 'std': std_values, 'median': median_values, \n",
    "                                        'window': window_value, 'steps': steps_value, 'dmodel': dmodel_value,\n",
    "                                        'loss_type': loss_type}\n",
    "\n",
    "        \n",
    "print(all_results['creditcard_normal'].keys())\n",
    "print(all_results['creditcard_normal']['penalty']['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_std_err(all_results, datasets, keys, datanames=None, metric='MCC', name=None, labels=None):\n",
    "    if labels is None:\n",
    "        labels = keys\n",
    "    if datanames is None:\n",
    "        datanames = datasets\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharey=True)\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(keys)+1))\n",
    "    plt.rcParams.update({'lines.markersize': 8})\n",
    "    width_models = 0.2\n",
    "    datasets_split = [datasets[:len(datasets)//2], datasets[len(datasets)//2:]]  # Split datasets into two halves\n",
    "    datanames_split = [datanames[:len(datanames)//2], datanames[len(datanames)//2:]]  # Split dataset names into two halves\n",
    "\n",
    "    for ax, dataset_group, datanames_group in zip(axes, datasets_split, datanames_split):\n",
    "        x = np.arange(len(dataset_group))  # the label locations for the current group\n",
    "        for dataset_idx, dataset in enumerate(dataset_group):\n",
    "            for j, model in enumerate(keys):\n",
    "                # print(f'{model} {dataset}')\n",
    "                scores = {'mean': np.empty(0), 'std': np.empty(0)}\n",
    "                maxmode = all_results[dataset][model]['mean']['MCC'].idxmax()\n",
    "                if model in all_results[dataset].keys():\n",
    "                    scores['mean'] = np.append(scores['mean'], all_results[dataset][model]['mean'][metric].max())\n",
    "                    scores['std'] = np.append(scores['std'], all_results[dataset][model]['std'].loc[f'{maxmode}', 'MCC'])\n",
    "                else:\n",
    "                    scores['mean'] = np.append(scores['mean'], 0)\n",
    "                    scores['std'] = np.append(scores['std'], 0)\n",
    "\n",
    "                x_positions = x[dataset_idx] + (j-1) * width_models # Adjust x positions for datasets and keys\n",
    "                ax.errorbar(x_positions, scores['mean'], yerr=scores['std'], fmt='o', label=f'{labels[j]}', color=colors[j])\n",
    "\n",
    "        ax.set_xticks(ticks=x)\n",
    "        ax.set_xticklabels(datanames_group)\n",
    "        ax.grid('gray', axis='y')\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(6))  # Set a maximum of 10 grid lines on the y-axis\n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.25))\n",
    "        if metric == 'MCC':\n",
    "            ax.set_ylim(-0.0, 1.05)\n",
    "        else:\n",
    "            ax.set_ylim(top=1.0)\n",
    "        if metric == 'ROC/AUC':\n",
    "            ax.set_ylabel('ROC AUC')\n",
    "        elif metric == 'f1':\n",
    "            ax.set_ylabel('F1')\n",
    "        else:\n",
    "            ax.set_ylabel(metric)\n",
    "\n",
    "    # Create legend handles for colors\n",
    "    color_handles = [\n",
    "        mlines.Line2D([], [], color=colors[i], marker='o', linestyle='None', markersize=10, label=labels[i]) for i, model in enumerate(keys)\n",
    "    ]\n",
    "    legend1 = fig.legend(handles=color_handles, title='Loss functions', loc='upper center', bbox_to_anchor=(1.1, .95))  #, ncol=len(keys))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if name:\n",
    "        if metric == 'ROC/AUC':\n",
    "            metric = 'rocauc'\n",
    "        else:\n",
    "            if reco:\n",
    "                plt.savefig(f'./studies_loss_reco/{metric}_loss_{name}.png', facecolor='w')\n",
    "            else:\n",
    "                plt.savefig(f'./studies_loss_fc/{metric}_loss_{name}.png', facecolor='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = datasets\n",
    "\n",
    "for dataset in data_plot:\n",
    "    models_plot = list(all_results[dataset].keys())\n",
    "    labels = ['MSE', 'Huber', 'Huber (MSE for test)', 'Huber + quantile', 'Huber + quantile \\n(MSE for test)', \n",
    "              'Huber + quantile + penalty', 'Huber + quantile + penalty \\n(MSE for test)', 'soft DTW \\n(MSE for test)']\n",
    "    #['MSE', 'Huber', 'Huber + quantile loss', 'Huber + quantile + penalty']\n",
    "    name = 'reco'\n",
    "#     name = None\n",
    "    plot_scores_mean_std_err2(all_results, modes, dataset, models_plot, metric='MCC', name=name, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_plot = ['MSE', 'Huber_testMSE', 'softDTW']  # list(all_results[dataset].keys())\n",
    "labels = ['MSE', 'Huber \\n(MSE for test)', 'Soft-DTW \\n(MSE for test)']\n",
    "name = None  # 'reco'\n",
    "datanames = ['Credit card', 'GECCO', 'IEEECIS', 'MSL', 'SMAP', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "\n",
    "plot_best_std_err(all_results, datasets, models_plot, datanames, metric='MCC', name=name, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mcc_scores = []  # List to store all MCC scores across datasets\n",
    "testMSE = False\n",
    "all = True\n",
    "\n",
    "for dataset, results in all_results.items():\n",
    "    if results == {}:\n",
    "        print(f'Skipping {dataset} as it has no results')\n",
    "        continue\n",
    "    for key, metrics in results.items():\n",
    "        if key in models_plot:\n",
    "            if all:\n",
    "                all_mcc_scores.append({\n",
    "                    'dataset': dataset,\n",
    "                    'model': key,\n",
    "                    'local_all': metrics['mean'].loc['local_all', 'MCC'],\n",
    "                    'local_all_maj': metrics['mean'].loc['local_all_maj', 'MCC'],\n",
    "                    'global': metrics['mean'].loc['global', 'MCC'],\n",
    "                    'max_MCC': metrics['mean']['MCC'].max().round(3),\n",
    "                    'max_MCC_std': metrics['std'].loc[f'{metrics[\"mean\"][\"MCC\"].idxmax()}', 'MCC'].round(3),\n",
    "                    'max_MCC_mode': metrics['mean']['MCC'].idxmax(),\n",
    "                    'local_all_std': rf\"{metrics['mean'].loc['local_all', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all', 'MCC'].round(3)}\",\n",
    "                    'local_all_maj_std': rf\"{metrics['mean'].loc['local_all_maj', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all_maj', 'MCC'].round(3)}\",\n",
    "                    'global_std': rf\"{metrics['mean'].loc['global', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['global', 'MCC'].round(3)}\",\n",
    "                    'window': metrics['window'],\n",
    "                    'steps': metrics['steps'],\n",
    "                    'dmodel': metrics['dmodel'],\n",
    "                })\n",
    "            elif testMSE and 'MSE' in key:\n",
    "                all_mcc_scores.append({\n",
    "                    'dataset': dataset,\n",
    "                    'model': key,\n",
    "                    'local_all': metrics['mean'].loc['local_all', 'MCC'],\n",
    "                    'local_all_maj': metrics['mean'].loc['local_all_maj', 'MCC'],\n",
    "                    'global': metrics['mean'].loc['global', 'MCC'],\n",
    "                    'max_MCC': metrics['mean']['MCC'].max().round(3),\n",
    "                    'max_MCC_std': metrics['std'].loc[f'{metrics[\"mean\"][\"MCC\"].idxmax()}', 'MCC'].round(3),\n",
    "                    'max_MCC_mode': metrics['mean']['MCC'].idxmax(),\n",
    "                    'local_all_std': rf\"{metrics['mean'].loc['local_all', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all', 'MCC'].round(3)}\",\n",
    "                    'local_all_maj_std': rf\"{metrics['mean'].loc['local_all_maj', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all_maj', 'MCC'].round(3)}\",\n",
    "                    'global_std': rf\"{metrics['mean'].loc['global', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['global', 'MCC'].round(3)}\",\n",
    "                    'window': metrics['window'],\n",
    "                    'steps': metrics['steps'],\n",
    "                    'dmodel': metrics['dmodel'],\n",
    "                })\n",
    "            elif not testMSE and not 'testMSE' in key:\n",
    "                all_mcc_scores.append({\n",
    "                    'dataset': dataset,\n",
    "                    'model': key,\n",
    "                    'local_all': metrics['mean'].loc['local_all', 'MCC'],\n",
    "                    'local_all_maj': metrics['mean'].loc['local_all_maj', 'MCC'],\n",
    "                    'global': metrics['mean'].loc['global', 'MCC'],\n",
    "                    'max_MCC': metrics['mean']['MCC'].max().round(3),\n",
    "                    'max_MCC_std': metrics['std'].loc[f'{metrics[\"mean\"][\"MCC\"].idxmax()}', 'MCC'].round(3),\n",
    "                    'max_MCC_mode': metrics['mean']['MCC'].idxmax(),\n",
    "                    'local_all_std': rf\"{metrics['mean'].loc['local_all', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all', 'MCC'].round(3)}\",\n",
    "                    'local_all_maj_std': rf\"{metrics['mean'].loc['local_all_maj', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['local_all_maj', 'MCC'].round(3)}\",\n",
    "                    'global_std': rf\"{metrics['mean'].loc['global', 'MCC'].round(3)} $\\pm$ {metrics['std'].loc['global', 'MCC'].round(3)}\",\n",
    "                    'window': metrics['window'],\n",
    "                    'steps': metrics['steps'],\n",
    "                    'dmodel': metrics['dmodel'],\n",
    "                })\n",
    "# Create a single DataFrame from the collected data\n",
    "all_mcc_df = pd.DataFrame(all_mcc_scores)\n",
    "print(all_mcc_df)\n",
    "\n",
    "# all_mcc_df.to_csv(f'studies_loss_reco/anomalies_mcc.csv')\n",
    "\n",
    "# all_mcc_df.to_csv(f'studies_loss_reco/anomalies_mcc_3losses.csv')\n",
    "\n",
    "# if all:\n",
    "#     if reco:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_reco/all_datasets_mcc_all.csv')\n",
    "#     else:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_fc/all_datasets_all.csv')\n",
    "# elif testMSE:\n",
    "#     if reco:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_reco/all_datasets_mcc_testMSE.csv')\n",
    "#     else:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_fc/all_datasets_mcc_testMSE.csv')\n",
    "# else:\n",
    "#     if reco:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_reco/all_datasets_mcc_notestMSE.csv')\n",
    "#     else:\n",
    "#         all_mcc_df.to_csv(f'studies_loss_fc/all_datasets_mcc_notestMSE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare Transformer and iTransformer on optimal configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['creditcard_normal', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "# ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "reco = True\n",
    "modeltypes =  ['None', 'iTransformer-fc', 'iTransformer-reco', 'Transformer', 'TranAD'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results, datasets, modeltypes, datanames=None, metric='MCC', safe=False, reco=True):\n",
    "    markers = ['X', 'o', 's', 'D', '^', 'v', 'P'] \n",
    "    plt.rcParams.update({'lines.markersize': 8})\n",
    "    # colors = plt.rcParams['axes.prop_cycle'].by_key()['color']  # Access the color cycle\n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(modeltypes)+1))\n",
    "\n",
    "    n = 1 # len(datasets) // 5 + (1 if len(datasets) % 5 != 0 else 0)  # Calculate the number of subplots needed\n",
    "    fig, axes = plt.subplots(1, n, figsize=(24, 6), sharey=True, constrained_layout=True)\n",
    "\n",
    "    if n == 1:  # If there's only one subplot, wrap it in a list for consistency\n",
    "        axes = [axes]\n",
    "\n",
    "    for subplot_idx in range(n):\n",
    "        # start_idx = subplot_idx * 5\n",
    "        # end_idx = min(start_idx + 5, len(datasets))\n",
    "        subset_datasets = datasets  # datasets[start_idx:end_idx]\n",
    "        datanamessub = datanames  # datanamessub[start_idx:end_idx]\n",
    "\n",
    "        for dataset_idx, dataset in enumerate(subset_datasets):\n",
    "            for i, modeltype in enumerate(modeltypes):\n",
    "                key = f'{modeltype}_{dataset}'\n",
    "                if key not in results.keys():\n",
    "                    print(f'Skipping {key} as it has no results')\n",
    "                    continue\n",
    "\n",
    "                if metric == 'MCC':\n",
    "                    if modeltype == 'None':\n",
    "                        axes[subplot_idx].plot(\n",
    "                        dataset_idx + (i * 0.15) - 0.3,  # Offset x positions for better visibility\n",
    "                        results[key]['max_MCC'].max(),\n",
    "                        linestyle='',\n",
    "                        marker=markers[i],\n",
    "                        color=colors[i],\n",
    "                        markersize=8,\n",
    "                        linewidth=2,\n",
    "                        label=f'{modeltype}'\n",
    "                    )\n",
    "                    else:\n",
    "                        axes[subplot_idx].errorbar(\n",
    "                            dataset_idx + (i * 0.15) - 0.3,  # Offset x positions for better visibility\n",
    "                            results[key]['max_MCC'].max(),\n",
    "                            yerr=results[key]['max_MCC_std'].max(),\n",
    "                            linestyle='',\n",
    "                            marker=markers[i],\n",
    "                            color=colors[i],\n",
    "                            markersize=8,\n",
    "                            linewidth=2,\n",
    "                            label=f'{modeltype}'\n",
    "                        )\n",
    "                elif metric == 'train_time':\n",
    "                    axes[subplot_idx].bar(\n",
    "                        dataset_idx + (i * 0.15) - 0.075,  # Adjust x positions for thinner bars\n",
    "                        results[key]['train_time'].max(),\n",
    "                        width=0.1,  # Set the width of the bars to make them thinner\n",
    "                        color=colors[i],\n",
    "                        label=f'{modeltype}'\n",
    "                    )\n",
    "                else:\n",
    "                     axes[subplot_idx].plot(\n",
    "                        dataset_idx + (i * 0.2) - 0.1,  # Offset x positions for better visibility\n",
    "                        results[key]['metric'].max(),\n",
    "                        linestyle='',\n",
    "                        marker=markers[i],\n",
    "                        color=colors[i],\n",
    "                        markersize=6,\n",
    "                        label=f'{modeltype}'\n",
    "                    )\n",
    "                if dataset_idx == 0 and subplot_idx == 0:\n",
    "                    if metric == 'MCC':\n",
    "                        # axes[subplot_idx].legend(loc='lower right', title='Models') \n",
    "                        axes[subplot_idx].legend(bbox_to_anchor=(1, 1.05), loc='upper left', title='Models')\n",
    "                    else:\n",
    "                        axes[subplot_idx].legend(loc='upper left', title='Models')\n",
    "\n",
    "            # axes[subplot_idx].set_ylim(0, 1.05)\n",
    "            axes[subplot_idx].set_xticks(range(len(datanamessub)))\n",
    "            axes[subplot_idx].set_xticklabels(datanamessub, rotation=0)\n",
    "            axes[subplot_idx].grid('gray', axis='y')\n",
    "            axes[subplot_idx].set_ylabel(metric)\n",
    "\n",
    "    # fig.suptitle(f'Comparison of iTransformer vs vanilla transformer')\n",
    "    # fig.supylabel('Training time [s]')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # if safe:\n",
    "    #     if reco:\n",
    "    #         plt.savefig(f'./studies_loss_reco/comparison_{dataset}.png', facecolor='w')\n",
    "    #     else:\n",
    "    #         plt.savefig(f'./studies_loss_fc/comparison_{dataset}.png', facecolor='w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = datasets \n",
    "datanames = ['Credit card', 'GECCO', 'IEEECIS', 'MSL', 'SMAP', 'SMD', 'SWaT' ,'SWaT_1D', 'UCR', 'WADI']\n",
    "# datanames = datasets\n",
    "results_both = {}\n",
    "\n",
    "for dataset in data_plot:\n",
    "    print(f'Loading {dataset}')\n",
    "    for modeltype in modeltypes:\n",
    "        print(f'\\tLoading {modeltype}')\n",
    "        if reco:\n",
    "            if modeltype == 'Transformer':\n",
    "                results_both[f'{modeltype}_{dataset}'] = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_transformer_mcc.csv')\n",
    "            elif modeltype == 'iTransformer-reco':\n",
    "                chosen_config = pd.read_csv(f'studies_paramsearch_reco/configs/best_config_me.csv')\n",
    "                chosen_config = chosen_config[chosen_config['dataset'] == dataset]\n",
    "                results_tmp = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_mcc.csv')\n",
    "                results_both[f'{modeltype}_{dataset}'] = results_tmp[results_tmp['model'] == chosen_config['model'].iloc[0]]\n",
    "            elif modeltype == 'iTransformer-fc':\n",
    "                chosen_config = pd.read_csv(f'studies_paramsearch_fc/configs/best_config_me.csv')\n",
    "                chosen_config = chosen_config[chosen_config['dataset'] == dataset]\n",
    "                results_tmp = pd.read_csv(f'studies_paramsearch_fc/data/{dataset}_mcc.csv')\n",
    "                results_both[f'{modeltype}_{dataset}'] = results_tmp[results_tmp['model'] == chosen_config['model'].iloc[0]]\n",
    "            elif modeltype == 'TranAD':\n",
    "                if os.path.exists(f'studies_paramsearch_reco/data/{dataset}_TranAD_mcc.csv'):\n",
    "                    results_both[f'{modeltype}_{dataset}'] = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_TranAD_mcc.csv')\n",
    "                # if os.path.exists(f'studies_paramsearch_reco/data/{dataset}_TranAD_mcc.csv'):\n",
    "                #     results_both[f'{modeltype}_{dataset}'] = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_TranAD_mcc.csv')\n",
    "                # else:\n",
    "                #     continue\n",
    "            elif modeltype == 'None':\n",
    "                if os.path.exists(f'studies_paramsearch_reco/data/{dataset}_None_mcc.csv'):\n",
    "                    results_both[f'{modeltype}_{dataset}'] = pd.read_csv(f'studies_paramsearch_reco/data/{dataset}_None_mcc.csv')\n",
    "                else:\n",
    "                    continue\n",
    "print('loading done')\n",
    "    \n",
    "print(results_both.keys())\n",
    "# print(results_both['iTransformer-fc_creditcard_normal'])\n",
    "compare_models(results_both, datasets, modeltypes, datanames, metric='MCC', safe=False, reco=reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into a single DataFrame with an additional column for the keys\n",
    "combined_results = []\n",
    "\n",
    "for key, df in results_both.items():\n",
    "    df['key'] = key  # Add a column for the key\n",
    "    combined_results.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_results_df = pd.concat(combined_results, ignore_index=True)\n",
    "\n",
    "# Set 'key' as the index and drop 'Unnamed: 0' column\n",
    "combined_results_df = combined_results_df.set_index('key').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(combined_results_df)\n",
    "# combined_results_df.to_csv(f'studies_paramsearch_reco/configs/combined_results_transf_tranAD_eps10_none.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare anomaly labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =  ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "# ['creditcard_normal', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "# ['creditcard', 'creditcard_normal', 'GECCO', 'GECCO_normal', 'IEEECIS_new2.2', 'MSL_new', 'SMAP_new', 'SMD', 'SWaT', 'SWaT_1D', 'UCR', 'WADI']\n",
    "modeltype = 'iTransformer'\n",
    "reco = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.plasma(np.linspace(0, 1, 6))\n",
    "plt.rcParams.update({'lines.markersize': 8})\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    config = {}\n",
    "\n",
    "    if dataset in ['SWaT_1D', 'UCR']:\n",
    "        print(f'Skipping 1D datasets {dataset}')\n",
    "        continue\n",
    "    print(f'Loading {dataset}')\n",
    "    if reco:\n",
    "        if modeltype == 'Transformer':\n",
    "            paths = glob.glob(f'{modeltype}/{modeltype}_{dataset}/*' )\n",
    "        else:\n",
    "            paths = glob.glob(f'{modeltype}_loss_fct/{modeltype}_{dataset}/*' )\n",
    "    else:\n",
    "        paths = glob.glob(f'{modeltype}_loss_fct/{modeltype}_{dataset}/*' )\n",
    "    paths = [p for p in paths if not '.txt' in p]\n",
    "    paths = [p for p in paths if not 'Huber' in p]\n",
    "    paths = [p for p in paths if not 'Huber_quant' in p]\n",
    "    paths = [p for p in paths if not 'penalty' in p]\n",
    "    paths = sorted(paths)\n",
    "\n",
    "\n",
    "    for path in paths:\n",
    "        key = path.split('/')[2]\n",
    "        path = glob.glob(f'{path}/rep_*/results/')  # will give all 5 repetitions\n",
    "        path = sorted(path)\n",
    "        # print(len(path))\n",
    "\n",
    "        tmp = pd.DataFrame()\n",
    "        fig = plt.figure(figsize=(20, 7), constrained_layout=True)\n",
    "        for i, rep_path in enumerate(path):\n",
    "            if not os.path.exists(rep_path):\n",
    "                print(f'Skipping {key} rep {i+1}')\n",
    "                continue\n",
    "            res = pd.read_csv(os.path.join(rep_path, 'res_local_all.csv'))\n",
    "            res_glob = pd.read_csv(os.path.join(rep_path, 'res.csv')).iloc[-2:]\n",
    "            res.rename(columns={'Unnamed: 0': 'mode'}, inplace=True)\n",
    "            res_glob.rename(columns={'Unnamed: 0': 'mode'}, inplace=True)\n",
    "            res_glob.drop(columns=['threshold', 'Hit@100%', 'Hit@150%', 'NDCG@100%', 'NDCG@150%', 'train_time',\n",
    "                            'train_loss', 'test_loss'], inplace=True)\n",
    "\n",
    "            middle = math.ceil(res['mode'].iloc[-1]/2)\n",
    "            nb_dims = res['mode'].iloc[-1]\n",
    "            res = pd.concat((res, res_glob))\n",
    "            res.loc[res.index[-2], 'mode'] = middle\n",
    "            res.loc[res.index[-1], 'mode'] = nb_dims\n",
    "            res.sort_values(by='mode', inplace=True)\n",
    "            # print(res)\n",
    "\n",
    "            plt.plot(res['mode'][:-1], res['MCC'][:-1], '-o', color=colors[i], label=f'local rep {i}')\n",
    "            plt.plot(middle, res_glob['MCC'].iloc[-2], 's', color=colors[i], label=f'maj. voting rep {i}')\n",
    "            plt.plot(nb_dims, res_glob['MCC'].iloc[-1], 'D', color=colors[i], label=f'global rep {i}')\n",
    "            plt.ylim(top=1.0)\n",
    "        plt.xlabel('# of dimensions used to declare anomaly')\n",
    "        plt.ylabel('MCC')\n",
    "        plt.title(f'{dataset} ({nb_dims} dimensions)')\n",
    "\n",
    "        # Create legend handles for colors\n",
    "        color_handles = [\n",
    "            mpatches.Patch(color=colors[j], label=f'Run {j+1}') for j in range(5)\n",
    "        ]\n",
    "        # Create legend handles for marker shapes\n",
    "        shape_handles = [\n",
    "            mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label=f'local'),\n",
    "            mlines.Line2D([], [], color='black', marker='s', linestyle='None', markersize=10, label=f'maj. voting'),\n",
    "            mlines.Line2D([], [], color='black', marker='D', linestyle='None', markersize=10, label=f'global'),\n",
    "        ]\n",
    "\n",
    "        # plt.legend(bbox_to_anchor=(0.7, 0.95), loc='upper left')\n",
    "        if dataset in ['SMAP_new', 'SWaT']: \n",
    "            legend2 = plt.legend(handles=color_handles, title='Runs', bbox_to_anchor=(0.0, 0.0), loc='lower left')\n",
    "            plt.gca().add_artist(legend2)  # Add the first legend manually to the axes  \n",
    "            legend1 = plt.legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(0.15, 0.0), loc='lower left')  \n",
    "        else:\n",
    "            legend2 = plt.legend(handles=color_handles, title='Runs', bbox_to_anchor=(0.8, 1.0), loc='upper left')\n",
    "            plt.gca().add_artist(legend2)  # Add the first legend manually to the axes\n",
    "            legend1 = plt.legend(handles=shape_handles, title='Anomaly labels', bbox_to_anchor=(0.6, 1.0), loc='upper left')  \n",
    "            \n",
    "        plt.grid('gray', axis='y')\n",
    "        # if reco:\n",
    "        #     plt.savefig(f'studies_anomalylabels/{dataset}_reco.png', facecolor='w')\n",
    "        # else:\n",
    "        #     plt.savefig(f'studies_anomalylabels/{dataset}_fc.png', facecolor='w')\n",
    "        plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check whether there's correlations in training data that explains plots above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Shifted data filled with NaNs \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return datax.corr(datay.shift(lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    if dataset in ['SWaT_1D', 'UCR']:\n",
    "        print(f'Skipping 1D datasets {dataset}')\n",
    "        continue\n",
    "    elif dataset in ['IEEECIS_new2.2', 'WADI']:\n",
    "        feats = 30\n",
    "    else:\n",
    "        feats = -1\n",
    "\n",
    "    print(f'\\nLoading {dataset}')\n",
    "    \n",
    "    train = MyDataset(dataset, 10, 10, modeltype, flag='train', feats=feats, less=False, enc=False, k=-1)\n",
    "    features = train.feats\n",
    "    print(train.feats)\n",
    "    train = train.get_complete_data()\n",
    "    print(train.shape)\n",
    "\n",
    "    # # do PCA\n",
    "    # scaler = StandardScaler()\n",
    "    # train_scaled = scaler.fit_transform(train)\n",
    "    # pca = PCA()\n",
    "    # pca.fit(train_scaled)\n",
    "    # explained_variance = pca.explained_variance_ratio_\n",
    "    # idx = np.where(np.cumsum(explained_variance) >= 0.9)[0][0]\n",
    "    # print('how many PC needed to explain 90% of variance:', idx)\n",
    "\n",
    "    # # Plot the explained variance summed\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--', label='Explained variance')\n",
    "    # # plt.hlines(0.9, 0, len(explained_variance), color='black', alpha=0.8)\n",
    "    # plt.axvline(x=np.where(np.cumsum(explained_variance) >= 0.9)[0][0], color='red', linestyle='dashed', label=rf'$\\geq 90$% variance ($x={idx}$)')\n",
    "    # plt.title(f'PCA Components for {dataset}')\n",
    "    # plt.xlabel('Number of principal components')\n",
    "    # plt.ylabel('Cumulative explained variance')\n",
    "    # plt.legend()\n",
    "    # plt.ylim(0.2, 1.0)\n",
    "    # plt.grid()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f'./studies_anomalylabels/pca_{dataset}.png', facecolor='w')\n",
    "    # plt.show()\n",
    "\n",
    "    constfeat = np.where(np.var(train, axis=0)==0)[0]\n",
    "    print('constant feature:', constfeat)\n",
    "    print(f'# of const features', len(constfeat))\n",
    "    train = np.delete(train, constfeat, axis=1) \n",
    "    features = train.shape[1]\n",
    "    # print(train.shape)\n",
    "    # corr_matrix = np.corrcoef(train.T)\n",
    "\n",
    "    # print(np.any(np.isnan(corr_matrix)))\n",
    "    # # Count the number of columns that are very lowly correlated with all others\n",
    "    # low_corr_cols = np.sum(np.mean(corr_matrix, axis=0) < 0.1)\n",
    "    # print(f'Number of columns very lowly correlated with all others: {low_corr_cols}')\n",
    "    # print(f'Indices of very lowly correlated columns: {np.where(np.mean(corr_matrix, axis=0) < 0.1)[0]}')\n",
    "    \n",
    "    # # Count the number of columns that are very highly correlated with all others\n",
    "    # high_corr_cols =np.sum(np.mean(corr_matrix, axis=0) > 0.5)\n",
    "    # print(f'Number of columns very highly correlated with all others: {high_corr_cols}')\n",
    "    # print(f'Indices of very highly correlated columns: {np.where(np.mean(corr_matrix, axis=0) > 0.9)[0]}')\n",
    "\n",
    "    # plt.imshow(corr_matrix, cmap='Blues', interpolation='nearest', vmin=0, vmax=1)\n",
    "    # plt.colorbar(shrink=0.7)\n",
    "    # plt.title(f'Correlation matrix for {dataset}')\n",
    "    # # plt.xticks(ticks=np.arange(features), labels=np.arange(1, features + 1))\n",
    "    # # plt.yticks(ticks=np.arange(features), labels=np.arange(1, features + 1))\n",
    "    # plt.tight_layout()\n",
    "    # # plt.savefig(f'./studies_anomalylabels/corr_matrix_{dataset}.png', facecolor='w')\n",
    "    # plt.show()\n",
    "\n",
    "    # cross correlation across dimensions\n",
    "    crosscorr_matrix = np.zeros((features, features))\n",
    "    for lag in [1, 5, 10, 20]:\n",
    "        for i in range(features):\n",
    "            for j in range(features):\n",
    "                x = pd.Series(train[:, i])\n",
    "                y = pd.Series(train[:, j])\n",
    "                crosscorr_matrix[i, j] = crosscorr(x, y, lag=lag)\n",
    "        low_corr_cols = np.sum(np.mean(crosscorr_matrix, axis=0) < 0.1)\n",
    "        print(f'Number of columns very lowly correlated with all others: {low_corr_cols}')\n",
    "        print(f'Indices of very lowly correlated columns: {np.where(np.mean(crosscorr_matrix, axis=0) < 0.1)[0]}')\n",
    "        # Count the number of columns that are very highly correlated with all others\n",
    "        high_corr_cols =np.sum(np.mean(crosscorr_matrix, axis=0) > 0.5)\n",
    "        print(f'Number of columns very highly correlated with all others: {high_corr_cols}')\n",
    "        print(f'Indices of very highly correlated columns: {np.where(np.mean(crosscorr_matrix, axis=0) > 0.9)[0]}')\n",
    "\n",
    "    \n",
    "        plt.imshow(crosscorr_matrix, cmap='Blues', interpolation='nearest', vmax=1)    \n",
    "        plt.colorbar(shrink=0.7)\n",
    "        plt.title(f'Cross-correlation matrix for {dataset}, lag={lag}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./studies_anomalylabels/cross_corr_matrix_{dataset}_lag{lag}.png', facecolor='w')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## studies on IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for GECCO_normal data set\n",
    "percentile = [0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 0.99, 0.999]\n",
    "mccscores = [0.426, 0.364, 0.623, 0.628, 0.652, 0.607, 0.558, 0.610, 0.574, 0.545]\n",
    "mccscores2 = [0.398, 0.26, 0.575, 0.563, 0.377, 0.557, 0.533, 0.574, 0.337, 0.547]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hlines(y=0.74, xmin=0, xmax=1, color='black', linestyle='--', label='anomaly score = MSE = $(y_{pred} - y_{true})^2$')\n",
    "plt.plot(percentile, mccscores, '-o', label='anomaly score = $IQR_r = |y_{(1-r/2)} - y_{r/2}|$')\n",
    "plt.plot(percentile, mccscores2, '-o', label='anomaly score = max$(|y_{pred} - y_{(1-r/2)}|, |y_{pred} - y_{r/2}|)$')\n",
    "plt.legend()\n",
    "plt.ylim(0.2, 1.)\n",
    "plt.xlabel(r'Range $r$ for $IQR_r$')\n",
    "plt.ylabel('MCC')\n",
    "plt.title('GECCO_normal')\n",
    "plt.grid('gray', axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcc(tp, tn, fp, fn):\n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    if denominator == 0:\n",
    "        return 0  # To handle division by zero\n",
    "    return numerator / denominator\n",
    "\n",
    "# Example usage:\n",
    "# Confusion matrix values\n",
    "## local incl or\n",
    "# tp = 48023  # True Positives\n",
    "# tn = 352809  # True Negatives\n",
    "# fp = 27163  # False Positives\n",
    "# fn = 7831   # False Negatives\n",
    "# global\n",
    "tp = 41442  # True Positives\n",
    "tn = 371259 # True Negatives\n",
    "fp = 8713  # False Positives\n",
    "fn = 14412   # False Negatives\n",
    "\n",
    "\n",
    "mcc = compute_mcc(tp, tn, fp, fn)\n",
    "print(f\"MCC Score: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the range of x values\n",
    "# x = torch.linspace(-4, 4, 100)\n",
    "\n",
    "# # Compute activation function values\n",
    "# y_relu = F.relu(x)\n",
    "# y_leaky_relu = F.leaky_relu(x, negative_slope=0.1)\n",
    "# y_sigmoid = torch.sigmoid(x)\n",
    "# y_tanh = torch.tanh(x)\n",
    "# y_elu = F.elu(x, alpha=1.0)\n",
    "# y_softsign = F.softsign(x)\n",
    "# # List of activation functions and their names\n",
    "# activations = [\n",
    "#     (y_relu, 'ReLU', r'$g(z) = $max$(0, z)$'),\n",
    "#     (y_leaky_relu, 'Leaky ReLU', r'$g(z) = $ max$(0.1z, z)$'),\n",
    "#     (y_sigmoid, 'Sigmoid', r'$g(z) = 1/(1+e^{-z})$'),\n",
    "#     (y_tanh, 'Tanh', r'$g(z) = $tanh$(z)$'),\n",
    "#     (y_elu, 'ELU', r'$g(z) = z$ if $z>0$'f'\\n'r'$g(z)=e^z-1$ elsewhere'),\n",
    "#     (y_softsign, 'Softsign', r'$g(z) = z/(1+|z|)$')\n",
    "# ]\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(16, 10), sharex=True, sharey=True)\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# colors = plt.cm.plasma(np.linspace(0, 1, len(activations)+1))\n",
    "\n",
    "# for i, (y, name, equation) in enumerate(activations):\n",
    "#     axes[i].plot(x.numpy(), y.numpy(), label=fr'{equation}', color=colors[i])\n",
    "#     axes[i].axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "#     axes[i].axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "#     axes[i].legend(loc='upper left')\n",
    "#     axes[i].grid()\n",
    "#     axes[i].set_title(name)\n",
    "# fig.supxlabel(r\"$z$\")\n",
    "# fig.supylabel(r\"Activation function $g(z)$\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTransf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
