{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "plt.style.use(hep.style.firamath)\n",
    "plt.rcParams['lines.markersize'] = 4\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, features, y=None, path=None):\n",
    "    # plt.rcParams[\"text.usetex\"] = False\n",
    "    # # plt.rcParams['figure.figsize'] = 6, 2\n",
    "    # # plt.rcParams['lines.markersize'] = 2\n",
    "    # plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "    size = int(len(features)*1.5)   # old *0.6)\n",
    "    dims = len(features)\n",
    "    if y is not None:\n",
    "        dims += 1\n",
    "\n",
    "    fig, axs = plt.subplots(dims, 1, figsize=(17, size), sharex=True)\n",
    "\n",
    "    for dim, feat in enumerate(features):  # iterate through the features we're using\n",
    "        # print(feat)\n",
    "        x_t = X[:, dim]\n",
    "        axs[dim].plot(x_t)\n",
    "        # if y is not None:\n",
    "        #     y_scaled = np.max(x_t)*y\n",
    "        #     axs[dim].plot(y_scaled, '--', linewidth=1, color='tab:orange', label='anomalies')\n",
    "        # turn ylabel by 90 degrees and shift it to the left\n",
    "        axs[dim].set_ylabel(feat, rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        # Align all y-axis labels by setting the same label coordinates\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        # axs[dim].legend(loc='upper right')\n",
    "        # axs[dim].set_ylim(-1,1)\n",
    "    if y is not None: # plot the target variable in last dimension if we have truth labels\n",
    "        axs[-1].plot(y, '--', color='tab:red')\n",
    "        # axs[-1].set_yticks([0, 1])\n",
    "        axs[-1].set_ylabel('anomalies', rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "    axs[-1].set_xlabel('Events')  # looks alignment-wise better than supxlabel\n",
    "    # fig.supylabel('values')\n",
    "    # fig.supxlabel('Time stamp [min]')\n",
    "    plt.tight_layout()\n",
    "    if path is not None:\n",
    "        plt.savefig(f'{path}.png', dpi=100, facecolor='white')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclipse data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_data.hdf', 'r')\n",
    "f_test = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_data.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_train.keys())\n",
    "print(f_train.attrs.keys())\n",
    "\n",
    "print(f_train['prod_train_data'].keys())\n",
    "print(f_train['prod_train_data']['axis0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_train as file:\n",
    "    for key in file['prod_train_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_train_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_test as file:\n",
    "    for key in file['prod_test_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_test_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_label.csv')\n",
    "label_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_test.shape, label_train.shape)\n",
    "print(label_train)\n",
    "y_train = label_train['binary_anom']\n",
    "print(y_train[y_train==1])\n",
    "y_test = label_test['binary_anom']\n",
    "print(y_test[y_test==1], '\\n', len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_test\n",
    "label['job_id'] = label['job_id'].astype('str')\n",
    "label['component_id'] = label['component_id'].astype('str') \n",
    "label.set_index(['job_id', 'component_id'], inplace=True)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEE-CIS fraud detection data set (from kaggle challenge 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_transaction.csv')\n",
    "x_train2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_train2.shape)\n",
    "print(x_train.columns) #, x_train2.columns)\n",
    "\n",
    "for i, name in enumerate(x_train.columns):\n",
    "    print(name)\n",
    "    # print(x_train[name].dtypes)\n",
    "\n",
    "# Filter columns that are of type 'float'\n",
    "float_cols = x_train.select_dtypes(include='float').columns\n",
    "# print(\"Columns with float values:\", float_cols)\n",
    "# for i, name in enumerate(float_cols.columns):\n",
    "#     print(name)\n",
    "\n",
    "# Filter columns that are NOT of type 'float'\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(by='TransactionDT')\n",
    "x_train = x_train.drop(columns=non_num_cols)\n",
    "y_train = x_train['isFraud']\n",
    "x_train = x_train.drop(columns='isFraud')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(0)\n",
    "has_nan = y_train.isnull().any()\n",
    "print(has_nan)\n",
    "print(x_train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "print(max(X_train[0]))\n",
    "X_train, min_a, max_a = normalize3(X_train, min_a=None)\n",
    "print(max(X_train[0]))\n",
    "X_test, _, _ = normalize3(X_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for dim in range(5):\n",
    "    x_t, l = X_train[:10000, dim], y_train[:10000]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(x_t, label='data')\n",
    "    plt.plot(l, '--', linewidth=0.2)\n",
    "    plt.fill_between(np.arange(l.shape[0]), l, color='tab:orange', alpha=0.3, label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/train_1.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/test_1.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/labels_1.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['day'] = x_train['TransactionDT'] / (24 * 60 * 60)  # to convert seconds to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['uid1'] =  (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str)\n",
    "# x_train['uid2'] =  (x_train.card1.astype(str) +'_' + \\\n",
    "#             x_train.addr1.astype(str) +'_' + \\\n",
    "#             (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train['uid1'])\n",
    "# print(x_train['uid1'].unique)\n",
    "# print(x_train['uid2'])\n",
    "# print(x_train['uid2'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_transaction.csv')\n",
    "x_test2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_identity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.columns)\n",
    "print(x_test.shape)\n",
    "\n",
    "for i, name in enumerate(x_test.columns):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.sort_values(by='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEEE-CIS challenge from kaggle loaded from fraud-dataset-benchmark (contains UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data/ieeecis/train.csv')\n",
    "x_test = pd.read_csv('data/ieeecis/test.csv')\n",
    "labels = pd.read_csv('data/ieeecis/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "# print(x_train.columns, x_test.columns)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtypes)\n",
    "\n",
    "# Filter columns that are NOT of type 'float' and for now pop them out\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)\n",
    "    # if name not in ['ENTITY_ID', 'EVENT_TIMESTAMP']:\n",
    "    #     x_train.pop(name)\n",
    "    #     x_test.pop(name)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(['EVENT_TIMESTAMP'])\n",
    "x_test = x_test.sort_values(['EVENT_TIMESTAMP'])\n",
    "date_train = x_train.pop('EVENT_TIMESTAMP')\n",
    "print(date_train.shape, date_train)\n",
    "date_test = x_test.pop('EVENT_TIMESTAMP')\n",
    "print(date_test.shape, date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(x_train.columns)\n",
    "feature_names.remove('ENTITY_ID') # because we don't want to use them as a feature\n",
    "feature_names.remove('TransactionID') # because we don't want to use them as a feature\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for train data\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 50\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "print(len(train_uid), train_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert each group to a 2D numpy array and stack them into a list of 2D data frames, first approach\n",
    "# x_train3 = [group.drop(columns=['ENTITY_ID', 'TransactionID']).values for name, group in grouped if len(group) >= 50]\n",
    "\n",
    "# print(len(x_train3), len(train_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach, here first plotting\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):    # enumerate(short_list): \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "    print(feat)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) >= 50:\n",
    "            # print(name, group)\n",
    "            df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                \n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        # for plotting\n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if unique_values.shape[0] > 25:  # take 25 most frequent values for plots\n",
    "            idx = np.argsort(counts)[::-1][:25]\n",
    "            unique_values = unique_values[idx]\n",
    "            counts = counts[idx]\n",
    "            lab = f'showing 25 most frequent \\nof {len(np.unique(tmp))} unique values'\n",
    "        else:\n",
    "            lab = f'{len(np.unique(tmp))} unique values'\n",
    "        \n",
    "        plt.bar(unique_values, counts, label=lab)\n",
    "        if feat in ['p_emaildomain', 'r_emaildomain', 'deviceinfo']:\n",
    "            plt.xticks(rotation=50, ha='right')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print(np.max(tmp), np.min(tmp))\n",
    "\n",
    "        plt.hist(tmp, bins=50, range=[np.quantile(tmp,0.03), np.quantile(tmp,0.97)], label=f'mean: {np.mean(tmp):.1f}, std: {np.std(tmp):.1f} \\nmin: {np.min(tmp):.1f}, max: {np.max(tmp):.1f}')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D arrays, second approach\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "other_values = {}\n",
    "encoding = {}\n",
    "# means = {}  # next step\n",
    "# std = {}\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):  \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "\n",
    "    # for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "    #     if len(group) >= 50:\n",
    "    #         # print(name, group)\n",
    "    #         df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "    #         if feat in non_num_cols:\n",
    "    #             df[feat] = df[feat].fillna('missing')\n",
    "    #         else:\n",
    "    #             df[feat] = df[feat].fillna(0)\n",
    "    #         tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    # print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        print(feat)\n",
    "        for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "            if len(group) >= 50:\n",
    "                # print(name, group)\n",
    "                df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                tmp = np.concatenate((tmp, df[feat].values))\n",
    "\n",
    "        # for encoding \n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if len(unique_values) > 50:  # take 100 most frequent values for encoding\n",
    "            idx = np.argsort(counts)\n",
    "            unique_values = unique_values[idx]  # unique values sorted in frequency\n",
    "            other_values[feat] = unique_values[49:]   # values to be encoded as 'other'\n",
    "            print(other_values)\n",
    "            # replace all elements of tmp that are in other_values with 'other'\n",
    "            idx = np.where(np.isin(tmp, other_values[feat]))\n",
    "            tmp[idx] = 'other'\n",
    "            # print(len(np.unique(tmp)))\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore').fit(tmp.reshape(-1, 1))\n",
    "        print(enc.categories_, len(enc.categories_[0]))\n",
    "        encoding[feat] = enc\n",
    "\n",
    "print(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach\n",
    "x_train3 = []\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "for name, group in grouped:  # second get all transactions in train data keeping structure of uid\n",
    "    if len(group) >= 50:\n",
    "        # print(name, group)\n",
    "        df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_train3.append(df.values)\n",
    "\n",
    "print(len(x_train3), len(train_uid))\n",
    "updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train3), x_train3[0].shape, x_train3[0])\n",
    "print(len(train_uid))\n",
    "print(len(updated_feature_names), updated_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach: for test data now\n",
    "x_test3 = []\n",
    "\n",
    "grouped_test = x_test.groupby('ENTITY_ID')\n",
    "test_uid = grouped_test.size()[grouped_test.size() >= 20].index  # allow shorter time series for testing!\n",
    "test_uid = list(test_uid)\n",
    "for name, group in grouped_test:  # second get all transactions in test data keeping structure of uid\n",
    "    if len(group) >= 20:\n",
    "        df = group.drop(columns=['ENTITY_ID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_test3.append(df.values)\n",
    "\n",
    "print(len(x_test3))\n",
    "# updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_train3):\n",
    "    print(f'time series for user: {train_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/train_{train_uid[i]}_{j}')\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # arr_scaled = scaler.fit_transform(arr)\n",
    "    # plot_data(arr_scaled, feature_names, path=f'plots_data/IEEECIS_new/train_scaled_{train_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/train_{train_uid[i]}.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_{j}')\n",
    "    \n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = arr[:, 1:]                    # remove the transaction IDs from test data\n",
    "    print(arr.shape)\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # if test_uid[i] == '13623.0_498.0_117.0':\n",
    "    #     for j in range(0, len(updated_feature_names), 20):\n",
    "    #         plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_{j}')\n",
    "    # else:\n",
    "    #     plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_0')\n",
    "    plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/test_{test_uid[i]}.npy', arr)\n",
    "    np.save(f'processed/IEEECIS_new2/labels_{test_uid[i]}.npy', y_test)\n",
    "    # np.save(f'processed/IEEECIS_new2/ids_{test_uid[i]}.npy', test_transaction_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for test data\n",
    "grouped = x_test.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 20\n",
    "test_uid = grouped.size()[grouped.size() >= 20].index\n",
    "test_uid = list(test_uid)\n",
    "print(test_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n",
    "\n",
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames\n",
    "x_test3 = [group.drop(columns='ENTITY_ID').values for name, group in grouped if len(group) >= 20]\n",
    "\n",
    "print(len(x_test3), len(test_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the truth labels of the test data\n",
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = np.nan_to_num(arr)            # test data\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # scaler already defined for train data? or better to redefine?\n",
    "    arr_scaled = scaler.fit_transform(arr)\n",
    "\n",
    "    print(arr_scaled.shape, y_test.shape)\n",
    "\n",
    "    if len(y_test[y_test==1]) > 0:\n",
    "        print(test_uid[i])\n",
    "        plot_data(arr, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}')\n",
    "        plot_data(arr_scaled, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_scaled{test_uid[i]}')\n",
    "    \n",
    "        np.save(f'processed/IEEECIS_new/test_scaled_{test_uid[i]}.npy', arr_scaled)\n",
    "        np.save(f'processed/IEEECIS_new/labels_{test_uid[i]}.npy', y_test)\n",
    "        np.save(f'processed/IEEECIS_new/ids_{test_uid[i]}.npy', test_transaction_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATLAS time series data for AD in LAr data (from Vilius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5py.File('data/ATLAS_TS/user.vcepaiti.38697672.EXT0._000002.AnnotatorNtuple.h5', 'r')\n",
    "f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.Good.04May2024.00361862.physics_Main_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.SevNois.14May2024.00462542.physics_UPC.r15414_p6133_EXT0_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.Good.16May2024.00361862.physics_CosmicCalo.r14138_p5427_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.HVNONNOMINAL.03Jun2024.00430897.physics_Main.r15052_p5604_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.PumpNoise.04Jun2024.00451140.physics_Main.r14858_p5785_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.SevNois.14May2024.00462542.physics_HardProbes.f1399_m2209_EXT0.h5', 'r')\n",
    "print(f.keys(), f.attrs.keys())\n",
    "data = np.array(f['data'])\n",
    "lumiblock = np.array(f['lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, data[0].shape, data[0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f['event_labels'], f['event_numbers'], f['features'], f['run'])\n",
    "evt_lb = np.array(f['event_labels'])\n",
    "print(evt_lb.shape, evt_lb[0].shape)\n",
    "print(np.min(evt_lb[2]), np.max(evt_lb[2]))\n",
    "print(np.unique(evt_lb[2]))\n",
    "print(np.any(np.logical_and(evt_lb != 0, evt_lb !=2)))\n",
    "evt_nb = np.array(f['event_numbers'])\n",
    "print(evt_nb.shape, evt_nb[0].shape)\n",
    "features = np.array(f['features'])\n",
    "print(features)\n",
    "run = np.array(f['run'])\n",
    "print(run.shape)\n",
    "lb = np.array(f['lb'])\n",
    "print(lb.shape)\n",
    "print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 802 for CosmicCalo, 504 for HVNONNOMINAL, 787 for PumpNoise, 312 for HardProbes\n",
    "# lb_idx = np.where(lb == 787)[0][0]  \n",
    "# print(lb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if run number the same over all runs\n",
    "print(np.unique(run))\n",
    "# check if all runs are actually the same\n",
    "events = 0\n",
    "for i in range(data.shape[0]):\n",
    "    # print(data[i,0].shape)\n",
    "    # print(np.all(data[0,0] ==data[i,0]))\n",
    "    events += data[i,0].shape[0]\n",
    "    if data[i,0].shape[0] > 1800 and data[i,0].shape[0] < 2000:\n",
    "        print(i)\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(16, 1, figsize=(10, 25), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        l = evt_lb[lb]\n",
    "        l1 = (l==1) + 0\n",
    "        l2 = (l==2) + 0\n",
    "        l3 = (l==3) + 0\n",
    "        # print(x_t.shape, x_t[0].shape)\n",
    "        axs[dim].plot(x_t, label='data')\n",
    "        axs[dim].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "        axs[dim].plot(l2, '--', linewidth=1, color='tab:red', label='noise burst')\n",
    "        axs[dim].plot(l3, '--', linewidth=1, color='tab:purple', label='data corruption')\n",
    "        axs[dim].set_ylabel(str(features[dim])[2:-1])\n",
    "    axs[0].legend()\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(17, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "        axs[dim].legend(loc='upper right')\n",
    "        # axs[dim].set_title(features[dim][2:])\n",
    "    \n",
    "    l = evt_lb[lb]      # same label for all dimensions/features\n",
    "    l1 = (l==1) + 0\n",
    "    l2 = (l==2) + 0\n",
    "    l3 = (l==3) + 0\n",
    "    axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "    axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "    axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "    axs[-1].legend(loc='upper right')\n",
    "    # fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    features[f] = str(features[f])[2:-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "x_t = np.stack(data[lb_idx])\n",
    "x_t = x_t.T\n",
    "y_t = evt_lb[lb_idx]\n",
    "print(np.max(y_t), np.min(y_t))\n",
    "print(x_t.shape, y_t.shape)\n",
    "plot_data(x_t, features, y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(data.shape[0]):\n",
    "    x_t = np.stack(data[l])\n",
    "    x_t = x_t.T\n",
    "    y_t = evt_lb[l]\n",
    "    lb_idx = lb[l]\n",
    "    # print(lb_idx, x_t.shape, y_t.shape)\n",
    "    np.save(f'processed/ATLAS_DQM_TS/train_Main_{lb_idx}.npy', x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly save as train data\n",
    "# np.save('processed/ATLAS_DQM_TS/test_cosmicCalo.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_cosmicCalo.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_hvononNominal.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_hvononNominal.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_pumpNoise.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_pumpNoise.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_hardProbesnpy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_hardProbes.npy', y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(5):\n",
    "    labels[f'lumiblock_{i}'] = (evt_lb[i] > 0) + 0\n",
    "    l = labels[f'lumiblock_{i}']\n",
    "    print(len(l[l>0]), len(l[l==0]), len(l[l>0])/len(l[l==0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].shape)\n",
    "temp = np.stack(data[0])\n",
    "print(np.array(data).shape, temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for lb in range(5):\n",
    "    \n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x[:,0].shape)\n",
    "    print(np.max(x[:,0]), np.min(x[:,0]), np.mean(x[:,0]), np.std(x[:,0]))\n",
    "    y = labels[f'lumiblock_{lb}']\n",
    "    print(x.shape, y.shape)\n",
    "    scaler = StandardScaler()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    print(np.max(x_norm[:,0]), np.min(x_norm[:,0]), np.mean(x_norm[:,0]), np.std(x_norm[:,0]))  \n",
    "\n",
    "    plt.plot(x_norm[:,0], label='normalised')\n",
    "    plt.plot(x[:,0], label='raw')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "\n",
    "x_train = np.stack(data[0]).T\n",
    "for lb in range(1,4):\n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x.shape)\n",
    "    x_train = np.concatenate((x_train, x), axis=0)\n",
    "    print(x_train.shape)\n",
    "x_test = np.stack(data[4]).T\n",
    "y_test = np.array(labels['lumiblock_4'][:, np.newaxis])\n",
    "y_test = np.repeat(y_test, repeats=16, axis=1)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardise the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# # print(max(x[0]))\n",
    "# # x, min_a, max_a = normalize3(x_train, min_a=None)\n",
    "# # print(max(x_train[0]))\n",
    "# # x_test, _, _ = normalize3(x_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "fig, axs = plt.subplots(16, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "for dim in range(16):  # iterate through 16 features we're using\n",
    "    x_t = x_train[:, dim]   \n",
    "    axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "    axs[dim].legend(loc='upper right')\n",
    "    # axs[dim].set_title(features[dim][2:])\n",
    "\n",
    "# l = y_train      # same label for all dimensions/features\n",
    "# l1 = (l==1) + 0\n",
    "# l2 = (l==2) + 0\n",
    "# l3 = (l==3) + 0\n",
    "# axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "# axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "# axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "axs[-1].legend(loc='upper right')\n",
    "fig.supylabel('label')\n",
    "fig.supxlabel('events')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'processed/ATLAS_TS/train.npy', x_train)\n",
    "np.save(f'processed/ATLAS_TS/test.npy', x_test)\n",
    "np.save(f'processed/ATLAS_TS/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in range(5):\n",
    "    x = np.stack(data[lb]).T\n",
    "    y = np.array(labels[f'lumiblock_{lb}'][:, np.newaxis])\n",
    "    y = np.repeat(y, repeats=16, axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_train.npy', x)\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    \n",
    "    # if lb < 4:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_train.npy', x)\n",
    "    #     # np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    # else:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_test.npy', x)\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set on water quality (GECCO IoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/3884398/1_gecco2018_water_quality.csv')\n",
    "data = data.sort_values(by='Time')\n",
    "print(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Time', 'Unnamed: 0', 'EVENT'], axis=1)\n",
    "y = data.EVENT\n",
    "time = data.Time\n",
    "features = X.columns\n",
    "print(X.shape, y.shape)\n",
    "print(features)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.to_datetime(time, unit='s')\n",
    "print(time)\n",
    "\n",
    "# Assuming `time.values` is a list or array of datetime objects\n",
    "df_stamp = pd.DataFrame({'date': list(time.values)})\n",
    "\n",
    "# Extract datetime components and process\n",
    "df_stamp['month'] = df_stamp['date'].apply(lambda row: row.month)\n",
    "df_stamp['day'] = df_stamp['date'].apply(lambda row: row.day)\n",
    "# df_stamp['weekday'] = df_stamp['date'].apply(lambda row: row.weekday())\n",
    "df_stamp['hour'] = df_stamp['date'].apply(lambda row: row.hour)\n",
    "# df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute)\n",
    "df_stamp['minute'] = df_stamp['date'].apply(lambda row: row.minute // 15)\n",
    "\n",
    "# Drop the 'date' column and convert the remaining data to values\n",
    "data_stamp = df_stamp.drop(columns=['date']).values\n",
    "\n",
    "# Print statements\n",
    "print(df_stamp)\n",
    "print(data_stamp)\n",
    "print(data_stamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y+0)\n",
    "print(len(y[y==1]), len(y[y==0]), len(y[y==1])/len(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X, features, y, 'plots_data/GECCO/data_all_withnan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initially just 0 padded nan values\n",
    "# pbm = np.where(np.isnan(X))[0]\n",
    "# print(pbm)  \n",
    "# print(len(pbm))\n",
    "# X = np.nan_to_num(X, nan=0.0)\n",
    "# print(np.isnan(X).any())\n",
    "\n",
    "# replace nan values with previous value (forward fill)\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(0, X.shape[0]):\n",
    "        if np.isnan(X[j,i]):\n",
    "            X[j,i] = X[j-1,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X, features, y, 'plots_data/GECCO/data_all5_withoutnan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(X)*0.5)\n",
    "print(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:cut]\n",
    "X_test = X[cut:]\n",
    "y_train = y[:cut]\n",
    "y_test = y[cut:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out anomalies of train\n",
    "idx = np.where(y_train == 0)\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_train = data_stamp[:cut]\n",
    "# time_test = data_stamp[cut:]\n",
    "# print(time_train.shape, time_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need y to be 2D\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y[y==1])/len(y))\n",
    "print(len(y_train[y_train==1])/len(y_train), len(y_test[y_test==1])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(9):\n",
    "    if dim > 0: \n",
    "        x_up = np.percentile(X_train[:,dim], 98)\n",
    "        x_low = np.percentile(X_train[:,dim], 2)\n",
    "        plt.hist(X_train[:,dim], histtype='step', range=[x_low, x_up], label='train', linewidth=2, bins=30)\n",
    "        plt.hist(X_test[:,dim], histtype='step', range=[x_low, x_up], label='test', linewidth=2, bins=30)\n",
    "    else:\n",
    "        plt.hist(X_train[:,dim], histtype='step', label='train', linewidth=2, bins=30)\n",
    "        plt.hist(X_test[:,dim], histtype='step', label='test', linewidth=2, bins=30)\n",
    "    # plt.hist(X_train[:,dim], histtype='step', label='train', linewidth=2, bins=30)\n",
    "    # plt.hist(X_test[:,dim], histtype='step', label='test', linewidth=2, bins=30)\n",
    "    plt.title(features[dim])\n",
    "    plt.legend()\n",
    "    # plt.savefig(f'plots_data/GECCO/feature_distr_forwardfill/{features[dim]}_7.png', dpi=300, facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip data to [2, 98] percentile of train data (except for first feature)\n",
    "for dim in range(9):\n",
    "    if dim > 0:\n",
    "        x_up = np.percentile(X_train[:,dim], 98)\n",
    "        x_low = np.percentile(X_train[:,dim], 2)\n",
    "        X_train[:,dim] = np.clip(X_train[:,dim], x_low, x_up)\n",
    "        X_test[:,dim] = np.clip(X_test[:,dim], x_low, x_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X_train, features, y_train, 'plots_data/GECCO/data_train1')\n",
    "# plot_data(X_test, features, y_test, 'plots_data/GECCO/data_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(len(y_test[y_test==1]), len(y_test[y_test==0]), len(y_test[y_test==1])/len(y_test))   \n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled = np.clip(X_train_scaled, -5, 5)\n",
    "# X_test_scaled = np.clip(X_test_scaled, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train_scaled, features, y_train, 'plots_data/GECCO/data_train_norm1')\n",
    "plot_data(X_test_scaled, features, y_test, 'plots_data/GECCO/data_test_norm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = np.concatenate((time_train, X_train_scaled), axis=1)\n",
    "print(X_train_scaled.shape)\n",
    "X_test_scaled = np.concatenate((time_test, X_test_scaled), axis=1)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('processed/GECCO_normal/train.npy', X_train_scaled)\n",
    "# np.save('processed/GECCO_normal/test.npy', X_test_scaled)\n",
    "# np.save('processed/GECCO_normal/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## credit card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31) Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/creditcard/creditcard.csv')\n",
    "print(data.shape, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 29) (284807,)\n",
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
      "      dtype='object')\n",
      "0              0.0\n",
      "1              0.0\n",
      "2              1.0\n",
      "3              1.0\n",
      "4              2.0\n",
      "            ...   \n",
      "284802    172786.0\n",
      "284803    172787.0\n",
      "284805    172788.0\n",
      "284804    172788.0\n",
      "284806    172792.0\n",
      "Name: Time, Length: 284807, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data.sort_values(by='Time', inplace=True)\n",
    "X = data.drop(['Time', 'Class'], axis=1)\n",
    "y = data.Class\n",
    "time = data.Time\n",
    "features = X.columns\n",
    "print(X.shape, y.shape)\n",
    "print(features)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n",
      "0.0017304750013189597\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())\n",
    "print(492/284315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1970-01-01 00:00:00\n",
      "1        1970-01-01 00:00:00\n",
      "2        1970-01-01 00:00:01\n",
      "3        1970-01-01 00:00:01\n",
      "4        1970-01-01 00:00:02\n",
      "                 ...        \n",
      "284802   1970-01-02 23:59:46\n",
      "284803   1970-01-02 23:59:47\n",
      "284805   1970-01-02 23:59:48\n",
      "284804   1970-01-02 23:59:48\n",
      "284806   1970-01-02 23:59:52\n",
      "Name: Time, Length: 284807, dtype: datetime64[ns]\n",
      "                      date  month  day  hour  minute\n",
      "0      1970-01-01 00:00:00      1    1     0       0\n",
      "1      1970-01-01 00:00:00      1    1     0       0\n",
      "2      1970-01-01 00:00:01      1    1     0       0\n",
      "3      1970-01-01 00:00:01      1    1     0       0\n",
      "4      1970-01-01 00:00:02      1    1     0       0\n",
      "...                    ...    ...  ...   ...     ...\n",
      "284802 1970-01-02 23:59:46      1    2    23       3\n",
      "284803 1970-01-02 23:59:47      1    2    23       3\n",
      "284804 1970-01-02 23:59:48      1    2    23       3\n",
      "284805 1970-01-02 23:59:48      1    2    23       3\n",
      "284806 1970-01-02 23:59:52      1    2    23       3\n",
      "\n",
      "[284807 rows x 5 columns]\n",
      "[[ 1  1  0  0]\n",
      " [ 1  1  0  0]\n",
      " [ 1  1  0  0]\n",
      " ...\n",
      " [ 1  2 23  3]\n",
      " [ 1  2 23  3]\n",
      " [ 1  2 23  3]]\n",
      "(284807, 4)\n"
     ]
    }
   ],
   "source": [
    "time = pd.to_datetime(time, unit='s')\n",
    "print(time)\n",
    "\n",
    "# Assuming `time.values` is a list or array of datetime objects\n",
    "df_stamp = pd.DataFrame({'date': list(time.values)})\n",
    "\n",
    "# Extract datetime components and process\n",
    "df_stamp['month'] = df_stamp['date'].apply(lambda row: row.month)\n",
    "df_stamp['day'] = df_stamp['date'].apply(lambda row: row.day)\n",
    "# df_stamp['weekday'] = df_stamp['date'].apply(lambda row: row.weekday())\n",
    "df_stamp['hour'] = df_stamp['date'].apply(lambda row: row.hour)\n",
    "# df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute)\n",
    "df_stamp['minute'] = df_stamp['date'].apply(lambda row: row.minute // 15)\n",
    "\n",
    "# Drop the 'date' column and convert the remaining data to values\n",
    "data_stamp = df_stamp.drop(columns=['date']).values\n",
    "\n",
    "# Print statements\n",
    "print(df_stamp)\n",
    "print(data_stamp)\n",
    "print(data_stamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199364\n"
     ]
    }
   ],
   "source": [
    "cut = int(len(X)*0.7)\n",
    "print(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199364, 29) (85443, 29) (199364,) (85443,)\n",
      "Class\n",
      "0    198980\n",
      "1       384\n",
      "Name: count, dtype: int64 0.0019261250777472363\n",
      "Class\n",
      "0    85335\n",
      "1      108\n",
      "Name: count, dtype: int64 0.0012640005617780275\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:cut]\n",
    "X_test = X[cut:]\n",
    "y_train = y[:cut]\n",
    "y_test = y[cut:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train.value_counts(), y_train.value_counts()[1]/len(y_train))\n",
    "print(y_test.value_counts(), y_test.value_counts()[1]/len(y_test))\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198980, 29) (198980,)\n",
      "0 198980 0.0\n"
     ]
    }
   ],
   "source": [
    "# take out anomalies of train\n",
    "idx = np.where(y_train == 0)\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train = data_stamp[:cut]\n",
    "time_test = data_stamp[cut:]\n",
    "print(time_train.shape, time_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198980, 29) (198980, 1) (85443, 29) (85443, 1)\n"
     ]
    }
   ],
   "source": [
    "# need y to be 2D\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "108 0.0012640005617780275\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[y_train==1]), len(y_train[y_train==1]) / len(y_train))\n",
    "print(len(y_test[y_test==1]), len(y_test[y_test==1]) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, features, y_train, 'plots_data/creditcard/data_train_unscaled_normal')\n",
    "# plot_data(X_test, features, y_test, 'plots_data/creditcard/data_test_unscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dim in range(len(features)):\n",
    "#     x_up = np.percentile(X_train[:,dim], 99)\n",
    "#     x_low = np.percentile(X_train[:,dim], 1)\n",
    "#     plt.hist(X_train[:,dim], histtype='step', range=[x_low, x_up], label='train', linewidth=2, bins=30)\n",
    "#     plt.hist(X_test[:,dim], histtype='step', range=[x_low, x_up], label='test', linewidth=2, bins=30)\n",
    "#     # plt.hist(X_train[:,dim], histtype='step', label='train', linewidth=2, bins=30)\n",
    "#     # plt.hist(X_test[:,dim], histtype='step', label='test', linewidth=2, bins=30)\n",
    "#     plt.title(features[dim])\n",
    "#     plt.legend()\n",
    "#     # plt.savefig(f'plots_data/creditcard/feature_distr/{features[dim]}.png', dpi=300, facecolor='white')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip data to [1, 99] percentile of train data\n",
    "for dim in range(len(features)):\n",
    "    x_up = np.percentile(X_train[:,dim], 99)\n",
    "    x_low = np.percentile(X_train[:,dim], 1)\n",
    "    X_train[:,dim] = np.clip(X_train[:,dim], x_low, x_up)\n",
    "    X_test[:,dim] = np.clip(X_test[:,dim], x_low, x_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, features, y_train, 'plots_data/creditcard/data_train_clip_unscaled_normal')\n",
    "# plot_data(X_test, features, y_test, 'plots_data/creditcard/data_test_clip_unscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only scale 'amount' feature\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "X_train_scaled[:, -1:] = scaler.fit_transform(X_train[:, -1:])\n",
    "X_test_scaled[:, -1:] = scaler.transform(X_test[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.256543074811737e-17 1.0 -0.025962097257665868 0.9793857452227972\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_train_scaled[:, -1]), np.std(X_train_scaled[:, -1]), np.mean(X_test_scaled[:, -1]), np.std(X_test_scaled[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train_scaled, features, y_train, 'plots_data/creditcard/data_train_clip_scaled_normal')\n",
    "# plot_data(X_test_scaled, features, y_test, 'plots_data/creditcard/data_test_clip_scaled2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save with time encoding\n",
    "X_train_scaled = np.concatenate((time_train, X_train_scaled), axis=1)\n",
    "print(X_train_scaled.shape)\n",
    "X_test_scaled = np.concatenate((time_test, X_test_scaled), axis=1)\n",
    "print(X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('processed/creditcard_time/train.npy', X_train_scaled)\n",
    "# np.save('processed/creditcard_time/test.npy', X_test_scaled)\n",
    "# np.save('processed/creditcard_time/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('processed/creditcard/train.npy', X_train_scaled)\n",
    "# np.save('processed/creditcard/test.npy', X_test_scaled)\n",
    "# np.save('processed/creditcard/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('processed/creditcard_normal/train.npy', X_train_scaled)\n",
    "# np.save('processed/creditcard_normal/test.npy', X_test_scaled)\n",
    "# np.save('processed/creditcard_normal/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTransf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
