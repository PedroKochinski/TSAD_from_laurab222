{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "plt.style.use(hep.style.firamath)\n",
    "plt.rcParams['lines.markersize'] = 4\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, features, y=None, path=None):\n",
    "    # plt.rcParams[\"text.usetex\"] = False\n",
    "    # # plt.rcParams['figure.figsize'] = 6, 2\n",
    "    # # plt.rcParams['lines.markersize'] = 2\n",
    "    # plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "    size = int(len(features)*1.5)   # old *0.6)\n",
    "    dims = len(features)\n",
    "    if y is not None:\n",
    "        dims += 1\n",
    "\n",
    "    fig, axs = plt.subplots(dims, 1, figsize=(17, size), sharex=True)\n",
    "\n",
    "    for dim, feat in enumerate(features):  # iterate through the features we're using\n",
    "        # print(feat)\n",
    "        x_t = X[:, dim]\n",
    "        axs[dim].plot(x_t)\n",
    "        # if y is not None:\n",
    "        #     y_scaled = np.max(x_t)*y\n",
    "        #     axs[dim].plot(y_scaled, '--', linewidth=1, color='tab:orange', label='anomalies')\n",
    "        # turn ylabel by 90 degrees and shift it to the left\n",
    "        axs[dim].set_ylabel(feat, rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        # Align all y-axis labels by setting the same label coordinates\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        # axs[dim].legend(loc='upper right')\n",
    "        axs[dim].set_ylim(-1,1)\n",
    "    if y is not None: # plot the target variable in last dimension if we have truth labels\n",
    "        axs[-1].plot(y, '--', color='tab:red')\n",
    "        # axs[-1].set_yticks([0, 1])\n",
    "        axs[-1].set_ylabel('anomalies', rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "    axs[-1].set_xlabel('Events')  # looks alignment-wise better than supxlabel\n",
    "    # fig.supylabel('values')\n",
    "    # fig.supxlabel('Time stamp [min]')\n",
    "    plt.tight_layout()\n",
    "    if path is not None:\n",
    "        plt.savefig(f'{path}.png', dpi=100, facecolor='white')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclipse data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_data.hdf', 'r')\n",
    "f_test = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_data.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_train.keys())\n",
    "print(f_train.attrs.keys())\n",
    "\n",
    "print(f_train['prod_train_data'].keys())\n",
    "print(f_train['prod_train_data']['axis0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_train as file:\n",
    "    for key in file['prod_train_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_train_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_test as file:\n",
    "    for key in file['prod_test_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_test_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_label.csv')\n",
    "label_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_test.shape, label_train.shape)\n",
    "print(label_train)\n",
    "y_train = label_train['binary_anom']\n",
    "print(y_train[y_train==1])\n",
    "y_test = label_test['binary_anom']\n",
    "print(y_test[y_test==1], '\\n', len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_test\n",
    "label['job_id'] = label['job_id'].astype('str')\n",
    "label['component_id'] = label['component_id'].astype('str') \n",
    "label.set_index(['job_id', 'component_id'], inplace=True)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEE-CIS fraud detection data set (from kaggle challenge 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_transaction.csv')\n",
    "x_train2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_train2.shape)\n",
    "print(x_train.columns) #, x_train2.columns)\n",
    "\n",
    "for i, name in enumerate(x_train.columns):\n",
    "    print(name)\n",
    "    # print(x_train[name].dtypes)\n",
    "\n",
    "# Filter columns that are of type 'float'\n",
    "float_cols = x_train.select_dtypes(include='float').columns\n",
    "# print(\"Columns with float values:\", float_cols)\n",
    "# for i, name in enumerate(float_cols.columns):\n",
    "#     print(name)\n",
    "\n",
    "# Filter columns that are NOT of type 'float'\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(by='TransactionDT')\n",
    "x_train = x_train.drop(columns=non_num_cols)\n",
    "y_train = x_train['isFraud']\n",
    "x_train = x_train.drop(columns='isFraud')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(0)\n",
    "has_nan = y_train.isnull().any()\n",
    "print(has_nan)\n",
    "print(x_train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "print(max(X_train[0]))\n",
    "X_train, min_a, max_a = normalize3(X_train, min_a=None)\n",
    "print(max(X_train[0]))\n",
    "X_test, _, _ = normalize3(X_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for dim in range(5):\n",
    "    x_t, l = X_train[:10000, dim], y_train[:10000]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(x_t, label='data')\n",
    "    plt.plot(l, '--', linewidth=0.2)\n",
    "    plt.fill_between(np.arange(l.shape[0]), l, color='tab:orange', alpha=0.3, label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/train_1.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/test_1.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/labels_1.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['day'] = x_train['TransactionDT'] / (24 * 60 * 60)  # to convert seconds to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['uid1'] =  (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str)\n",
    "# x_train['uid2'] =  (x_train.card1.astype(str) +'_' + \\\n",
    "#             x_train.addr1.astype(str) +'_' + \\\n",
    "#             (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train['uid1'])\n",
    "# print(x_train['uid1'].unique)\n",
    "# print(x_train['uid2'])\n",
    "# print(x_train['uid2'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_transaction.csv')\n",
    "x_test2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_identity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.columns)\n",
    "print(x_test.shape)\n",
    "\n",
    "for i, name in enumerate(x_test.columns):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.sort_values(by='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEEE-CIS challenge from kaggle loaded from fraud-dataset-benchmark (contains UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data/ieeecis/train.csv')\n",
    "x_test = pd.read_csv('data/ieeecis/test.csv')\n",
    "labels = pd.read_csv('data/ieeecis/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "# print(x_train.columns, x_test.columns)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtypes)\n",
    "\n",
    "# Filter columns that are NOT of type 'float' and for now pop them out\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)\n",
    "    # if name not in ['ENTITY_ID', 'EVENT_TIMESTAMP']:\n",
    "    #     x_train.pop(name)\n",
    "    #     x_test.pop(name)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(['EVENT_TIMESTAMP'])\n",
    "x_test = x_test.sort_values(['EVENT_TIMESTAMP'])\n",
    "date_train = x_train.pop('EVENT_TIMESTAMP')\n",
    "print(date_train.shape, date_train)\n",
    "date_test = x_test.pop('EVENT_TIMESTAMP')\n",
    "print(date_test.shape, date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(x_train.columns)\n",
    "feature_names.remove('ENTITY_ID') # because we don't want to use them as a feature\n",
    "feature_names.remove('TransactionID') # because we don't want to use them as a feature\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for train data\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 50\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "print(len(train_uid), train_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert each group to a 2D numpy array and stack them into a list of 2D data frames, first approach\n",
    "# x_train3 = [group.drop(columns=['ENTITY_ID', 'TransactionID']).values for name, group in grouped if len(group) >= 50]\n",
    "\n",
    "# print(len(x_train3), len(train_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach, here first plotting\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):    # enumerate(short_list): \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "    print(feat)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) >= 50:\n",
    "            # print(name, group)\n",
    "            df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                \n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        # for plotting\n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if unique_values.shape[0] > 25:  # take 25 most frequent values for plots\n",
    "            idx = np.argsort(counts)[::-1][:25]\n",
    "            unique_values = unique_values[idx]\n",
    "            counts = counts[idx]\n",
    "            lab = f'showing 25 most frequent \\nof {len(np.unique(tmp))} unique values'\n",
    "        else:\n",
    "            lab = f'{len(np.unique(tmp))} unique values'\n",
    "        \n",
    "        plt.bar(unique_values, counts, label=lab)\n",
    "        if feat in ['p_emaildomain', 'r_emaildomain', 'deviceinfo']:\n",
    "            plt.xticks(rotation=50, ha='right')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print(np.max(tmp), np.min(tmp))\n",
    "\n",
    "        plt.hist(tmp, bins=50, range=[np.quantile(tmp,0.03), np.quantile(tmp,0.97)], label=f'mean: {np.mean(tmp):.1f}, std: {np.std(tmp):.1f} \\nmin: {np.min(tmp):.1f}, max: {np.max(tmp):.1f}')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D arrays, second approach\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "other_values = {}\n",
    "encoding = {}\n",
    "# means = {}  # next step\n",
    "# std = {}\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):  \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "\n",
    "    # for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "    #     if len(group) >= 50:\n",
    "    #         # print(name, group)\n",
    "    #         df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "    #         if feat in non_num_cols:\n",
    "    #             df[feat] = df[feat].fillna('missing')\n",
    "    #         else:\n",
    "    #             df[feat] = df[feat].fillna(0)\n",
    "    #         tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    # print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        print(feat)\n",
    "        for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "            if len(group) >= 50:\n",
    "                # print(name, group)\n",
    "                df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                tmp = np.concatenate((tmp, df[feat].values))\n",
    "\n",
    "        # for encoding \n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if len(unique_values) > 50:  # take 100 most frequent values for encoding\n",
    "            idx = np.argsort(counts)\n",
    "            unique_values = unique_values[idx]  # unique values sorted in frequency\n",
    "            other_values[feat] = unique_values[49:]   # values to be encoded as 'other'\n",
    "            print(other_values)\n",
    "            # replace all elements of tmp that are in other_values with 'other'\n",
    "            idx = np.where(np.isin(tmp, other_values[feat]))\n",
    "            tmp[idx] = 'other'\n",
    "            # print(len(np.unique(tmp)))\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore').fit(tmp.reshape(-1, 1))\n",
    "        print(enc.categories_, len(enc.categories_[0]))\n",
    "        encoding[feat] = enc\n",
    "\n",
    "print(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach\n",
    "x_train3 = []\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "for name, group in grouped:  # second get all transactions in train data keeping structure of uid\n",
    "    if len(group) >= 50:\n",
    "        # print(name, group)\n",
    "        df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_train3.append(df.values)\n",
    "\n",
    "print(len(x_train3), len(train_uid))\n",
    "updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train3), x_train3[0].shape, x_train3[0])\n",
    "print(len(train_uid))\n",
    "print(len(updated_feature_names), updated_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach: for test data now\n",
    "x_test3 = []\n",
    "\n",
    "grouped_test = x_test.groupby('ENTITY_ID')\n",
    "test_uid = grouped_test.size()[grouped_test.size() >= 20].index  # allow shorter time series for testing!\n",
    "test_uid = list(test_uid)\n",
    "for name, group in grouped_test:  # second get all transactions in test data keeping structure of uid\n",
    "    if len(group) >= 20:\n",
    "        df = group.drop(columns=['ENTITY_ID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_test3.append(df.values)\n",
    "\n",
    "print(len(x_test3))\n",
    "# updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_train3):\n",
    "    print(f'time series for user: {train_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/train_{train_uid[i]}_{j}')\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # arr_scaled = scaler.fit_transform(arr)\n",
    "    # plot_data(arr_scaled, feature_names, path=f'plots_data/IEEECIS_new/train_scaled_{train_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/train_{train_uid[i]}.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_{j}')\n",
    "    \n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = arr[:, 1:]                    # remove the transaction IDs from test data\n",
    "    print(arr.shape)\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # if test_uid[i] == '13623.0_498.0_117.0':\n",
    "    #     for j in range(0, len(updated_feature_names), 20):\n",
    "    #         plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_{j}')\n",
    "    # else:\n",
    "    #     plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_0')\n",
    "    plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/test_{test_uid[i]}.npy', arr)\n",
    "    np.save(f'processed/IEEECIS_new2/labels_{test_uid[i]}.npy', y_test)\n",
    "    # np.save(f'processed/IEEECIS_new2/ids_{test_uid[i]}.npy', test_transaction_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for test data\n",
    "grouped = x_test.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 20\n",
    "test_uid = grouped.size()[grouped.size() >= 20].index\n",
    "test_uid = list(test_uid)\n",
    "print(test_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n",
    "\n",
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames\n",
    "x_test3 = [group.drop(columns='ENTITY_ID').values for name, group in grouped if len(group) >= 20]\n",
    "\n",
    "print(len(x_test3), len(test_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the truth labels of the test data\n",
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = np.nan_to_num(arr)            # test data\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # scaler already defined for train data? or better to redefine?\n",
    "    arr_scaled = scaler.fit_transform(arr)\n",
    "\n",
    "    print(arr_scaled.shape, y_test.shape)\n",
    "\n",
    "    if len(y_test[y_test==1]) > 0:\n",
    "        print(test_uid[i])\n",
    "        plot_data(arr, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}')\n",
    "        plot_data(arr_scaled, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_scaled{test_uid[i]}')\n",
    "    \n",
    "        np.save(f'processed/IEEECIS_new/test_scaled_{test_uid[i]}.npy', arr_scaled)\n",
    "        np.save(f'processed/IEEECIS_new/labels_{test_uid[i]}.npy', y_test)\n",
    "        np.save(f'processed/IEEECIS_new/ids_{test_uid[i]}.npy', test_transaction_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATLAS time series data for AD in LAr data (from Vilius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data', 'event_labels', 'event_numbers', 'features', 'lb', 'run', 'timestamps']> <KeysViewHDF5 []>\n"
     ]
    }
   ],
   "source": [
    "# f = h5py.File('data/ATLAS_TS/user.vcepaiti.38697672.EXT0._000002.AnnotatorNtuple.h5', 'r')\n",
    "f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.Good.04May2024.00361862.physics_Main_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.SevNois.14May2024.00462542.physics_UPC.r15414_p6133_EXT0_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.Good.16May2024.00361862.physics_CosmicCalo.r14138_p5427_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.HVNONNOMINAL.03Jun2024.00430897.physics_Main.r15052_p5604_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.PumpNoise.04Jun2024.00451140.physics_Main.r14858_p5785_EXT0.h5', 'r')\n",
    "# f = h5py.File('data/ATLAS_DQM_TS/user.vcepaiti.Annotator.SevNois.14May2024.00462542.physics_HardProbes.f1399_m2209_EXT0.h5', 'r')\n",
    "print(f.keys(), f.attrs.keys())\n",
    "data = np.array(f['data'])\n",
    "lumiblock = np.array(f['lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 16) (16,) (79882,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data[0].shape, data[0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"event_labels\": shape (734,), type \"|O\"> <HDF5 dataset \"event_numbers\": shape (734,), type \"|O\"> <HDF5 dataset \"features\": shape (16,), type \"|O\"> <HDF5 dataset \"run\": shape (734,), type \"<i4\">\n",
      "(734,) (79882,)\n",
      "0 1\n",
      "[0 1]\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/8xvqd9y134510qh2hy9fj_7c0000gn/T/ipykernel_50644/1264260933.py:6: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(np.any(np.logical_and(evt_lb != 0, evt_lb !=2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734,) (79882,)\n",
      "[b'EMBA_AveLARQ_mean' b'EMBA_AveLARQ_std' b'EMBA_ClusTime_mean'\n",
      " b'EMBA_ClusTime_std' b'EMBC_AveLARQ_mean' b'EMBC_AveLARQ_std'\n",
      " b'EMBC_ClusTime_mean' b'EMBC_ClusTime_std' b'EMECA_AveLARQ_mean'\n",
      " b'EMECA_AveLARQ_std' b'EMECA_ClusTime_mean' b'EMECA_ClusTime_std'\n",
      " b'EMECC_AveLARQ_mean' b'EMECC_AveLARQ_std' b'EMECC_ClusTime_mean'\n",
      " b'EMECC_ClusTime_std']\n",
      "(734,)\n",
      "(734,)\n",
      "[ 105  106  107  108  109  110  111  112  113  114  115  116  117  118\n",
      "  119  120  121  159  160  161  162  163  164  165  166  167  168  169\n",
      "  170  171  172  173  174  175  176  177  178  179  180  181  182  183\n",
      "  184  185  186  187  188  189  190  191  192  193  194  195  196  197\n",
      "  198  199  200  201  203  208  209  210  211  212  213  214  215  216\n",
      "  218  219  221  222  223  224  225  226  227  228  230  231  232  233\n",
      "  235  236  237  238  239  240  241  242  243  244  245  246  247  248\n",
      "  249  250  251  252  253  254  255  256  257  258  259  260  261  262\n",
      "  263  268  270  271  273  274  275  277  278  279  280  281  282  283\n",
      "  284  289  290  296  298  299  300  302  303  306  307  308  309  310\n",
      "  311  313  314  315  316  317  318  319  321  322  323  324  325  326\n",
      "  327  328  329  330  331  332  333  334  336  337  338  339  340  341\n",
      "  342  343  344  345  346  347  348  349  350  351  352  353  354  355\n",
      "  356  357  358  359  360  361  362  363  364  365  366  367  368  369\n",
      "  370  371  372  373  374  375  376  377  378  379  380  381  382  383\n",
      "  384  385  386  389  390  391  392  393  394  395  396  397  399  400\n",
      "  401  402  403  404  405  406  407  408  409  410  411  412  413  414\n",
      "  415  416  417  418  419  420  421  422  423  424  425  426  427  428\n",
      "  429  430  431  432  433  434  435  436  438  439  440  441  444  445\n",
      "  448  452  453  454  455  456  457  458  459  460  461  462  463  464\n",
      "  465  466  473  474  475  476  477  478  479  480  481  482  483  484\n",
      "  485  486  487  488  489  490  491  492  493  494  495  496  497  498\n",
      "  499  501  505  506  507  511  512  513  514  515  516  517  518  519\n",
      "  520  521  529  530  531  532  533  534  535  536  537  538  542  543\n",
      "  544  548  549  550  551  552  553  554  555  556  557  558  559  560\n",
      "  561  562  563  564  565  566  567  568  569  570  571  572  573  574\n",
      "  575  576  577  578  579  580  581  582  583  584  585  586  587  588\n",
      "  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n",
      "  603  604  605  606  607  608  609  610  611  612  613  614  615  616\n",
      "  617  618  619  620  621  622  623  624  625  630  631  632  633  634\n",
      "  635  636  637  638  639  640  641  642  643  644  645  647  648  649\n",
      "  650  651  652  653  654  655  656  657  658  659  660  661  662  663\n",
      "  664  665  666  667  668  669  672  673  674  675  676  679  680  681\n",
      "  682  683  684  685  686  687  688  689  690  691  692  693  694  695\n",
      "  696  697  698  699  700  701  702  703  704  705  706  707  708  709\n",
      "  710  711  713  714  715  716  717  718  719  720  721  722  727  728\n",
      "  729  730  731  732  733  734  735  736  737  738  739  740  741  745\n",
      "  747  748  749  750  751  752  753  754  755  756  757  758  759  760\n",
      "  761  762  763  764  765  766  767  768  771  784  785  786  792  793\n",
      "  794  795  796  801  802  803  804  805  806  807  808  809  810  811\n",
      "  812  813  814  816  817  818  819  820  821  822  823  824  825  826\n",
      "  827  828  829  831  832  833  834  835  837  842  843  844  845  846\n",
      "  847  848  849  850  851  852  853  854  855  856  857  858  859  860\n",
      "  861  862  863  864  865  866  867  868  869  870  871  872  873  874\n",
      "  875  876  877  878  879  880  881  882  883  884  885  886  887  888\n",
      "  889  890  891  892  894  895  896  897  898  899  900  901  902  903\n",
      "  904  905  906  907  908  909  910  911  912  913  914  915  916  917\n",
      "  918  919  920  921  922  923  924  933  934  935  936  937  938  939\n",
      "  940  941  942  943  944  945  946  947  948  949  950  951  952  953\n",
      "  954  955  956  957  959  960  961  962  963  964  965  966  967  968\n",
      "  969  970  971  972  973  974  975  976  977  979  981  982  983  984\n",
      "  985  986  988  990  991  992  993  994  998  999 1000 1001 1003 1041\n",
      " 1042 1043 1044 1045 1046 1047]\n"
     ]
    }
   ],
   "source": [
    "print(f['event_labels'], f['event_numbers'], f['features'], f['run'])\n",
    "evt_lb = np.array(f['event_labels'])\n",
    "print(evt_lb.shape, evt_lb[0].shape)\n",
    "print(np.min(evt_lb[2]), np.max(evt_lb[2]))\n",
    "print(np.unique(evt_lb[2]))\n",
    "print(np.any(np.logical_and(evt_lb != 0, evt_lb !=2)))\n",
    "evt_nb = np.array(f['event_numbers'])\n",
    "print(evt_nb.shape, evt_nb[0].shape)\n",
    "features = np.array(f['features'])\n",
    "print(features)\n",
    "run = np.array(f['run'])\n",
    "print(run.shape)\n",
    "lb = np.array(f['lb'])\n",
    "print(lb.shape)\n",
    "print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 802 for CosmicCalo, 504 for HVNONNOMINAL, 787 for PumpNoise, 312 for HardProbes\n",
    "# lb_idx = np.where(lb == 787)[0][0]  \n",
    "# print(lb_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[361862]\n",
      "39524932\n"
     ]
    }
   ],
   "source": [
    "# check if run number the same over all runs\n",
    "print(np.unique(run))\n",
    "# check if all runs are actually the same\n",
    "events = 0\n",
    "for i in range(data.shape[0]):\n",
    "    # print(data[i,0].shape)\n",
    "    # print(np.all(data[0,0] ==data[i,0]))\n",
    "    events += data[i,0].shape[0]\n",
    "    if data[i,0].shape[0] > 1800 and data[i,0].shape[0] < 2000:\n",
    "        print(i)\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(16, 1, figsize=(10, 25), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        l = evt_lb[lb]\n",
    "        l1 = (l==1) + 0\n",
    "        l2 = (l==2) + 0\n",
    "        l3 = (l==3) + 0\n",
    "        # print(x_t.shape, x_t[0].shape)\n",
    "        axs[dim].plot(x_t, label='data')\n",
    "        axs[dim].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "        axs[dim].plot(l2, '--', linewidth=1, color='tab:red', label='noise burst')\n",
    "        axs[dim].plot(l3, '--', linewidth=1, color='tab:purple', label='data corruption')\n",
    "        axs[dim].set_ylabel(str(features[dim])[2:-1])\n",
    "    axs[0].legend()\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(17, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "        axs[dim].legend(loc='upper right')\n",
    "        # axs[dim].set_title(features[dim][2:])\n",
    "    \n",
    "    l = evt_lb[lb]      # same label for all dimensions/features\n",
    "    l1 = (l==1) + 0\n",
    "    l2 = (l==2) + 0\n",
    "    l3 = (l==3) + 0\n",
    "    axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "    axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "    axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "    axs[-1].legend(loc='upper right')\n",
    "    # fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMBA_AveLARQ_mean' 'EMBA_AveLARQ_std' 'EMBA_ClusTime_mean'\n",
      " 'EMBA_ClusTime_std' 'EMBC_AveLARQ_mean' 'EMBC_AveLARQ_std'\n",
      " 'EMBC_ClusTime_mean' 'EMBC_ClusTime_std' 'EMECA_AveLARQ_mean'\n",
      " 'EMECA_AveLARQ_std' 'EMECA_ClusTime_mean' 'EMECA_ClusTime_std'\n",
      " 'EMECC_AveLARQ_mean' 'EMECC_AveLARQ_std' 'EMECC_ClusTime_mean'\n",
      " 'EMECC_ClusTime_std']\n"
     ]
    }
   ],
   "source": [
    "for f in range(len(features)):\n",
    "    features[f] = str(features[f])[2:-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lb in range(data.shape[0]):  # iterate through lumiblocks\n",
    "x_t = np.stack(data[lb_idx])\n",
    "x_t = x_t.T\n",
    "y_t = evt_lb[lb_idx]\n",
    "print(np.max(y_t), np.min(y_t))\n",
    "print(x_t.shape, y_t.shape)\n",
    "plot_data(x_t, features, y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(data.shape[0]):\n",
    "    x_t = np.stack(data[l])\n",
    "    x_t = x_t.T\n",
    "    y_t = evt_lb[l]\n",
    "    lb_idx = lb[l]\n",
    "    # print(lb_idx, x_t.shape, y_t.shape)\n",
    "    np.save(f'processed/ATLAS_DQM_TS/train_Main_{lb_idx}.npy', x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly save as train data\n",
    "# np.save('processed/ATLAS_DQM_TS/test_cosmicCalo.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_cosmicCalo.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_hvononNominal.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_hvononNominal.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_pumpNoise.npy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_pumpNoise.npy', y_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/test_hardProbesnpy', x_t)\n",
    "# np.save('processed/ATLAS_DQM_TS/labels_hardProbes.npy', y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(5):\n",
    "    labels[f'lumiblock_{i}'] = (evt_lb[i] > 0) + 0\n",
    "    l = labels[f'lumiblock_{i}']\n",
    "    print(len(l[l>0]), len(l[l==0]), len(l[l>0])/len(l[l==0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].shape)\n",
    "temp = np.stack(data[0])\n",
    "print(np.array(data).shape, temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for lb in range(5):\n",
    "    \n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x[:,0].shape)\n",
    "    print(np.max(x[:,0]), np.min(x[:,0]), np.mean(x[:,0]), np.std(x[:,0]))\n",
    "    y = labels[f'lumiblock_{lb}']\n",
    "    print(x.shape, y.shape)\n",
    "    scaler = StandardScaler()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    print(np.max(x_norm[:,0]), np.min(x_norm[:,0]), np.mean(x_norm[:,0]), np.std(x_norm[:,0]))  \n",
    "\n",
    "    plt.plot(x_norm[:,0], label='normalised')\n",
    "    plt.plot(x[:,0], label='raw')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "\n",
    "x_train = np.stack(data[0]).T\n",
    "for lb in range(1,4):\n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x.shape)\n",
    "    x_train = np.concatenate((x_train, x), axis=0)\n",
    "    print(x_train.shape)\n",
    "x_test = np.stack(data[4]).T\n",
    "y_test = np.array(labels['lumiblock_4'][:, np.newaxis])\n",
    "y_test = np.repeat(y_test, repeats=16, axis=1)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardise the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# # print(max(x[0]))\n",
    "# # x, min_a, max_a = normalize3(x_train, min_a=None)\n",
    "# # print(max(x_train[0]))\n",
    "# # x_test, _, _ = normalize3(x_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "fig, axs = plt.subplots(16, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "for dim in range(16):  # iterate through 16 features we're using\n",
    "    x_t = x_train[:, dim]   \n",
    "    axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "    axs[dim].legend(loc='upper right')\n",
    "    # axs[dim].set_title(features[dim][2:])\n",
    "\n",
    "# l = y_train      # same label for all dimensions/features\n",
    "# l1 = (l==1) + 0\n",
    "# l2 = (l==2) + 0\n",
    "# l3 = (l==3) + 0\n",
    "# axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "# axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "# axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "axs[-1].legend(loc='upper right')\n",
    "fig.supylabel('label')\n",
    "fig.supxlabel('events')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'processed/ATLAS_TS/train.npy', x_train)\n",
    "np.save(f'processed/ATLAS_TS/test.npy', x_test)\n",
    "np.save(f'processed/ATLAS_TS/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in range(5):\n",
    "    x = np.stack(data[lb]).T\n",
    "    y = np.array(labels[f'lumiblock_{lb}'][:, np.newaxis])\n",
    "    y = np.repeat(y, repeats=16, axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_train.npy', x)\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    \n",
    "    # if lb < 4:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_train.npy', x)\n",
    "    #     # np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    # else:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_test.npy', x)\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set on water quality (GECCO IoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/3884398/1_gecco2018_water_quality.csv')\n",
    "data = data.sort_values(by='Time')\n",
    "print(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Time', 'Unnamed: 0', 'EVENT'], axis=1)\n",
    "y = data.EVENT\n",
    "time = data.Time\n",
    "features = X.columns\n",
    "print(X.shape, y.shape)\n",
    "print(features)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.to_datetime(time, unit='s')\n",
    "print(time)\n",
    "\n",
    "# Assuming `time.values` is a list or array of datetime objects\n",
    "df_stamp = pd.DataFrame({'date': list(time.values)})\n",
    "\n",
    "# Extract datetime components and process\n",
    "df_stamp['month'] = df_stamp['date'].apply(lambda row: row.month)\n",
    "df_stamp['day'] = df_stamp['date'].apply(lambda row: row.day)\n",
    "# df_stamp['weekday'] = df_stamp['date'].apply(lambda row: row.weekday())\n",
    "df_stamp['hour'] = df_stamp['date'].apply(lambda row: row.hour)\n",
    "# df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute)\n",
    "df_stamp['minute'] = df_stamp['date'].apply(lambda row: row.minute // 15)\n",
    "\n",
    "# Drop the 'date' column and convert the remaining data to values\n",
    "data_stamp = df_stamp.drop(columns=['date']).values\n",
    "\n",
    "# Print statements\n",
    "print(df_stamp)\n",
    "print(data_stamp)\n",
    "print(data_stamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y+0)\n",
    "print(len(y[y==1]), len(y[y==0]), len(y[y==1])/len(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X, features, y, 'plots_data/GECCO/data_all_withnan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially just 0 padded nan values\n",
    "pbm = np.where(np.isnan(X))[0]\n",
    "print(pbm)  \n",
    "print(len(pbm))\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "print(np.isnan(X).any())\n",
    "\n",
    "# # replace nan values with previous value (forward fill)\n",
    "# for i in range(X.shape[1]):\n",
    "#     for j in range(0, X.shape[0]):\n",
    "#         if np.isnan(X[j,i]):\n",
    "#             X[j,i] = X[j-1,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X, features, y, 'plots_data/GECCO/data_all5_withoutnan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(X)*0.6)\n",
    "print(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:cut]\n",
    "X_test = X[cut:]\n",
    "y_train = y[:cut]\n",
    "y_test = y[cut:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train = data_stamp[:cut]\n",
    "time_test = data_stamp[cut:]\n",
    "print(time_train.shape, time_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need y to be 2D\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y[y==1])/len(y))\n",
    "print(len(y_train[y_train==1])/len(y_train), len(y_test[y_test==1])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(9):\n",
    "    if dim > 0: \n",
    "        x_up = np.percentile(X_train[:,dim], 98)\n",
    "        x_low = np.percentile(X_train[:,dim], 2)\n",
    "        plt.hist(X_train[:,dim], histtype='step', range=[x_low, x_up], label='train', linewidth=2, bins=30)\n",
    "        plt.hist(X_test[:,dim], histtype='step', range=[x_low, x_up], label='test', linewidth=2, bins=30)\n",
    "    else:\n",
    "        plt.hist(X_train[:,dim], histtype='step', label='train', linewidth=2, bins=30)\n",
    "        plt.hist(X_test[:,dim], histtype='step', label='test', linewidth=2, bins=30)\n",
    "    # plt.hist(X_train[:,dim], histtype='step', label='train', linewidth=2, bins=30)\n",
    "    # plt.hist(X_test[:,dim], histtype='step', label='test', linewidth=2, bins=30)\n",
    "    plt.title(features[dim])\n",
    "    plt.legend()\n",
    "    # plt.savefig(f'plots_data/GECCO/feature_distr_forwardfill/{features[dim]}_7.png', dpi=300, facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip data to [2, 98] percentile of train data (except for first feature)\n",
    "for dim in range(9):\n",
    "    if dim > 0:\n",
    "        x_up = np.percentile(X_train[:,dim], 98)\n",
    "        x_low = np.percentile(X_train[:,dim], 2)\n",
    "        X_train[:,dim] = np.clip(X_train[:,dim], x_low, x_up)\n",
    "        X_test[:,dim] = np.clip(X_test[:,dim], x_low, x_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(X_train, features, y_train, 'plots_data/GECCO/data_train1')\n",
    "# plot_data(X_test, features, y_test, 'plots_data/GECCO/data_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(len(y_test[y_test==1]), len(y_test[y_test==0]), len(y_test[y_test==1])/len(y_test))   \n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled = np.clip(X_train_scaled, -5, 5)\n",
    "# X_test_scaled = np.clip(X_test_scaled, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train_scaled, features, y_train, 'plots_data/GECCO/data_train_norm1')\n",
    "plot_data(X_test_scaled, features, y_test, 'plots_data/GECCO/data_test_norm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = np.concatenate((time_train, X_train_scaled), axis=1)\n",
    "print(X_train_scaled.shape)\n",
    "X_test_scaled = np.concatenate((time_test, X_test_scaled), axis=1)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('processed/GECCO/train.npy', X_train_scaled)\n",
    "np.save('processed/GECCO/test.npy', X_test_scaled)\n",
    "np.save('processed/GECCO/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
