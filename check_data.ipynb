{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "plt.style.use(hep.style.firamath)\n",
    "plt.rcParams['lines.markersize'] = 4\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, features, y=None, path=None):\n",
    "    # plt.rcParams[\"text.usetex\"] = False\n",
    "    # # plt.rcParams['figure.figsize'] = 6, 2\n",
    "    # # plt.rcParams['lines.markersize'] = 2\n",
    "    # plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "    size = int(len(features)*1.5)   # old *0.6)\n",
    "    dims = len(features)\n",
    "    if y is not None:\n",
    "        dims += 1\n",
    "\n",
    "    fig, axs = plt.subplots(dims, 1, figsize=(17, size), sharex=True)\n",
    "\n",
    "    for dim, feat in enumerate(features):  # iterate through the features we're using\n",
    "        # print(feat)\n",
    "        x_t = X[:, dim]\n",
    "        axs[dim].plot(x_t)\n",
    "        # if y is not None:\n",
    "        #     y_scaled = np.max(x_t)*y\n",
    "        #     axs[dim].plot(y_scaled, '--', linewidth=1, color='tab:orange', label='anomalies')\n",
    "        # turn ylabel by 90 degrees and shift it to the left\n",
    "        axs[dim].set_ylabel(feat, rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        # Align all y-axis labels by setting the same label coordinates\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        # axs[dim].legend(loc='upper right')\n",
    "    if y is not None: # plot the target variable in last dimension if we have truth labels\n",
    "        axs[-1].plot(y, '--', color='tab:red')\n",
    "        axs[-1].set_yticks([0, 1])\n",
    "        axs[-1].set_ylabel('anomalies', rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "    axs[-1].set_xlabel('Transactions')  # looks alignment-wise better than supxlabel\n",
    "    # fig.supylabel('values')\n",
    "    # fig.supxlabel('Time stamp [min]')\n",
    "    plt.tight_layout()\n",
    "    if path is not None:\n",
    "        plt.savefig(f'{path}.png', dpi=100, facecolor='white')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclipse data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_data.hdf', 'r')\n",
    "f_test = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_data.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_train.keys())\n",
    "print(f_train.attrs.keys())\n",
    "\n",
    "print(f_train['prod_train_data'].keys())\n",
    "print(f_train['prod_train_data']['axis0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_train as file:\n",
    "    for key in file['prod_train_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_train_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_test as file:\n",
    "    for key in file['prod_test_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_test_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_label.csv')\n",
    "label_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_test.shape, label_train.shape)\n",
    "print(label_train)\n",
    "y_train = label_train['binary_anom']\n",
    "print(y_train[y_train==1])\n",
    "y_test = label_test['binary_anom']\n",
    "print(y_test[y_test==1], '\\n', len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_test\n",
    "label['job_id'] = label['job_id'].astype('str')\n",
    "label['component_id'] = label['component_id'].astype('str') \n",
    "label.set_index(['job_id', 'component_id'], inplace=True)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEE-CIS fraud detection data set (from kaggle challenge 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_transaction.csv')\n",
    "x_train2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_train2.shape)\n",
    "print(x_train.columns) #, x_train2.columns)\n",
    "\n",
    "for i, name in enumerate(x_train.columns):\n",
    "    print(name)\n",
    "    # print(x_train[name].dtypes)\n",
    "\n",
    "# Filter columns that are of type 'float'\n",
    "float_cols = x_train.select_dtypes(include='float').columns\n",
    "# print(\"Columns with float values:\", float_cols)\n",
    "# for i, name in enumerate(float_cols.columns):\n",
    "#     print(name)\n",
    "\n",
    "# Filter columns that are NOT of type 'float'\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(by='TransactionDT')\n",
    "x_train = x_train.drop(columns=non_num_cols)\n",
    "y_train = x_train['isFraud']\n",
    "x_train = x_train.drop(columns='isFraud')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(0)\n",
    "has_nan = y_train.isnull().any()\n",
    "print(has_nan)\n",
    "print(x_train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "print(max(X_train[0]))\n",
    "X_train, min_a, max_a = normalize3(X_train, min_a=None)\n",
    "print(max(X_train[0]))\n",
    "X_test, _, _ = normalize3(X_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for dim in range(5):\n",
    "    x_t, l = X_train[:10000, dim], y_train[:10000]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(x_t, label='data')\n",
    "    plt.plot(l, '--', linewidth=0.2)\n",
    "    plt.fill_between(np.arange(l.shape[0]), l, color='tab:orange', alpha=0.3, label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/train_1.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/test_1.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/labels_1.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['day'] = x_train['TransactionDT'] / (24 * 60 * 60)  # to convert seconds to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['uid1'] =  (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str)\n",
    "# x_train['uid2'] =  (x_train.card1.astype(str) +'_' + \\\n",
    "#             x_train.addr1.astype(str) +'_' + \\\n",
    "#             (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train['uid1'])\n",
    "# print(x_train['uid1'].unique)\n",
    "# print(x_train['uid2'])\n",
    "# print(x_train['uid2'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_transaction.csv')\n",
    "x_test2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_identity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.columns)\n",
    "print(x_test.shape)\n",
    "\n",
    "for i, name in enumerate(x_test.columns):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.sort_values(by='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEEE-CIS challenge from kaggle loaded from fraud-dataset-benchmark (contains UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data/ieeecis/train.csv')\n",
    "x_test = pd.read_csv('data/ieeecis/test.csv')\n",
    "labels = pd.read_csv('data/ieeecis/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561013, 70) (29527, 70)\n",
      "(29527, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "# print(x_train.columns, x_test.columns)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561013, 70)\n",
      "TransactionID      float64\n",
      "transactionamt     float64\n",
      "productcd           object\n",
      "card1              float64\n",
      "card2              float64\n",
      "                    ...   \n",
      "id_20              float64\n",
      "devicetype          object\n",
      "deviceinfo          object\n",
      "ENTITY_ID           object\n",
      "EVENT_TIMESTAMP     object\n",
      "Length: 70, dtype: object\n",
      "Columns without numeric values: Index(['productcd', 'card6', 'p_emaildomain', 'r_emaildomain', 'devicetype',\n",
      "       'deviceinfo', 'ENTITY_ID', 'EVENT_TIMESTAMP'],\n",
      "      dtype='object') 8\n",
      "productcd object\n",
      "card6 object\n",
      "p_emaildomain object\n",
      "r_emaildomain object\n",
      "devicetype object\n",
      "deviceinfo object\n",
      "ENTITY_ID object\n",
      "EVENT_TIMESTAMP object\n",
      "(561013, 70)\n",
      "(29527, 70)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtypes)\n",
    "\n",
    "# Filter columns that are NOT of type 'float' and for now pop them out\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)\n",
    "    # if name not in ['ENTITY_ID', 'EVENT_TIMESTAMP']:\n",
    "    #     x_train.pop(name)\n",
    "    #     x_test.pop(name)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561013,) 0         2021-01-02T00:00:00Z\n",
      "1         2021-01-02T00:00:01Z\n",
      "2         2021-01-02T00:01:09Z\n",
      "3         2021-01-02T00:01:39Z\n",
      "4         2021-01-02T00:01:46Z\n",
      "                  ...         \n",
      "561008    2021-06-21T23:10:39Z\n",
      "561009    2021-06-21T23:10:41Z\n",
      "561010    2021-06-21T23:10:58Z\n",
      "561011    2021-06-21T23:11:04Z\n",
      "561012    2021-06-21T23:11:12Z\n",
      "Name: EVENT_TIMESTAMP, Length: 561013, dtype: object\n",
      "(29527,) 0        2021-06-21T23:11:15Z\n",
      "1        2021-06-21T23:11:29Z\n",
      "2        2021-06-21T23:11:45Z\n",
      "3        2021-06-21T23:12:00Z\n",
      "4        2021-06-21T23:12:11Z\n",
      "                 ...         \n",
      "29522    2021-07-02T23:57:27Z\n",
      "29523    2021-07-02T23:57:29Z\n",
      "29524    2021-07-02T23:57:59Z\n",
      "29525    2021-07-02T23:58:08Z\n",
      "29526    2021-07-02T23:58:51Z\n",
      "Name: EVENT_TIMESTAMP, Length: 29527, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.sort_values(['EVENT_TIMESTAMP'])\n",
    "x_test = x_test.sort_values(['EVENT_TIMESTAMP'])\n",
    "date_train = x_train.pop('EVENT_TIMESTAMP')\n",
    "print(date_train.shape, date_train)\n",
    "date_test = x_test.pop('EVENT_TIMESTAMP')\n",
    "print(date_test.shape, date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transactionamt', 'productcd', 'card1', 'card2', 'card3', 'card5', 'card6', 'addr1', 'dist1', 'p_emaildomain', 'r_emaildomain', 'c1', 'c2', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12', 'c13', 'c14', 'v62', 'v70', 'v76', 'v78', 'v82', 'v91', 'v127', 'v130', 'v139', 'v160', 'v165', 'v187', 'v203', 'v207', 'v209', 'v210', 'v221', 'v234', 'v257', 'v258', 'v261', 'v264', 'v266', 'v267', 'v271', 'v274', 'v277', 'v283', 'v285', 'v289', 'v291', 'v294', 'id_01', 'id_02', 'id_05', 'id_06', 'id_09', 'id_13', 'id_17', 'id_19', 'id_20', 'devicetype', 'deviceinfo']\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(x_train.columns)\n",
    "feature_names.remove('ENTITY_ID') # because we don't want to use them as a feature\n",
    "feature_names.remove('TransactionID') # because we don't want to use them as a feature\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY_ID\n",
      "1000.0_nan_66.0          1\n",
      "10000.0_184.0_-37.0      1\n",
      "10003.0_nan_90.0         5\n",
      "10004.0_123.0_-215.0     1\n",
      "10004.0_177.0_7.0        1\n",
      "                        ..\n",
      "9998.0_512.0_162.0       1\n",
      "9999.0_330.0_13.0        1\n",
      "9999.0_330.0_21.0        1\n",
      "9999.0_330.0_35.0       15\n",
      "9999.0_441.0_81.0        1\n",
      "Length: 208962, dtype: int64 208962\n",
      "213 ['11157.0_204.0_-58.0', '11207.0_126.0_-74.0', '1137.0_299.0_-480.0', '12163.0_204.0_-71.0', '12318.0_299.0_-7.0', '12501.0_204.0_-482.0', '12544.0_441.0_-54.0', '12544.0_476.0_-102.0', '12570.0_191.0_-22.0', '12616.0_nan_-491.0', '12695.0_126.0_33.0', '12695.0_325.0_-124.0', '12695.0_325.0_-28.0', '12695.0_325.0_-342.0', '12695.0_325.0_-480.0', '12725.0_204.0_-115.0', '12741.0_143.0_-202.0', '12839.0_264.0_36.0', '12839.0_264.0_40.0', '12866.0_330.0_-423.0', '12932.0_325.0_-23.0', '13108.0_191.0_-50.0', '13155.0_472.0_21.0', '13241.0_126.0_33.0', '13455.0_231.0_-298.0', '13481.0_325.0_-23.0', '13597.0_191.0_-48.0', '13623.0_498.0_117.0', '13780.0_441.0_-454.0', '1393.0_308.0_40.0', '14482.0_181.0_56.0', '14649.0_441.0_-38.0', '14649.0_476.0_47.0', '15063.0_181.0_-14.0', '15280.0_191.0_-136.0', '15286.0_343.0_-21.0', '15651.0_272.0_-259.0', '15775.0_330.0_129.0', '15813.0_441.0_-22.0', '15885.0_nan_1.0', '15885.0_nan_103.0', '15885.0_nan_105.0', '15885.0_nan_106.0', '15885.0_nan_107.0', '15885.0_nan_109.0', '15885.0_nan_111.0', '15885.0_nan_112.0', '15885.0_nan_113.0', '15885.0_nan_114.0', '15885.0_nan_117.0', '15885.0_nan_118.0', '15885.0_nan_119.0', '15885.0_nan_120.0', '15885.0_nan_122.0', '15885.0_nan_126.0', '15885.0_nan_132.0', '15885.0_nan_14.0', '15885.0_nan_15.0', '15885.0_nan_16.0', '15885.0_nan_160.0', '15885.0_nan_162.0', '15885.0_nan_163.0', '15885.0_nan_165.0', '15885.0_nan_168.0', '15885.0_nan_17.0', '15885.0_nan_18.0', '15885.0_nan_19.0', '15885.0_nan_2.0', '15885.0_nan_20.0', '15885.0_nan_21.0', '15885.0_nan_22.0', '15885.0_nan_23.0', '15885.0_nan_24.0', '15885.0_nan_25.0', '15885.0_nan_26.0', '15885.0_nan_27.0', '15885.0_nan_28.0', '15885.0_nan_29.0', '15885.0_nan_30.0', '15885.0_nan_31.0', '15885.0_nan_33.0', '15885.0_nan_34.0', '15885.0_nan_35.0', '15885.0_nan_36.0', '15885.0_nan_4.0', '15885.0_nan_51.0', '15885.0_nan_52.0', '15885.0_nan_57.0', '15885.0_nan_59.0', '15885.0_nan_63.0', '15885.0_nan_64.0', '15885.0_nan_65.0', '15885.0_nan_67.0', '15885.0_nan_71.0', '15885.0_nan_72.0', '15885.0_nan_76.0', '15885.0_nan_77.0', '15885.0_nan_78.0', '15885.0_nan_79.0', '15885.0_nan_83.0', '15885.0_nan_84.0', '15885.0_nan_85.0', '15885.0_nan_90.0', '15885.0_nan_92.0', '15885.0_nan_95.0', '15885.0_nan_97.0', '15986.0_441.0_-39.0', '16132.0_299.0_-445.0', '16136.0_nan_161.0', '16163.0_325.0_-28.0', '16691.0_181.0_-308.0', '16709.0_264.0_-13.0', '1675.0_330.0_71.0', '16865.0_433.0_-25.0', '16876.0_204.0_61.0', '16883.0_476.0_-13.0', '16916.0_181.0_-2.0', '16923.0_441.0_-23.0', '16927.0_203.0_-477.0', '16998.0_330.0_-37.0', '17043.0_126.0_34.0', '17188.0_122.0_-139.0', '17188.0_299.0_-254.0', '17188.0_299.0_16.0', '17188.0_299.0_17.0', '17188.0_299.0_24.0', '17188.0_299.0_25.0', '17188.0_299.0_61.0', '17188.0_310.0_-192.0', '1738.0_184.0_-6.0', '17399.0_204.0_-211.0', '17399.0_204.0_20.0', '17400.0_123.0_38.0', '17466.0_264.0_-12.0', '17492.0_387.0_78.0', '18132.0_476.0_-111.0', '18246.0_204.0_-97.0', '1974.0_184.0_-469.0', '2039.0_204.0_-139.0', '2581.0_325.0_36.0', '2772.0_226.0_-357.0', '2803.0_251.0_-320.0', '2803.0_264.0_-345.0', '2803.0_337.0_65.0', '2845.0_315.0_-10.0', '2884.0_204.0_-32.0', '2939.0_204.0_-137.0', '3154.0_nan_118.0', '3154.0_nan_21.0', '3154.0_nan_22.0', '3154.0_nan_23.0', '3154.0_nan_24.0', '3154.0_nan_26.0', '3154.0_nan_27.0', '3154.0_nan_31.0', '3865.0_272.0_-146.0', '3898.0_181.0_-188.0', '4063.0_330.0_-59.0', '4067.0_512.0_-428.0', '4074.0_441.0_-277.0', '4121.0_476.0_-8.0', '4461.0_nan_21.0', '4461.0_nan_23.0', '5293.0_123.0_-110.0', '5465.0_nan_-379.0', '5699.0_436.0_-15.0', '5938.0_272.0_28.0', '6019.0_325.0_-482.0', '6170.0_226.0_48.0', '6292.0_204.0_49.0', '6292.0_269.0_63.0', '6344.0_123.0_-47.0', '6542.0_324.0_-248.0', '7099.0_nan_-14.0', '7207.0_204.0_-128.0', '7207.0_204.0_-465.0', '7207.0_204.0_-482.0', '7288.0_122.0_10.0', '7585.0_126.0_90.0', '7585.0_191.0_-142.0', '7826.0_325.0_-33.0', '8058.0_330.0_-329.0', '8109.0_384.0_-9.0', '8135.0_325.0_49.0', '8320.0_444.0_-69.0', '8406.0_325.0_-422.0', '8528.0_387.0_-159.0', '8900.0_231.0_-60.0', '9002.0_272.0_-92.0', '9112.0_441.0_90.0', '9323.0_191.0_-50.0', '9500.0_126.0_-85.0', '9500.0_204.0_-53.0', '9500.0_204.0_-56.0', '9500.0_231.0_12.0', '9500.0_330.0_10.0', '9500.0_330.0_17.0', '9500.0_330.0_20.0', '9633.0_nan_119.0', '9633.0_nan_15.0', '9633.0_nan_20.0', '9633.0_nan_21.0', '9633.0_nan_22.0', '9633.0_nan_23.0', '9633.0_nan_26.0', '9633.0_nan_28.0', '9633.0_nan_29.0', '9633.0_nan_62.0', '9749.0_226.0_-5.0', '9749.0_231.0_33.0', '9885.0_264.0_-142.0', '9917.0_nan_116.0', '9917.0_nan_117.0']\n",
      "1025 15775.0_330.0_129.0\n"
     ]
    }
   ],
   "source": [
    "# Group by ENTITY_ID for train data\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 50\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "print(len(train_uid), train_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert each group to a 2D numpy array and stack them into a list of 2D data frames, first approach\n",
    "# x_train3 = [group.drop(columns=['ENTITY_ID', 'TransactionID']).values for name, group in grouped if len(group) >= 50]\n",
    "\n",
    "# print(len(x_train3), len(train_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach, here first plotting\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):    # enumerate(short_list): \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "    print(feat)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) >= 50:\n",
    "            # print(name, group)\n",
    "            df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                \n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        # for plotting\n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if unique_values.shape[0] > 25:  # take 25 most frequent values for plots\n",
    "            idx = np.argsort(counts)[::-1][:25]\n",
    "            unique_values = unique_values[idx]\n",
    "            counts = counts[idx]\n",
    "            lab = f'showing 25 most frequent \\nof {len(np.unique(tmp))} unique values'\n",
    "        else:\n",
    "            lab = f'{len(np.unique(tmp))} unique values'\n",
    "        \n",
    "        plt.bar(unique_values, counts, label=lab)\n",
    "        if feat in ['p_emaildomain', 'r_emaildomain', 'deviceinfo']:\n",
    "            plt.xticks(rotation=50, ha='right')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print(np.max(tmp), np.min(tmp))\n",
    "\n",
    "        plt.hist(tmp, bins=50, range=[np.quantile(tmp,0.03), np.quantile(tmp,0.97)], label=f'mean: {np.mean(tmp):.1f}, std: {np.std(tmp):.1f} \\nmin: {np.min(tmp):.1f}, max: {np.max(tmp):.1f}')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productcd\n",
      "[array(['C', 'H', 'R', 'S', 'W'], dtype=object)] 5\n",
      "card6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m non_num_cols:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feat)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m grouped:  \u001b[38;5;66;03m# first get all transactions in train data regardless of uid\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(group) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;66;03m# print(name, group)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m             df \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENTITY_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransactionID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/groupby/ops.py:727\u001b[0m, in \u001b[0;36mBaseGrouper.get_iterator\u001b[0;34m(self, data, axis)\u001b[0m\n\u001b[1;32m    725\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(data, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    726\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys_seq\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, splitter)\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1239\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1236\u001b[0m starts, ends \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mgenerate_slices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(starts, ends):\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chop\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1265\u001b[0m, in \u001b[0;36mFrameSplitter._chop\u001b[0;34m(self, sdata, slice_obj)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chop\u001b[39m(\u001b[38;5;28mself\u001b[39m, sdata: DataFrame, slice_obj: \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;66;03m# Fastpath equivalent to:\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;66;03m# if self.axis == 0:\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;66;03m#     return sdata.iloc[slice_obj]\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m#     return sdata.iloc[:, slice_obj]\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m sdata\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_slice(slice_obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m-> 1265\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43msdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39m__finalize__(sdata, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/frame.py:666\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# first check if a Manager is passed without any other arguments\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# -> use fastpath (without checking Manager type)\u001b[39;00m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;66;03m# GH#33357 fastpath\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m         \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    669\u001b[0m manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/generic.py:281\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[0;34m(self, data, copy, attrs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs)\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/VSCode_projects/iTransformerAD/.conda/lib/python3.8/site-packages/pandas/core/flags.py:51\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[0;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D arrays, second approach\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "other_values = {}\n",
    "encoding = {}\n",
    "# means = {}  # next step\n",
    "# std = {}\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):  \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "\n",
    "    # for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "    #     if len(group) >= 50:\n",
    "    #         # print(name, group)\n",
    "    #         df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "    #         if feat in non_num_cols:\n",
    "    #             df[feat] = df[feat].fillna('missing')\n",
    "    #         else:\n",
    "    #             df[feat] = df[feat].fillna(0)\n",
    "    #         tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    # print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        print(feat)\n",
    "        for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "            if len(group) >= 50:\n",
    "                # print(name, group)\n",
    "                df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                tmp = np.concatenate((tmp, df[feat].values))\n",
    "\n",
    "        # for encoding \n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if len(unique_values) > 50:  # take 100 most frequent values for encoding\n",
    "            idx = np.argsort(counts)\n",
    "            unique_values = unique_values[idx]  # unique values sorted in frequency\n",
    "            other_values[feat] = unique_values[49:]   # values to be encoded as 'other'\n",
    "            print(other_values)\n",
    "            # replace all elements of tmp that are in other_values with 'other'\n",
    "            idx = np.where(np.isin(tmp, other_values[feat]))\n",
    "            tmp[idx] = 'other'\n",
    "            # print(len(np.unique(tmp)))\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore').fit(tmp.reshape(-1, 1))\n",
    "        print(enc.categories_, len(enc.categories_[0]))\n",
    "        encoding[feat] = enc\n",
    "\n",
    "print(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach\n",
    "x_train3 = []\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "for name, group in grouped:  # second get all transactions in train data keeping structure of uid\n",
    "    if len(group) >= 50:\n",
    "        # print(name, group)\n",
    "        df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_train3.append(df.values)\n",
    "\n",
    "print(len(x_train3), len(train_uid))\n",
    "updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train3), x_train3[0].shape, x_train3[0])\n",
    "print(len(train_uid))\n",
    "print(len(updated_feature_names), updated_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach: for test data now\n",
    "x_test3 = []\n",
    "\n",
    "grouped_test = x_test.groupby('ENTITY_ID')\n",
    "test_uid = grouped_test.size()[grouped_test.size() >= 20].index  # allow shorter time series for testing!\n",
    "test_uid = list(test_uid)\n",
    "for name, group in grouped_test:  # second get all transactions in test data keeping structure of uid\n",
    "    if len(group) >= 20:\n",
    "        df = group.drop(columns=['ENTITY_ID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_test3.append(df.values)\n",
    "\n",
    "print(len(x_test3))\n",
    "# updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_train3):\n",
    "    print(f'time series for user: {train_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/train_{train_uid[i]}_{j}')\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # arr_scaled = scaler.fit_transform(arr)\n",
    "    # plot_data(arr_scaled, feature_names, path=f'plots_data/IEEECIS_new/train_scaled_{train_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/train_{train_uid[i]}.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_{j}')\n",
    "    \n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = arr[:, 1:]                    # remove the transaction IDs from test data\n",
    "    print(arr.shape)\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # if test_uid[i] == '13623.0_498.0_117.0':\n",
    "    #     for j in range(0, len(updated_feature_names), 20):\n",
    "    #         plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_{j}')\n",
    "    # else:\n",
    "    #     plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}_0')\n",
    "    plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new2/test_{test_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new2/test_{test_uid[i]}.npy', arr)\n",
    "    np.save(f'processed/IEEECIS_new2/labels_{test_uid[i]}.npy', y_test)\n",
    "    # np.save(f'processed/IEEECIS_new2/ids_{test_uid[i]}.npy', test_transaction_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for test data\n",
    "grouped = x_test.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are >= 20\n",
    "test_uid = grouped.size()[grouped.size() >= 20].index\n",
    "test_uid = list(test_uid)\n",
    "print(test_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n",
    "\n",
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames\n",
    "x_test3 = [group.drop(columns='ENTITY_ID').values for name, group in grouped if len(group) >= 20]\n",
    "\n",
    "print(len(x_test3), len(test_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the truth labels of the test data\n",
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = np.nan_to_num(arr)            # test data\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # scaler already defined for train data? or better to redefine?\n",
    "    arr_scaled = scaler.fit_transform(arr)\n",
    "\n",
    "    print(arr_scaled.shape, y_test.shape)\n",
    "\n",
    "    if len(y_test[y_test==1]) > 0:\n",
    "        print(test_uid[i])\n",
    "        plot_data(arr, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}')\n",
    "        plot_data(arr_scaled, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_scaled{test_uid[i]}')\n",
    "    \n",
    "        np.save(f'processed/IEEECIS_new/test_scaled_{test_uid[i]}.npy', arr_scaled)\n",
    "        np.save(f'processed/IEEECIS_new/labels_{test_uid[i]}.npy', y_test)\n",
    "        np.save(f'processed/IEEECIS_new/ids_{test_uid[i]}.npy', test_transaction_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATLAS time series data for AD in LAr data (from Vilius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/user.vcepaiti.38697672.EXT0._000002.AnnotatorNtuple.h5', 'r')\n",
    "print(f.keys(), f.attrs.keys())\n",
    "data = np.array(f['data'])\n",
    "lumiblock = np.array(f['lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data, data.shape, data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f['event_labels'], f['event_numbers'], f['features'], f['run'])\n",
    "evt_lb = np.array(f['event_labels'])\n",
    "print(evt_lb, evt_lb[0].shape)\n",
    "print(np.min(evt_lb[2]), np.max(evt_lb[2]))\n",
    "print(np.unique(evt_lb[2]))\n",
    "print(np.any(np.logical_and(evt_lb != 0, evt_lb !=2)))\n",
    "evt_nb = np.array(f['event_numbers'])\n",
    "print(evt_nb, evt_nb[0].shape)\n",
    "features = np.array(f['features'])\n",
    "print(features)\n",
    "run = np.array(f['run'])\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(5):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(16, 1, figsize=(10, 25), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        l = evt_lb[lb]\n",
    "        l1 = (l==1) + 0\n",
    "        l2 = (l==2) + 0\n",
    "        l3 = (l==3) + 0\n",
    "        # print(x_t.shape, x_t[0].shape)\n",
    "        axs[dim].plot(x_t, label='data')\n",
    "        axs[dim].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "        axs[dim].plot(l2, '--', linewidth=1, color='tab:red', label='noise burst')\n",
    "        axs[dim].plot(l3, '--', linewidth=1, color='tab:purple', label='data corruption')\n",
    "        axs[dim].set_title(str(features[dim])[2:-1])\n",
    "    axs[0].legend()\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(5):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(17, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "        axs[dim].legend(loc='upper right')\n",
    "        # axs[dim].set_title(features[dim][2:])\n",
    "    \n",
    "    l = evt_lb[lb]      # same label for all dimensions/features\n",
    "    l1 = (l==1) + 0\n",
    "    l2 = (l==2) + 0\n",
    "    l3 = (l==3) + 0\n",
    "    axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "    axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "    axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "    axs[-1].legend(loc='upper right')\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    features[f] = str(features[f])[2:-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(5):\n",
    "    labels[f'lumiblock_{i}'] = (evt_lb[i] > 0) + 0\n",
    "    l = labels[f'lumiblock_{i}']\n",
    "    print(len(l[l>0]), len(l[l==0]), len(l[l>0])/len(l[l==0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].shape)\n",
    "temp = np.stack(data[0])\n",
    "print(np.array(data).shape, temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for lb in range(5):\n",
    "    \n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x[:,0].shape)\n",
    "    print(np.max(x[:,0]), np.min(x[:,0]), np.mean(x[:,0]), np.std(x[:,0]))\n",
    "    y = labels[f'lumiblock_{lb}']\n",
    "    print(x.shape, y.shape)\n",
    "    scaler = StandardScaler()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    print(np.max(x_norm[:,0]), np.min(x_norm[:,0]), np.mean(x_norm[:,0]), np.std(x_norm[:,0]))  \n",
    "\n",
    "    plt.plot(x_norm[:,0], label='normalised')\n",
    "    plt.plot(x[:,0], label='raw')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "\n",
    "x_train = np.stack(data[0]).T\n",
    "for lb in range(1,4):\n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x.shape)\n",
    "    x_train = np.concatenate((x_train, x), axis=0)\n",
    "    print(x_train.shape)\n",
    "x_test = np.stack(data[4]).T\n",
    "y_test = np.array(labels['lumiblock_4'][:, np.newaxis])\n",
    "y_test = np.repeat(y_test, repeats=16, axis=1)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# print(max(x[0]))\n",
    "# x, min_a, max_a = normalize3(x_train, min_a=None)\n",
    "# print(max(x_train[0]))\n",
    "# x_test, _, _ = normalize3(x_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "fig, axs = plt.subplots(16, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "for dim in range(16):  # iterate through 16 features we're using\n",
    "    x_t = x_train[:, dim]   \n",
    "    axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "    axs[dim].legend(loc='upper right')\n",
    "    # axs[dim].set_title(features[dim][2:])\n",
    "\n",
    "# l = y_train      # same label for all dimensions/features\n",
    "# l1 = (l==1) + 0\n",
    "# l2 = (l==2) + 0\n",
    "# l3 = (l==3) + 0\n",
    "# axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "# axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "# axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "axs[-1].legend(loc='upper right')\n",
    "fig.supylabel('label')\n",
    "fig.supxlabel('events')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'processed/ATLAS_TS/train.npy', x_train)\n",
    "np.save(f'processed/ATLAS_TS/test.npy', x_test)\n",
    "np.save(f'processed/ATLAS_TS/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in range(5):\n",
    "    x = np.stack(data[lb]).T\n",
    "    y = np.array(labels[f'lumiblock_{lb}'][:, np.newaxis])\n",
    "    y = np.repeat(y, repeats=16, axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_train.npy', x)\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    \n",
    "    # if lb < 4:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_train.npy', x)\n",
    "    #     # np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    # else:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_test.npy', x)\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set on water quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/1_gecco2018_water_quality.csv')\n",
    "data = data.sort_values(by='Time')\n",
    "print(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Time', 'Unnamed: 0', 'EVENT'], axis=1)\n",
    "y = data.EVENT\n",
    "features = X.columns\n",
    "print(X.shape, y.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y+0)\n",
    "print(len(y[y==1]), len(y[y==0]), len(y[y==1])/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbm = np.where(np.isnan(X))[0]\n",
    "print(pbm)  \n",
    "print(len(pbm))\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "print(np.isnan(X).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)*0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:83740]\n",
    "X_test = X[83740:]\n",
    "y_train = y[:83740]\n",
    "y_test = y[83740:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need y to be 2D\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, features, y_train, '/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/data_train_norm')\n",
    "plot_data(X_test, features, y_test, '/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/data_test_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(len(y_test[y_test==1]), len(y_test[y_test==0]), len(y_test[y_test==1])/len(y_test))   \n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/train.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/test.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
