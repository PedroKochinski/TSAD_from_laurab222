{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "plt.style.use(hep.style.firamath)\n",
    "plt.rcParams['lines.markersize'] = 4\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, features, y=None, path=None):\n",
    "    # plt.rcParams[\"text.usetex\"] = False\n",
    "    # # plt.rcParams['figure.figsize'] = 6, 2\n",
    "    # # plt.rcParams['lines.markersize'] = 2\n",
    "    # plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "    size = int(len(features)*1.5)   # old *0.6)\n",
    "    dims = len(features)\n",
    "    if y is not None:\n",
    "        dims += 1\n",
    "\n",
    "    fig, axs = plt.subplots(dims, 1, figsize=(17, size), sharex=True)\n",
    "\n",
    "    for dim, feat in enumerate(features):  # iterate through the features we're using\n",
    "        # print(feat)\n",
    "        x_t = X[:, dim]\n",
    "        axs[dim].plot(x_t)\n",
    "        # if y is not None:\n",
    "        #     y_scaled = np.max(x_t)*y\n",
    "        #     axs[dim].plot(y_scaled, '--', linewidth=1, color='tab:orange', label='anomalies')\n",
    "        # turn ylabel by 90 degrees and shift it to the left\n",
    "        axs[dim].set_ylabel(feat, rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        # Align all y-axis labels by setting the same label coordinates\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[dim].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        # axs[dim].legend(loc='upper right')\n",
    "    if y is not None: # plot the target variable in last dimension if we have truth labels\n",
    "        axs[-1].plot(y, '--', color='tab:red')\n",
    "        axs[-1].set_yticks([0, 1])\n",
    "        axs[-1].set_ylabel('anomalies', rotation=0, ha='right', rotation_mode='default', labelpad=5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "        axs[-1].yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "    axs[-1].set_xlabel('Transactions')  # looks alignment-wise better than supxlabel\n",
    "    # fig.supylabel('values')\n",
    "    # fig.supxlabel('Time stamp [min]')\n",
    "    plt.tight_layout()\n",
    "    if path is not None:\n",
    "        plt.savefig(f'{path}.png', dpi=100, facecolor='white')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclipse data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_data.hdf', 'r')\n",
    "f_test = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_data.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_train.keys())\n",
    "print(f_train.attrs.keys())\n",
    "\n",
    "print(f_train['prod_train_data'].keys())\n",
    "print(f_train['prod_train_data']['axis0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_train as file:\n",
    "    for key in file['prod_train_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_train_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with f_test as file:\n",
    "    for key in file['prod_test_data'].keys():\n",
    "        print('\\n', key)\n",
    "        # Access the dataset\n",
    "        dataset = file['prod_test_data'][key]\n",
    "        \n",
    "        # Read the dataset into a numpy array\n",
    "        data = dataset[:]\n",
    "        print(data.shape)\n",
    "        \n",
    "        # # Since the dataset contains strings of fixed length, you can decode the bytes\n",
    "        # # Convert bytes to strings\n",
    "        # if key in ['axis0', 'block0_items', 'block1_items']:\n",
    "        #     strings = [x.decode('utf-8').strip() for x in data]\n",
    "        \n",
    "        #     # Print the strings\n",
    "        #     for s in strings:\n",
    "        #         print(s)\n",
    "        # else:\n",
    "        #     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_train_label.csv')\n",
    "label_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/eclipse_small_prod_dataset/prod_test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_test.shape, label_train.shape)\n",
    "print(label_train)\n",
    "y_train = label_train['binary_anom']\n",
    "print(y_train[y_train==1])\n",
    "y_test = label_test['binary_anom']\n",
    "print(y_test[y_test==1], '\\n', len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_test\n",
    "label['job_id'] = label['job_id'].astype('str')\n",
    "label['component_id'] = label['component_id'].astype('str') \n",
    "label.set_index(['job_id', 'component_id'], inplace=True)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEE-CIS fraud detection data set (from kaggle challenge 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_transaction.csv')\n",
    "x_train2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_train2.shape)\n",
    "print(x_train.columns) #, x_train2.columns)\n",
    "\n",
    "for i, name in enumerate(x_train.columns):\n",
    "    print(name)\n",
    "    # print(x_train[name].dtypes)\n",
    "\n",
    "# Filter columns that are of type 'float'\n",
    "float_cols = x_train.select_dtypes(include='float').columns\n",
    "# print(\"Columns with float values:\", float_cols)\n",
    "# for i, name in enumerate(float_cols.columns):\n",
    "#     print(name)\n",
    "\n",
    "# Filter columns that are NOT of type 'float'\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(by='TransactionDT')\n",
    "x_train = x_train.drop(columns=non_num_cols)\n",
    "y_train = x_train['isFraud']\n",
    "x_train = x_train.drop(columns='isFraud')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(0)\n",
    "has_nan = y_train.isnull().any()\n",
    "print(has_nan)\n",
    "print(x_train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "print(max(X_train[0]))\n",
    "X_train, min_a, max_a = normalize3(X_train, min_a=None)\n",
    "print(max(X_train[0]))\n",
    "X_test, _, _ = normalize3(X_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for dim in range(5):\n",
    "    x_t, l = X_train[:10000, dim], y_train[:10000]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(x_t, label='data')\n",
    "    plt.plot(l, '--', linewidth=0.2)\n",
    "    plt.fill_between(np.arange(l.shape[0]), l, color='tab:orange', alpha=0.3, label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/train_1.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/test_1.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/IEEECIS/labels_1.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['day'] = x_train['TransactionDT'] / (24 * 60 * 60)  # to convert seconds to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['uid1'] =  (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str)\n",
    "# x_train['uid2'] =  (x_train.card1.astype(str) +'_' + \\\n",
    "#             x_train.addr1.astype(str) +'_' + \\\n",
    "#             (x_train.day - x_train.D1).astype(str) +'_' + \\\n",
    "#             x_train.P_emaildomain.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train['uid1'])\n",
    "# print(x_train['uid1'].unique)\n",
    "# print(x_train['uid2'])\n",
    "# print(x_train['uid2'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_transaction.csv')\n",
    "x_test2 = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieee-fraud-detection/test_identity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.columns)\n",
    "print(x_test.shape)\n",
    "\n",
    "for i, name in enumerate(x_test.columns):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.sort_values(by='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEEE-CIS challenge from kaggle loaded from fraud-dataset-benchmark (contains UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieeecis/train.csv')\n",
    "x_test = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieeecis/test.csv')\n",
    "labels = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/ieeecis/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "# print(x_train.columns, x_test.columns)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtypes)\n",
    "\n",
    "# Filter columns that are NOT of type 'float' and for now pop them out\n",
    "non_num_cols = x_train.select_dtypes(exclude=['float', 'int']).columns\n",
    "print(\"Columns without numeric values:\", non_num_cols, len(non_num_cols))\n",
    "for i, name in enumerate(non_num_cols):\n",
    "    print(name, x_train[name].dtypes)\n",
    "    # if name not in ['ENTITY_ID', 'EVENT_TIMESTAMP']:\n",
    "    #     x_train.pop(name)\n",
    "    #     x_test.pop(name)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(['EVENT_TIMESTAMP'])\n",
    "x_test = x_test.sort_values(['EVENT_TIMESTAMP'])\n",
    "date_train = x_train.pop('EVENT_TIMESTAMP')\n",
    "print(date_train.shape, date_train)\n",
    "date_test = x_test.pop('EVENT_TIMESTAMP')\n",
    "print(date_test.shape, date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(x_train.columns)\n",
    "feature_names.remove('ENTITY_ID') # because we don't want to use them as a feature\n",
    "feature_names.remove('TransactionID') # because we don't want to use them as a feature\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for train data\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are > 10\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "print(len(train_uid), train_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert each group to a 2D numpy array and stack them into a list of 2D data frames, first approach\n",
    "# x_train3 = [group.drop(columns=['ENTITY_ID', 'TransactionID']).values for name, group in grouped if len(group) >= 50]\n",
    "\n",
    "# print(len(x_train3), len(train_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach, here first plotting\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):    # enumerate(short_list): \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "    print(feat)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) >= 50:\n",
    "            # print(name, group)\n",
    "            df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                \n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        # for plotting\n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if unique_values.shape[0] > 25:  # take 25 most frequent values for plots\n",
    "            idx = np.argsort(counts)[::-1][:25]\n",
    "            unique_values = unique_values[idx]\n",
    "            counts = counts[idx]\n",
    "            lab = f'showing 25 most frequent \\nof {len(np.unique(tmp))} unique values'\n",
    "        else:\n",
    "            lab = f'{len(np.unique(tmp))} unique values'\n",
    "        \n",
    "        plt.bar(unique_values, counts, label=lab)\n",
    "        if feat in ['p_emaildomain', 'r_emaildomain', 'deviceinfo']:\n",
    "            plt.xticks(rotation=50, ha='right')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        print(np.max(tmp), np.min(tmp))\n",
    "\n",
    "        plt.hist(tmp, bins=50, range=[np.quantile(tmp,0.03), np.quantile(tmp,0.97)], label=f'mean: {np.mean(tmp):.1f}, std: {np.std(tmp):.1f} \\nmin: {np.min(tmp):.1f}, max: {np.max(tmp):.1f}')\n",
    "        plt.title(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "        plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/plots_data/IEEECIS_new/feature_distr_zoom/{feat}.png', dpi=300, facecolor='white')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D arrays, second approach\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "short_list = ['p_emaildomain', 'r_emaildomain', 'deviceinfo']\n",
    "other_values = {}\n",
    "encoding = {}\n",
    "# means = {}  # next step\n",
    "# std = {}\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "for i, feat in enumerate(feature_names):  \n",
    "    tmp = np.empty((0))  # reset this temporary array for each feature\n",
    "\n",
    "    # for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "    #     if len(group) >= 50:\n",
    "    #         # print(name, group)\n",
    "    #         df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "    #         if feat in non_num_cols:\n",
    "    #             df[feat] = df[feat].fillna('missing')\n",
    "    #         else:\n",
    "    #             df[feat] = df[feat].fillna(0)\n",
    "    #         tmp = np.concatenate((tmp, df[feat].values))\n",
    "    \n",
    "    # print(f'{len(np.unique(tmp))} unique values out of {len(tmp)}')  # constructs set of df[feat] and counts the number of unique values\n",
    "\n",
    "    if feat in non_num_cols:\n",
    "        print(feat)\n",
    "        for name, group in grouped:  # first get all transactions in train data regardless of uid\n",
    "            if len(group) >= 50:\n",
    "                # print(name, group)\n",
    "                df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "                tmp = np.concatenate((tmp, df[feat].values))\n",
    "\n",
    "        # for encoding \n",
    "        unique_values, counts = np.unique(tmp, return_counts=True)\n",
    "        if len(unique_values) > 50:  # take 100 most frequent values for encoding\n",
    "            idx = np.argsort(counts)\n",
    "            unique_values = unique_values[idx]  # unique values sorted in frequency\n",
    "            other_values[feat] = unique_values[49:]   # values to be encoded as 'other'\n",
    "            print(other_values)\n",
    "            # replace all elements of tmp that are in other_values with 'other'\n",
    "            idx = np.where(np.isin(tmp, other_values[feat]))\n",
    "            tmp[idx] = 'other'\n",
    "            # print(len(np.unique(tmp)))\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore').fit(tmp.reshape(-1, 1))\n",
    "        print(enc.categories_, len(enc.categories_[0]))\n",
    "        encoding[feat] = enc\n",
    "\n",
    "print(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach\n",
    "x_train3 = []\n",
    "\n",
    "grouped = x_train.groupby('ENTITY_ID')\n",
    "train_uid = grouped.size()[grouped.size() >= 50].index\n",
    "train_uid = list(train_uid)\n",
    "for name, group in grouped:  # second get all transactions in train data keeping structure of uid\n",
    "    if len(group) >= 50:\n",
    "        # print(name, group)\n",
    "        df = group.drop(columns=['ENTITY_ID', 'TransactionID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_train3.append(df.values)\n",
    "\n",
    "print(len(x_train3), len(train_uid))\n",
    "updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train3), x_train3[0].shape, x_train3[0])\n",
    "print(len(train_uid))\n",
    "print(len(updated_feature_names), updated_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames, second approach: for test data now\n",
    "x_test3 = []\n",
    "\n",
    "grouped_test = x_test.groupby('ENTITY_ID')\n",
    "test_uid = grouped_test.size()[grouped_test.size() >= 20].index  # allow shorter time series for testing!\n",
    "test_uid = list(test_uid)\n",
    "for name, group in grouped_test:  # second get all transactions in test data keeping structure of uid\n",
    "    if len(group) >= 20:\n",
    "        df = group.drop(columns=['ENTITY_ID'])\n",
    "        print(df.columns, df.shape)\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            print(feat)\n",
    "            if feat in non_num_cols:\n",
    "                df[feat] = df[feat].fillna('missing')\n",
    "            else:\n",
    "                df[feat] = df[feat].fillna(0)\n",
    "            \n",
    "            arr = df[feat].values\n",
    "            print(arr.shape, len(np.unique(arr)))\n",
    "\n",
    "            if feat in non_num_cols:\n",
    "                if feat in other_values.keys():\n",
    "                    idx = np.where(np.isin(arr, other_values[feat]))\n",
    "                    arr[idx] = 'other'\n",
    "\n",
    "                enc = encoding[feat]\n",
    "                new_feat = enc.get_feature_names_out(input_features=[feat])\n",
    "                print(new_feat)\n",
    "                arr = enc.transform(arr.reshape(-1, 1)).toarray()                \n",
    "\n",
    "                new_df = pd.DataFrame(arr, columns=new_feat)    # will start indexing from 0, creates pbm with concatenation\n",
    "                print(new_df.shape)\n",
    "                print(df.shape)\n",
    "                df = df.reset_index(drop=True)          # Reset index to ensure proper concatenation\n",
    "                new_df = new_df.reset_index(drop=True)  # Reset index to ensure proper concatenation\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(columns=[feat], inplace=True)\n",
    "                print(df.shape)\n",
    "\n",
    "        # check if still any nan in df\n",
    "        print(df.isnull().any().any())\n",
    "        x_test3.append(df.values)\n",
    "\n",
    "print(len(x_test3))\n",
    "# updated_feature_names = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_train3):\n",
    "    print(f'time series for user: {train_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/train_{train_uid[i]}_{j}')\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # arr_scaled = scaler.fit_transform(arr)\n",
    "    # plot_data(arr_scaled, feature_names, path=f'plots_data/IEEECIS_new/train_scaled_{train_uid[i]}')\n",
    "\n",
    "    np.save(f'processed/IEEECIS_new/train_{train_uid[i]}.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    # for j in range(0, len(updated_feature_names), 20):\n",
    "    #     plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_{j}')\n",
    "    \n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = arr[:, 1:]                    # remove the transaction IDs from test data\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    if test_uid[i] == '13623.0_498.0_117.0':\n",
    "        for j in range(0, len(updated_feature_names), 20):\n",
    "            plot_data(arr[:,j:j+20], updated_feature_names[j:j+20], y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_{j}')\n",
    "    else:\n",
    "        plot_data(arr[:,:20], updated_feature_names[:20], y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}_0')\n",
    "    \n",
    "    np.save(f'processed/IEEECIS_new/test_scaled_{test_uid[i]}.npy', arr)\n",
    "    np.save(f'processed/IEEECIS_new/labels_{test_uid[i]}.npy', y_test)\n",
    "    np.save(f'processed/IEEECIS_new/ids_{test_uid[i]}.npy', test_transaction_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ENTITY_ID for test data\n",
    "grouped = x_test.groupby('ENTITY_ID')\n",
    "print(grouped.size(), len(grouped))\n",
    "# count how many entries in group are > 10\n",
    "test_uid = grouped.size()[grouped.size() >= 20].index\n",
    "test_uid = list(test_uid)\n",
    "print(test_uid)\n",
    "max_len = max(grouped.size())\n",
    "print(max_len, max(grouped.size()[grouped.size() == max_len].index))\n",
    "\n",
    "# Convert each group to a 2D numpy array and stack them into a list of 2D data frames\n",
    "x_test3 = [group.drop(columns='ENTITY_ID').values for name, group in grouped if len(group) >= 20]\n",
    "\n",
    "print(len(x_test3), len(test_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test3), x_test3[0].shape, x_test3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the truth labels of the test data\n",
    "for i, arr in enumerate(x_test3):\n",
    "    print(f'time series for user: {test_uid[i]} has shape:\\n {arr.shape}')\n",
    "\n",
    "    test_transaction_id = arr[:, 0]     # transaction IDs of the test data (needed to identify corresponding labels)\n",
    "    arr = np.nan_to_num(arr)            # test data\n",
    "    \n",
    "    # pick the corresponding labels of the used test transactions\n",
    "    indices = np.where(np.isin(labels['TransactionID'], test_transaction_id))\n",
    "    y_test = np.array(labels.iloc[indices]['EVENT_LABEL'])\n",
    "    y_test = y_test[:, np.newaxis]\n",
    "    print('labels shape: ', y_test.shape, 'nb of anomalous transactions: ', len(y_test[y_test==1]))\n",
    "\n",
    "    # scaler already defined for train data? or better to redefine?\n",
    "    arr_scaled = scaler.fit_transform(arr)\n",
    "\n",
    "    print(arr_scaled.shape, y_test.shape)\n",
    "\n",
    "    if len(y_test[y_test==1]) > 0:\n",
    "        print(test_uid[i])\n",
    "        plot_data(arr, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_{test_uid[i]}')\n",
    "        plot_data(arr_scaled, feature_names, y=y_test, path=f'plots_data/IEEECIS_new/test_scaled{test_uid[i]}')\n",
    "    \n",
    "        np.save(f'processed/IEEECIS_new/test_scaled_{test_uid[i]}.npy', arr_scaled)\n",
    "        np.save(f'processed/IEEECIS_new/labels_{test_uid[i]}.npy', y_test)\n",
    "        np.save(f'processed/IEEECIS_new/ids_{test_uid[i]}.npy', test_transaction_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATLAS time series data for AD in LAr data (from Vilius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/user.vcepaiti.38697672.EXT0._000002.AnnotatorNtuple.h5', 'r')\n",
    "print(f.keys(), f.attrs.keys())\n",
    "data = np.array(f['data'])\n",
    "lumiblock = np.array(f['lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data, data.shape, data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f['event_labels'], f['event_numbers'], f['features'], f['run'])\n",
    "evt_lb = np.array(f['event_labels'])\n",
    "print(evt_lb, evt_lb[0].shape)\n",
    "print(np.min(evt_lb[2]), np.max(evt_lb[2]))\n",
    "print(np.unique(evt_lb[2]))\n",
    "print(np.any(np.logical_and(evt_lb != 0, evt_lb !=2)))\n",
    "evt_nb = np.array(f['event_numbers'])\n",
    "print(evt_nb, evt_nb[0].shape)\n",
    "features = np.array(f['features'])\n",
    "print(features)\n",
    "run = np.array(f['run'])\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(5):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(16, 1, figsize=(10, 25), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        l = evt_lb[lb]\n",
    "        l1 = (l==1) + 0\n",
    "        l2 = (l==2) + 0\n",
    "        l3 = (l==3) + 0\n",
    "        # print(x_t.shape, x_t[0].shape)\n",
    "        axs[dim].plot(x_t, label='data')\n",
    "        axs[dim].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "        axs[dim].plot(l2, '--', linewidth=1, color='tab:red', label='noise burst')\n",
    "        axs[dim].plot(l3, '--', linewidth=1, color='tab:purple', label='data corruption')\n",
    "        axs[dim].set_title(str(features[dim])[2:-1])\n",
    "    axs[0].legend()\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "for lb in range(5):  # iterate through lumiblocks\n",
    "    fig, axs = plt.subplots(17, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "    for dim in range(16):  # iterate through 16 features we're using\n",
    "        x_t = data[lb, dim]\n",
    "        axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "        axs[dim].legend(loc='upper right')\n",
    "        # axs[dim].set_title(features[dim][2:])\n",
    "    \n",
    "    l = evt_lb[lb]      # same label for all dimensions/features\n",
    "    l1 = (l==1) + 0\n",
    "    l2 = (l==2) + 0\n",
    "    l3 = (l==3) + 0\n",
    "    axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "    axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "    axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "    axs[-1].legend(loc='upper right')\n",
    "    fig.supylabel('label')\n",
    "    fig.supxlabel('events')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    features[f] = str(features[f])[2:-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(5):\n",
    "    labels[f'lumiblock_{i}'] = (evt_lb[i] > 0) + 0\n",
    "    l = labels[f'lumiblock_{i}']\n",
    "    print(len(l[l>0]), len(l[l==0]), len(l[l>0])/len(l[l==0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].shape)\n",
    "temp = np.stack(data[0])\n",
    "print(np.array(data).shape, temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for lb in range(5):\n",
    "    \n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x[:,0].shape)\n",
    "    print(np.max(x[:,0]), np.min(x[:,0]), np.mean(x[:,0]), np.std(x[:,0]))\n",
    "    y = labels[f'lumiblock_{lb}']\n",
    "    print(x.shape, y.shape)\n",
    "    scaler = StandardScaler()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    print(np.max(x_norm[:,0]), np.min(x_norm[:,0]), np.mean(x_norm[:,0]), np.std(x_norm[:,0]))  \n",
    "\n",
    "    plt.plot(x_norm[:,0], label='normalised')\n",
    "    plt.plot(x[:,0], label='raw')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import normalize3\n",
    "\n",
    "x_train = np.stack(data[0]).T\n",
    "for lb in range(1,4):\n",
    "    x = np.stack(data[lb]).T\n",
    "    print(x.shape)\n",
    "    x_train = np.concatenate((x_train, x), axis=0)\n",
    "    print(x_train.shape)\n",
    "x_test = np.stack(data[4]).T\n",
    "y_test = np.array(labels['lumiblock_4'][:, np.newaxis])\n",
    "y_test = np.repeat(y_test, repeats=16, axis=1)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# print(max(x[0]))\n",
    "# x, min_a, max_a = normalize3(x_train, min_a=None)\n",
    "# print(max(x_train[0]))\n",
    "# x_test, _, _ = normalize3(x_test, min_a, max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['figure.figsize'] = 6, 2\n",
    "plt.rcParams['lines.markersize'] = 3\n",
    "\n",
    "fig, axs = plt.subplots(16, 1, figsize=(10, 28), sharex=True)\n",
    "\n",
    "for dim in range(16):  # iterate through 16 features we're using\n",
    "    x_t = x_train[:, dim]   \n",
    "    axs[dim].plot(x_t, label=f'{str(features[dim])[2:-1]}')\n",
    "    axs[dim].legend(loc='upper right')\n",
    "    # axs[dim].set_title(features[dim][2:])\n",
    "\n",
    "# l = y_train      # same label for all dimensions/features\n",
    "# l1 = (l==1) + 0\n",
    "# l2 = (l==2) + 0\n",
    "# l3 = (l==3) + 0\n",
    "# axs[-1].plot(l1, '--', linewidth=1, color='tab:orange', label='mini noise burst')\n",
    "# axs[-1].plot(l2, ':', linewidth=1, color='tab:green', label='noise burst')\n",
    "# axs[-1].plot(l3, '-.', linewidth=1, color='tab:red', label='data corruption')\n",
    "axs[-1].legend(loc='upper right')\n",
    "fig.supylabel('label')\n",
    "fig.supxlabel('events')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(f'/Users/lauraboggia/VSCode_projects/TranAD/data/ATLAS_TS/plots/data_lb{lb}.png', dpi=300, facecolor='white')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'processed/ATLAS_TS/train.npy', x_train)\n",
    "np.save(f'processed/ATLAS_TS/test.npy', x_test)\n",
    "np.save(f'processed/ATLAS_TS/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in range(5):\n",
    "    x = np.stack(data[lb]).T\n",
    "    y = np.array(labels[f'lumiblock_{lb}'][:, np.newaxis])\n",
    "    y = np.repeat(y, repeats=16, axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_train.npy', x)\n",
    "    np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    \n",
    "    # if lb < 4:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_train.npy', x)\n",
    "    #     # np.save(f'processed/ATLAS_TS/lb_{lb}_labels.npy', y)\n",
    "    # else:\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_test.npy', x)\n",
    "    #     np.save(f'processed/ATLAS_TS/lb_{lb}_norm_labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set on water quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/1_gecco2018_water_quality.csv')\n",
    "data = data.sort_values(by='Time')\n",
    "print(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Time', 'Unnamed: 0', 'EVENT'], axis=1)\n",
    "y = data.EVENT\n",
    "features = X.columns\n",
    "print(X.shape, y.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y+0)\n",
    "print(len(y[y==1]), len(y[y==0]), len(y[y==1])/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbm = np.where(np.isnan(X))[0]\n",
    "print(pbm)  \n",
    "print(len(pbm))\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "print(np.isnan(X).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)*0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:83740]\n",
    "X_test = X[83740:]\n",
    "y_train = y[:83740]\n",
    "y_test = y[83740:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need y to be 2D\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, features, y_train, '/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/data_train_norm')\n",
    "plot_data(X_test, features, y_test, '/Users/lauraboggia/VSCode_projects/TranAD/data/3884398/data_test_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(len(y_test[y_test==1]), len(y_test[y_test==0]), len(y_test[y_test==1])/len(y_test))   \n",
    "print(len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/train.npy', X_train)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/test.npy', X_test)\n",
    "np.save('/Users/lauraboggia/VSCode_projects/TranAD/processed/GECCO/labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
